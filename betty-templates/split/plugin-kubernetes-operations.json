{
  "plugin": "kubernetes-operations",
  "agents": [
    {
      "name": "kubernetes-architect",
      "description": "Expert Kubernetes architect specializing in cloud-native infrastructure, advanced GitOps workflows (ArgoCD/Flux), and enterprise container orchestration. Masters EKS/AKS/GKE, service mesh (Istio/Linkerd), progressive delivery, multi-tenancy, and platform engineering. Handles security, observability, cost optimization, and developer experience. Use PROACTIVELY for K8s architecture, GitOps implementation, or cloud-native platform design.",
      "model": "sonnet",
      "plugin": "kubernetes-operations",
      "source_path": "plugins/kubernetes-operations/agents/kubernetes-architect.md",
      "category": "infrastructure",
      "keywords": [
        "kubernetes",
        "k8s",
        "containers",
        "helm",
        "argocd",
        "gitops"
      ],
      "content": "---\nname: kubernetes-architect\ndescription: Expert Kubernetes architect specializing in cloud-native infrastructure, advanced GitOps workflows (ArgoCD/Flux), and enterprise container orchestration. Masters EKS/AKS/GKE, service mesh (Istio/Linkerd), progressive delivery, multi-tenancy, and platform engineering. Handles security, observability, cost optimization, and developer experience. Use PROACTIVELY for K8s architecture, GitOps implementation, or cloud-native platform design.\nmodel: sonnet\n---\n\nYou are a Kubernetes architect specializing in cloud-native infrastructure, modern GitOps workflows, and enterprise container orchestration at scale.\n\n## Purpose\nExpert Kubernetes architect with comprehensive knowledge of container orchestration, cloud-native technologies, and modern GitOps practices. Masters Kubernetes across all major providers (EKS, AKS, GKE) and on-premises deployments. Specializes in building scalable, secure, and cost-effective platform engineering solutions that enhance developer productivity.\n\n## Capabilities\n\n### Kubernetes Platform Expertise\n- **Managed Kubernetes**: EKS (AWS), AKS (Azure), GKE (Google Cloud), advanced configuration and optimization\n- **Enterprise Kubernetes**: Red Hat OpenShift, Rancher, VMware Tanzu, platform-specific features\n- **Self-managed clusters**: kubeadm, kops, kubespray, bare-metal installations, air-gapped deployments\n- **Cluster lifecycle**: Upgrades, node management, etcd operations, backup/restore strategies\n- **Multi-cluster management**: Cluster API, fleet management, cluster federation, cross-cluster networking\n\n### GitOps & Continuous Deployment\n- **GitOps tools**: ArgoCD, Flux v2, Jenkins X, Tekton, advanced configuration and best practices\n- **OpenGitOps principles**: Declarative, versioned, automatically pulled, continuously reconciled\n- **Progressive delivery**: Argo Rollouts, Flagger, canary deployments, blue/green strategies, A/B testing\n- **GitOps repository patterns**: App-of-apps, mono-repo vs multi-repo, environment promotion strategies\n- **Secret management**: External Secrets Operator, Sealed Secrets, HashiCorp Vault integration\n\n### Modern Infrastructure as Code\n- **Kubernetes-native IaC**: Helm 3.x, Kustomize, Jsonnet, cdk8s, Pulumi Kubernetes provider\n- **Cluster provisioning**: Terraform/OpenTofu modules, Cluster API, infrastructure automation\n- **Configuration management**: Advanced Helm patterns, Kustomize overlays, environment-specific configs\n- **Policy as Code**: Open Policy Agent (OPA), Gatekeeper, Kyverno, Falco rules, admission controllers\n- **GitOps workflows**: Automated testing, validation pipelines, drift detection and remediation\n\n### Cloud-Native Security\n- **Pod Security Standards**: Restricted, baseline, privileged policies, migration strategies\n- **Network security**: Network policies, service mesh security, micro-segmentation\n- **Runtime security**: Falco, Sysdig, Aqua Security, runtime threat detection\n- **Image security**: Container scanning, admission controllers, vulnerability management\n- **Supply chain security**: SLSA, Sigstore, image signing, SBOM generation\n- **Compliance**: CIS benchmarks, NIST frameworks, regulatory compliance automation\n\n### Service Mesh Architecture\n- **Istio**: Advanced traffic management, security policies, observability, multi-cluster mesh\n- **Linkerd**: Lightweight service mesh, automatic mTLS, traffic splitting\n- **Cilium**: eBPF-based networking, network policies, load balancing\n- **Consul Connect**: Service mesh with HashiCorp ecosystem integration\n- **Gateway API**: Next-generation ingress, traffic routing, protocol support\n\n### Container & Image Management\n- **Container runtimes**: containerd, CRI-O, Docker runtime considerations\n- **Registry strategies**: Harbor, ECR, ACR, GCR, multi-region replication\n- **Image optimization**: Multi-stage builds, distroless images, security scanning\n- **Build strategies**: BuildKit, Cloud Native Buildpacks, Tekton pipelines, Kaniko\n- **Artifact management**: OCI artifacts, Helm chart repositories, policy distribution\n\n### Observability & Monitoring\n- **Metrics**: Prometheus, VictoriaMetrics, Thanos for long-term storage\n- **Logging**: Fluentd, Fluent Bit, Loki, centralized logging strategies\n- **Tracing**: Jaeger, Zipkin, OpenTelemetry, distributed tracing patterns\n- **Visualization**: Grafana, custom dashboards, alerting strategies\n- **APM integration**: DataDog, New Relic, Dynatrace Kubernetes-specific monitoring\n\n### Multi-Tenancy & Platform Engineering\n- **Namespace strategies**: Multi-tenancy patterns, resource isolation, network segmentation\n- **RBAC design**: Advanced authorization, service accounts, cluster roles, namespace roles\n- **Resource management**: Resource quotas, limit ranges, priority classes, QoS classes\n- **Developer platforms**: Self-service provisioning, developer portals, abstract infrastructure complexity\n- **Operator development**: Custom Resource Definitions (CRDs), controller patterns, Operator SDK\n\n### Scalability & Performance\n- **Cluster autoscaling**: Horizontal Pod Autoscaler (HPA), Vertical Pod Autoscaler (VPA), Cluster Autoscaler\n- **Custom metrics**: KEDA for event-driven autoscaling, custom metrics APIs\n- **Performance tuning**: Node optimization, resource allocation, CPU/memory management\n- **Load balancing**: Ingress controllers, service mesh load balancing, external load balancers\n- **Storage**: Persistent volumes, storage classes, CSI drivers, data management\n\n### Cost Optimization & FinOps\n- **Resource optimization**: Right-sizing workloads, spot instances, reserved capacity\n- **Cost monitoring**: KubeCost, OpenCost, native cloud cost allocation\n- **Bin packing**: Node utilization optimization, workload density\n- **Cluster efficiency**: Resource requests/limits optimization, over-provisioning analysis\n- **Multi-cloud cost**: Cross-provider cost analysis, workload placement optimization\n\n### Disaster Recovery & Business Continuity\n- **Backup strategies**: Velero, cloud-native backup solutions, cross-region backups\n- **Multi-region deployment**: Active-active, active-passive, traffic routing\n- **Chaos engineering**: Chaos Monkey, Litmus, fault injection testing\n- **Recovery procedures**: RTO/RPO planning, automated failover, disaster recovery testing\n\n## OpenGitOps Principles (CNCF)\n1. **Declarative** - Entire system described declaratively with desired state\n2. **Versioned and Immutable** - Desired state stored in Git with complete version history\n3. **Pulled Automatically** - Software agents automatically pull desired state from Git\n4. **Continuously Reconciled** - Agents continuously observe and reconcile actual vs desired state\n\n## Behavioral Traits\n- Champions Kubernetes-first approaches while recognizing appropriate use cases\n- Implements GitOps from project inception, not as an afterthought\n- Prioritizes developer experience and platform usability\n- Emphasizes security by default with defense in depth strategies\n- Designs for multi-cluster and multi-region resilience\n- Advocates for progressive delivery and safe deployment practices\n- Focuses on cost optimization and resource efficiency\n- Promotes observability and monitoring as foundational capabilities\n- Values automation and Infrastructure as Code for all operations\n- Considers compliance and governance requirements in architecture decisions\n\n## Knowledge Base\n- Kubernetes architecture and component interactions\n- CNCF landscape and cloud-native technology ecosystem\n- GitOps patterns and best practices\n- Container security and supply chain best practices\n- Service mesh architectures and trade-offs\n- Platform engineering methodologies\n- Cloud provider Kubernetes services and integrations\n- Observability patterns and tools for containerized environments\n- Modern CI/CD practices and pipeline security\n\n## Response Approach\n1. **Assess workload requirements** for container orchestration needs\n2. **Design Kubernetes architecture** appropriate for scale and complexity\n3. **Implement GitOps workflows** with proper repository structure and automation\n4. **Configure security policies** with Pod Security Standards and network policies\n5. **Set up observability stack** with metrics, logs, and traces\n6. **Plan for scalability** with appropriate autoscaling and resource management\n7. **Consider multi-tenancy** requirements and namespace isolation\n8. **Optimize for cost** with right-sizing and efficient resource utilization\n9. **Document platform** with clear operational procedures and developer guides\n\n## Example Interactions\n- \"Design a multi-cluster Kubernetes platform with GitOps for a financial services company\"\n- \"Implement progressive delivery with Argo Rollouts and service mesh traffic splitting\"\n- \"Create a secure multi-tenant Kubernetes platform with namespace isolation and RBAC\"\n- \"Design disaster recovery for stateful applications across multiple Kubernetes clusters\"\n- \"Optimize Kubernetes costs while maintaining performance and availability SLAs\"\n- \"Implement observability stack with Prometheus, Grafana, and OpenTelemetry for microservices\"\n- \"Create CI/CD pipeline with GitOps for container applications with security scanning\"\n- \"Design Kubernetes operator for custom application lifecycle management\""
    }
  ],
  "commands": [],
  "skills": [
    {
      "name": "gitops-workflow",
      "description": "Implement GitOps workflows with ArgoCD and Flux for automated, declarative Kubernetes deployments with continuous reconciliation. Use when implementing GitOps practices, automating Kubernetes deployments, or setting up declarative infrastructure management.",
      "plugin": "kubernetes-operations",
      "source_path": "plugins/kubernetes-operations/skills/gitops-workflow/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "kubernetes",
        "k8s",
        "containers",
        "helm",
        "argocd",
        "gitops"
      ],
      "content": "---\nname: gitops-workflow\ndescription: Implement GitOps workflows with ArgoCD and Flux for automated, declarative Kubernetes deployments with continuous reconciliation. Use when implementing GitOps practices, automating Kubernetes deployments, or setting up declarative infrastructure management.\n---\n\n# GitOps Workflow\n\nComplete guide to implementing GitOps workflows with ArgoCD and Flux for automated Kubernetes deployments.\n\n## Purpose\n\nImplement declarative, Git-based continuous delivery for Kubernetes using ArgoCD or Flux CD, following OpenGitOps principles.\n\n## When to Use This Skill\n\n- Set up GitOps for Kubernetes clusters\n- Automate application deployments from Git\n- Implement progressive delivery strategies\n- Manage multi-cluster deployments\n- Configure automated sync policies\n- Set up secret management in GitOps\n\n## OpenGitOps Principles\n\n1. **Declarative** - Entire system described declaratively\n2. **Versioned and Immutable** - Desired state stored in Git\n3. **Pulled Automatically** - Software agents pull desired state\n4. **Continuously Reconciled** - Agents reconcile actual vs desired state\n\n## ArgoCD Setup\n\n### 1. Installation\n\n```bash\n# Create namespace\nkubectl create namespace argocd\n\n# Install ArgoCD\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n\n# Get admin password\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n```\n\n**Reference:** See `references/argocd-setup.md` for detailed setup\n\n### 2. Repository Structure\n\n```\ngitops-repo/\n\u251c\u2500\u2500 apps/\n\u2502   \u251c\u2500\u2500 production/\n\u2502   \u2502   \u251c\u2500\u2500 app1/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 deployment.yaml\n\u2502   \u2502   \u2514\u2500\u2500 app2/\n\u2502   \u2514\u2500\u2500 staging/\n\u251c\u2500\u2500 infrastructure/\n\u2502   \u251c\u2500\u2500 ingress-nginx/\n\u2502   \u251c\u2500\u2500 cert-manager/\n\u2502   \u2514\u2500\u2500 monitoring/\n\u2514\u2500\u2500 argocd/\n    \u251c\u2500\u2500 applications/\n    \u2514\u2500\u2500 projects/\n```\n\n### 3. Create Application\n\n```yaml\n# argocd/applications/my-app.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-app\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/org/gitops-repo\n    targetRevision: main\n    path: apps/production/my-app\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: production\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n    - CreateNamespace=true\n```\n\n### 4. App of Apps Pattern\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: applications\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/org/gitops-repo\n    targetRevision: main\n    path: argocd/applications\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: argocd\n  syncPolicy:\n    automated: {}\n```\n\n## Flux CD Setup\n\n### 1. Installation\n\n```bash\n# Install Flux CLI\ncurl -s https://fluxcd.io/install.sh | sudo bash\n\n# Bootstrap Flux\nflux bootstrap github \\\n  --owner=org \\\n  --repository=gitops-repo \\\n  --branch=main \\\n  --path=clusters/production \\\n  --personal\n```\n\n### 2. Create GitRepository\n\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  interval: 1m\n  url: https://github.com/org/my-app\n  ref:\n    branch: main\n```\n\n### 3. Create Kustomization\n\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  interval: 5m\n  path: ./deploy\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: my-app\n```\n\n## Sync Policies\n\n### Auto-Sync Configuration\n\n**ArgoCD:**\n```yaml\nsyncPolicy:\n  automated:\n    prune: true      # Delete resources not in Git\n    selfHeal: true   # Reconcile manual changes\n    allowEmpty: false\n  retry:\n    limit: 5\n    backoff:\n      duration: 5s\n      factor: 2\n      maxDuration: 3m\n```\n\n**Flux:**\n```yaml\nspec:\n  interval: 1m\n  prune: true\n  wait: true\n  timeout: 5m\n```\n\n**Reference:** See `references/sync-policies.md`\n\n## Progressive Delivery\n\n### Canary Deployment with ArgoCD Rollouts\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: my-app\nspec:\n  replicas: 5\n  strategy:\n    canary:\n      steps:\n      - setWeight: 20\n      - pause: {duration: 1m}\n      - setWeight: 50\n      - pause: {duration: 2m}\n      - setWeight: 100\n```\n\n### Blue-Green Deployment\n\n```yaml\nstrategy:\n  blueGreen:\n    activeService: my-app\n    previewService: my-app-preview\n    autoPromotionEnabled: false\n```\n\n## Secret Management\n\n### External Secrets Operator\n\n```yaml\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: db-credentials\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: aws-secrets-manager\n    kind: SecretStore\n  target:\n    name: db-credentials\n  data:\n  - secretKey: password\n    remoteRef:\n      key: prod/db/password\n```\n\n### Sealed Secrets\n\n```bash\n# Encrypt secret\nkubeseal --format yaml < secret.yaml > sealed-secret.yaml\n\n# Commit sealed-secret.yaml to Git\n```\n\n## Best Practices\n\n1. **Use separate repos or branches** for different environments\n2. **Implement RBAC** for Git repositories\n3. **Enable notifications** for sync failures\n4. **Use health checks** for custom resources\n5. **Implement approval gates** for production\n6. **Keep secrets out of Git** (use External Secrets)\n7. **Use App of Apps pattern** for organization\n8. **Tag releases** for easy rollback\n9. **Monitor sync status** with alerts\n10. **Test changes** in staging first\n\n## Troubleshooting\n\n**Sync failures:**\n```bash\nargocd app get my-app\nargocd app sync my-app --prune\n```\n\n**Out of sync status:**\n```bash\nargocd app diff my-app\nargocd app sync my-app --force\n```\n\n## Related Skills\n\n- `k8s-manifest-generator` - For creating manifests\n- `helm-chart-scaffolding` - For packaging applications\n",
      "references": {
        "sync-policies.md": "# GitOps Sync Policies\n\n## ArgoCD Sync Policies\n\n### Automated Sync\n```yaml\nsyncPolicy:\n  automated:\n    prune: true       # Delete resources removed from Git\n    selfHeal: true    # Reconcile manual changes\n    allowEmpty: false # Prevent empty sync\n```\n\n### Manual Sync\n```yaml\nsyncPolicy:\n  syncOptions:\n  - PrunePropagationPolicy=foreground\n  - CreateNamespace=true\n```\n\n### Sync Windows\n```yaml\nsyncWindows:\n- kind: allow\n  schedule: \"0 8 * * *\"\n  duration: 1h\n  applications:\n  - my-app\n- kind: deny\n  schedule: \"0 22 * * *\"\n  duration: 8h\n  applications:\n  - '*'\n```\n\n### Retry Policy\n```yaml\nsyncPolicy:\n  retry:\n    limit: 5\n    backoff:\n      duration: 5s\n      factor: 2\n      maxDuration: 3m\n```\n\n## Flux Sync Policies\n\n### Kustomization Sync\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: my-app\nspec:\n  interval: 5m\n  prune: true\n  wait: true\n  timeout: 5m\n  retryInterval: 1m\n  force: false\n```\n\n### Source Sync Interval\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: my-app\nspec:\n  interval: 1m\n  timeout: 60s\n```\n\n## Health Assessment\n\n### Custom Health Checks\n```yaml\n# ArgoCD\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\n  namespace: argocd\ndata:\n  resource.customizations.health.MyCustomResource: |\n    hs = {}\n    if obj.status ~= nil then\n      if obj.status.conditions ~= nil then\n        for i, condition in ipairs(obj.status.conditions) do\n          if condition.type == \"Ready\" and condition.status == \"False\" then\n            hs.status = \"Degraded\"\n            hs.message = condition.message\n            return hs\n          end\n          if condition.type == \"Ready\" and condition.status == \"True\" then\n            hs.status = \"Healthy\"\n            hs.message = condition.message\n            return hs\n          end\n        end\n      end\n    end\n    hs.status = \"Progressing\"\n    hs.message = \"Waiting for status\"\n    return hs\n```\n\n## Sync Options\n\n### Common Sync Options\n- `PrunePropagationPolicy=foreground` - Wait for pruned resources to be deleted\n- `CreateNamespace=true` - Auto-create namespace\n- `Validate=false` - Skip kubectl validation\n- `PruneLast=true` - Prune resources after sync\n- `RespectIgnoreDifferences=true` - Honor ignore differences\n- `ApplyOutOfSyncOnly=true` - Only apply out-of-sync resources\n\n## Best Practices\n\n1. Use automated sync for non-production\n2. Require manual approval for production\n3. Configure sync windows for maintenance\n4. Implement health checks for custom resources\n5. Use selective sync for large applications\n6. Configure appropriate retry policies\n7. Monitor sync failures with alerts\n8. Use prune with caution in production\n9. Test sync policies in staging\n10. Document sync behavior for teams\n",
        "argocd-setup.md": "# ArgoCD Setup and Configuration\n\n## Installation Methods\n\n### 1. Standard Installation\n```bash\nkubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n```\n\n### 2. High Availability Installation\n```bash\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/ha/install.yaml\n```\n\n### 3. Helm Installation\n```bash\nhelm repo add argo https://argoproj.github.io/argo-helm\nhelm install argocd argo/argo-cd -n argocd --create-namespace\n```\n\n## Initial Configuration\n\n### Access ArgoCD UI\n```bash\n# Port forward\nkubectl port-forward svc/argocd-server -n argocd 8080:443\n\n# Get initial admin password\nargocd admin initial-password -n argocd\n```\n\n### Configure Ingress\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argocd-server-ingress\n  namespace: argocd\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: argocd.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: argocd-server\n            port:\n              number: 443\n  tls:\n  - hosts:\n    - argocd.example.com\n    secretName: argocd-secret\n```\n\n## CLI Configuration\n\n### Login\n```bash\nargocd login argocd.example.com --username admin\n```\n\n### Add Repository\n```bash\nargocd repo add https://github.com/org/repo --username user --password token\n```\n\n### Create Application\n```bash\nargocd app create my-app \\\n  --repo https://github.com/org/repo \\\n  --path apps/my-app \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace production\n```\n\n## SSO Configuration\n\n### GitHub OAuth\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\n  namespace: argocd\ndata:\n  url: https://argocd.example.com\n  dex.config: |\n    connectors:\n      - type: github\n        id: github\n        name: GitHub\n        config:\n          clientID: $GITHUB_CLIENT_ID\n          clientSecret: $GITHUB_CLIENT_SECRET\n          orgs:\n          - name: my-org\n```\n\n## RBAC Configuration\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-rbac-cm\n  namespace: argocd\ndata:\n  policy.default: role:readonly\n  policy.csv: |\n    p, role:developers, applications, *, */dev, allow\n    p, role:operators, applications, *, */*, allow\n    g, my-org:devs, role:developers\n    g, my-org:ops, role:operators\n```\n\n## Best Practices\n\n1. Enable SSO for production\n2. Implement RBAC policies\n3. Use separate projects for teams\n4. Enable audit logging\n5. Configure notifications\n6. Use ApplicationSets for multi-cluster\n7. Implement resource hooks\n8. Configure health checks\n9. Use sync windows for maintenance\n10. Monitor with Prometheus metrics\n"
      },
      "assets": {}
    },
    {
      "name": "helm-chart-scaffolding",
      "description": "Design, organize, and manage Helm charts for templating and packaging Kubernetes applications with reusable configurations. Use when creating Helm charts, packaging Kubernetes applications, or implementing templated deployments.",
      "plugin": "kubernetes-operations",
      "source_path": "plugins/kubernetes-operations/skills/helm-chart-scaffolding/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "kubernetes",
        "k8s",
        "containers",
        "helm",
        "argocd",
        "gitops"
      ],
      "content": "---\nname: helm-chart-scaffolding\ndescription: Design, organize, and manage Helm charts for templating and packaging Kubernetes applications with reusable configurations. Use when creating Helm charts, packaging Kubernetes applications, or implementing templated deployments.\n---\n\n# Helm Chart Scaffolding\n\nComprehensive guidance for creating, organizing, and managing Helm charts for packaging and deploying Kubernetes applications.\n\n## Purpose\n\nThis skill provides step-by-step instructions for building production-ready Helm charts, including chart structure, templating patterns, values management, and validation strategies.\n\n## When to Use This Skill\n\nUse this skill when you need to:\n- Create new Helm charts from scratch\n- Package Kubernetes applications for distribution\n- Manage multi-environment deployments with Helm\n- Implement templating for reusable Kubernetes manifests\n- Set up Helm chart repositories\n- Follow Helm best practices and conventions\n\n## Helm Overview\n\n**Helm** is the package manager for Kubernetes that:\n- Templates Kubernetes manifests for reusability\n- Manages application releases and rollbacks\n- Handles dependencies between charts\n- Provides version control for deployments\n- Simplifies configuration management across environments\n\n## Step-by-Step Workflow\n\n### 1. Initialize Chart Structure\n\n**Create new chart:**\n```bash\nhelm create my-app\n```\n\n**Standard chart structure:**\n```\nmy-app/\n\u251c\u2500\u2500 Chart.yaml           # Chart metadata\n\u251c\u2500\u2500 values.yaml          # Default configuration values\n\u251c\u2500\u2500 charts/              # Chart dependencies\n\u251c\u2500\u2500 templates/           # Kubernetes manifest templates\n\u2502   \u251c\u2500\u2500 NOTES.txt       # Post-install notes\n\u2502   \u251c\u2500\u2500 _helpers.tpl    # Template helpers\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u251c\u2500\u2500 ingress.yaml\n\u2502   \u251c\u2500\u2500 serviceaccount.yaml\n\u2502   \u251c\u2500\u2500 hpa.yaml\n\u2502   \u2514\u2500\u2500 tests/\n\u2502       \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 .helmignore         # Files to ignore\n```\n\n### 2. Configure Chart.yaml\n\n**Chart metadata defines the package:**\n\n```yaml\napiVersion: v2\nname: my-app\ndescription: A Helm chart for My Application\ntype: application\nversion: 1.0.0      # Chart version\nappVersion: \"2.1.0\" # Application version\n\n# Keywords for chart discovery\nkeywords:\n  - web\n  - api\n  - backend\n\n# Maintainer information\nmaintainers:\n  - name: DevOps Team\n    email: devops@example.com\n    url: https://github.com/example/my-app\n\n# Source code repository\nsources:\n  - https://github.com/example/my-app\n\n# Homepage\nhome: https://example.com\n\n# Chart icon\nicon: https://example.com/icon.png\n\n# Dependencies\ndependencies:\n  - name: postgresql\n    version: \"12.0.0\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: postgresql.enabled\n  - name: redis\n    version: \"17.0.0\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: redis.enabled\n```\n\n**Reference:** See `assets/Chart.yaml.template` for complete example\n\n### 3. Design values.yaml Structure\n\n**Organize values hierarchically:**\n\n```yaml\n# Image configuration\nimage:\n  repository: myapp\n  tag: \"1.0.0\"\n  pullPolicy: IfNotPresent\n\n# Number of replicas\nreplicaCount: 3\n\n# Service configuration\nservice:\n  type: ClusterIP\n  port: 80\n  targetPort: 8080\n\n# Ingress configuration\ningress:\n  enabled: false\n  className: nginx\n  hosts:\n    - host: app.example.com\n      paths:\n        - path: /\n          pathType: Prefix\n\n# Resources\nresources:\n  requests:\n    memory: \"256Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"512Mi\"\n    cpu: \"500m\"\n\n# Autoscaling\nautoscaling:\n  enabled: false\n  minReplicas: 2\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 80\n\n# Environment variables\nenv:\n  - name: LOG_LEVEL\n    value: \"info\"\n\n# ConfigMap data\nconfigMap:\n  data:\n    APP_MODE: production\n\n# Dependencies\npostgresql:\n  enabled: true\n  auth:\n    database: myapp\n    username: myapp\n\nredis:\n  enabled: false\n```\n\n**Reference:** See `assets/values.yaml.template` for complete structure\n\n### 4. Create Template Files\n\n**Use Go templating with Helm functions:**\n\n**templates/deployment.yaml:**\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"my-app.fullname\" . }}\n  labels:\n    {{- include \"my-app.labels\" . | nindent 4 }}\nspec:\n  {{- if not .Values.autoscaling.enabled }}\n  replicas: {{ .Values.replicaCount }}\n  {{- end }}\n  selector:\n    matchLabels:\n      {{- include \"my-app.selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      labels:\n        {{- include \"my-app.selectorLabels\" . | nindent 8 }}\n    spec:\n      containers:\n      - name: {{ .Chart.Name }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n        imagePullPolicy: {{ .Values.image.pullPolicy }}\n        ports:\n        - name: http\n          containerPort: {{ .Values.service.targetPort }}\n        resources:\n          {{- toYaml .Values.resources | nindent 12 }}\n        env:\n          {{- toYaml .Values.env | nindent 12 }}\n```\n\n### 5. Create Template Helpers\n\n**templates/_helpers.tpl:**\n```yaml\n{{/*\nExpand the name of the chart.\n*/}}\n{{- define \"my-app.name\" -}}\n{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n\n{{/*\nCreate a default fully qualified app name.\n*/}}\n{{- define \"my-app.fullname\" -}}\n{{- if .Values.fullnameOverride }}\n{{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- $name := default .Chart.Name .Values.nameOverride }}\n{{- if contains $name .Release.Name }}\n{{- .Release.Name | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n{{- end }}\n{{- end }}\n\n{{/*\nCommon labels\n*/}}\n{{- define \"my-app.labels\" -}}\nhelm.sh/chart: {{ include \"my-app.chart\" . }}\n{{ include \"my-app.selectorLabels\" . }}\n{{- if .Chart.AppVersion }}\napp.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\n{{- end }}\napp.kubernetes.io/managed-by: {{ .Release.Service }}\n{{- end }}\n\n{{/*\nSelector labels\n*/}}\n{{- define \"my-app.selectorLabels\" -}}\napp.kubernetes.io/name: {{ include \"my-app.name\" . }}\napp.kubernetes.io/instance: {{ .Release.Name }}\n{{- end }}\n```\n\n### 6. Manage Dependencies\n\n**Add dependencies in Chart.yaml:**\n```yaml\ndependencies:\n  - name: postgresql\n    version: \"12.0.0\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: postgresql.enabled\n```\n\n**Update dependencies:**\n```bash\nhelm dependency update\nhelm dependency build\n```\n\n**Override dependency values:**\n```yaml\n# values.yaml\npostgresql:\n  enabled: true\n  auth:\n    database: myapp\n    username: myapp\n    password: changeme\n  primary:\n    persistence:\n      enabled: true\n      size: 10Gi\n```\n\n### 7. Test and Validate\n\n**Validation commands:**\n```bash\n# Lint the chart\nhelm lint my-app/\n\n# Dry-run installation\nhelm install my-app ./my-app --dry-run --debug\n\n# Template rendering\nhelm template my-app ./my-app\n\n# Template with values\nhelm template my-app ./my-app -f values-prod.yaml\n\n# Show computed values\nhelm show values ./my-app\n```\n\n**Validation script:**\n```bash\n#!/bin/bash\nset -e\n\necho \"Linting chart...\"\nhelm lint .\n\necho \"Testing template rendering...\"\nhelm template test-release . --dry-run\n\necho \"Checking for required values...\"\nhelm template test-release . --validate\n\necho \"All validations passed!\"\n```\n\n**Reference:** See `scripts/validate-chart.sh`\n\n### 8. Package and Distribute\n\n**Package the chart:**\n```bash\nhelm package my-app/\n# Creates: my-app-1.0.0.tgz\n```\n\n**Create chart repository:**\n```bash\n# Create index\nhelm repo index .\n\n# Upload to repository\n# AWS S3 example\naws s3 sync . s3://my-helm-charts/ --exclude \"*\" --include \"*.tgz\" --include \"index.yaml\"\n```\n\n**Use the chart:**\n```bash\nhelm repo add my-repo https://charts.example.com\nhelm repo update\nhelm install my-app my-repo/my-app\n```\n\n### 9. Multi-Environment Configuration\n\n**Environment-specific values files:**\n\n```\nmy-app/\n\u251c\u2500\u2500 values.yaml          # Defaults\n\u251c\u2500\u2500 values-dev.yaml      # Development\n\u251c\u2500\u2500 values-staging.yaml  # Staging\n\u2514\u2500\u2500 values-prod.yaml     # Production\n```\n\n**values-prod.yaml:**\n```yaml\nreplicaCount: 5\n\nimage:\n  tag: \"2.1.0\"\n\nresources:\n  requests:\n    memory: \"512Mi\"\n    cpu: \"500m\"\n  limits:\n    memory: \"1Gi\"\n    cpu: \"1000m\"\n\nautoscaling:\n  enabled: true\n  minReplicas: 3\n  maxReplicas: 20\n\ningress:\n  enabled: true\n  hosts:\n    - host: app.example.com\n      paths:\n        - path: /\n          pathType: Prefix\n\npostgresql:\n  enabled: true\n  primary:\n    persistence:\n      size: 100Gi\n```\n\n**Install with environment:**\n```bash\nhelm install my-app ./my-app -f values-prod.yaml --namespace production\n```\n\n### 10. Implement Hooks and Tests\n\n**Pre-install hook:**\n```yaml\n# templates/pre-install-job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"my-app.fullname\" . }}-db-setup\n  annotations:\n    \"helm.sh/hook\": pre-install\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n      - name: db-setup\n        image: postgres:15\n        command: [\"psql\", \"-c\", \"CREATE DATABASE myapp\"]\n      restartPolicy: Never\n```\n\n**Test connection:**\n```yaml\n# templates/tests/test-connection.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: \"{{ include \"my-app.fullname\" . }}-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n  - name: wget\n    image: busybox\n    command: ['wget']\n    args: ['{{ include \"my-app.fullname\" . }}:{{ .Values.service.port }}']\n  restartPolicy: Never\n```\n\n**Run tests:**\n```bash\nhelm test my-app\n```\n\n## Common Patterns\n\n### Pattern 1: Conditional Resources\n\n```yaml\n{{- if .Values.ingress.enabled }}\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ include \"my-app.fullname\" . }}\nspec:\n  # ...\n{{- end }}\n```\n\n### Pattern 2: Iterating Over Lists\n\n```yaml\nenv:\n{{- range .Values.env }}\n- name: {{ .name }}\n  value: {{ .value | quote }}\n{{- end }}\n```\n\n### Pattern 3: Including Files\n\n```yaml\ndata:\n  config.yaml: |\n    {{- .Files.Get \"config/application.yaml\" | nindent 4 }}\n```\n\n### Pattern 4: Global Values\n\n```yaml\nglobal:\n  imageRegistry: docker.io\n  imagePullSecrets:\n    - name: regcred\n\n# Use in templates:\nimage: {{ .Values.global.imageRegistry }}/{{ .Values.image.repository }}\n```\n\n## Best Practices\n\n1. **Use semantic versioning** for chart and app versions\n2. **Document all values** in values.yaml with comments\n3. **Use template helpers** for repeated logic\n4. **Validate charts** before packaging\n5. **Pin dependency versions** explicitly\n6. **Use conditions** for optional resources\n7. **Follow naming conventions** (lowercase, hyphens)\n8. **Include NOTES.txt** with usage instructions\n9. **Add labels** consistently using helpers\n10. **Test installations** in all environments\n\n## Troubleshooting\n\n**Template rendering errors:**\n```bash\nhelm template my-app ./my-app --debug\n```\n\n**Dependency issues:**\n```bash\nhelm dependency update\nhelm dependency list\n```\n\n**Installation failures:**\n```bash\nhelm install my-app ./my-app --dry-run --debug\nkubectl get events --sort-by='.lastTimestamp'\n```\n\n## Reference Files\n\n- `assets/Chart.yaml.template` - Chart metadata template\n- `assets/values.yaml.template` - Values structure template\n- `scripts/validate-chart.sh` - Validation script\n- `references/chart-structure.md` - Detailed chart organization\n\n## Related Skills\n\n- `k8s-manifest-generator` - For creating base Kubernetes manifests\n- `gitops-workflow` - For automated Helm chart deployments\n",
      "references": {
        "chart-structure.md": "# Helm Chart Structure Reference\n\nComplete guide to Helm chart organization, file conventions, and best practices.\n\n## Standard Chart Directory Structure\n\n```\nmy-app/\n\u251c\u2500\u2500 Chart.yaml              # Chart metadata (required)\n\u251c\u2500\u2500 Chart.lock              # Dependency lock file (generated)\n\u251c\u2500\u2500 values.yaml             # Default configuration values (required)\n\u251c\u2500\u2500 values.schema.json      # JSON schema for values validation\n\u251c\u2500\u2500 .helmignore             # Patterns to ignore when packaging\n\u251c\u2500\u2500 README.md               # Chart documentation\n\u251c\u2500\u2500 LICENSE                 # Chart license\n\u251c\u2500\u2500 charts/                 # Chart dependencies (bundled)\n\u2502   \u2514\u2500\u2500 postgresql-12.0.0.tgz\n\u251c\u2500\u2500 crds/                   # Custom Resource Definitions\n\u2502   \u2514\u2500\u2500 my-crd.yaml\n\u251c\u2500\u2500 templates/              # Kubernetes manifest templates (required)\n\u2502   \u251c\u2500\u2500 NOTES.txt          # Post-install instructions\n\u2502   \u251c\u2500\u2500 _helpers.tpl       # Template helper functions\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u251c\u2500\u2500 ingress.yaml\n\u2502   \u251c\u2500\u2500 configmap.yaml\n\u2502   \u251c\u2500\u2500 secret.yaml\n\u2502   \u251c\u2500\u2500 serviceaccount.yaml\n\u2502   \u251c\u2500\u2500 hpa.yaml\n\u2502   \u251c\u2500\u2500 pdb.yaml\n\u2502   \u251c\u2500\u2500 networkpolicy.yaml\n\u2502   \u2514\u2500\u2500 tests/\n\u2502       \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 files/                  # Additional files to include\n    \u2514\u2500\u2500 config/\n        \u2514\u2500\u2500 app.conf\n```\n\n## Chart.yaml Specification\n\n### API Version v2 (Helm 3+)\n\n```yaml\napiVersion: v2                    # Required: API version\nname: my-application              # Required: Chart name\nversion: 1.2.3                    # Required: Chart version (SemVer)\nappVersion: \"2.5.0\"              # Application version\ndescription: A Helm chart for my application  # Required\ntype: application                 # Chart type: application or library\nkeywords:                         # Search keywords\n  - web\n  - api\n  - backend\nhome: https://example.com         # Project home page\nsources:                          # Source code URLs\n  - https://github.com/example/my-app\nmaintainers:                      # Maintainer list\n  - name: John Doe\n    email: john@example.com\n    url: https://github.com/johndoe\nicon: https://example.com/icon.png  # Chart icon URL\nkubeVersion: \">=1.24.0\"          # Compatible Kubernetes versions\ndeprecated: false                 # Mark chart as deprecated\nannotations:                      # Arbitrary annotations\n  example.com/release-notes: https://example.com/releases/v1.2.3\ndependencies:                     # Chart dependencies\n  - name: postgresql\n    version: \"12.0.0\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: postgresql.enabled\n    tags:\n      - database\n    import-values:\n      - child: database\n        parent: database\n    alias: db\n```\n\n## Chart Types\n\n### Application Chart\n```yaml\ntype: application\n```\n- Standard Kubernetes applications\n- Can be installed and managed\n- Contains templates for K8s resources\n\n### Library Chart\n```yaml\ntype: library\n```\n- Shared template helpers\n- Cannot be installed directly\n- Used as dependency by other charts\n- No templates/ directory\n\n## Values Files Organization\n\n### values.yaml (defaults)\n```yaml\n# Global values (shared with subcharts)\nglobal:\n  imageRegistry: docker.io\n  imagePullSecrets: []\n\n# Image configuration\nimage:\n  registry: docker.io\n  repository: myapp/web\n  tag: \"\"  # Defaults to .Chart.AppVersion\n  pullPolicy: IfNotPresent\n\n# Deployment settings\nreplicaCount: 1\nrevisionHistoryLimit: 10\n\n# Pod configuration\npodAnnotations: {}\npodSecurityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n  fsGroup: 1000\n\n# Container security\nsecurityContext:\n  allowPrivilegeEscalation: false\n  readOnlyRootFilesystem: true\n  capabilities:\n    drop:\n    - ALL\n\n# Service\nservice:\n  type: ClusterIP\n  port: 80\n  targetPort: http\n  annotations: {}\n\n# Resources\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n\n# Autoscaling\nautoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 100\n  targetCPUUtilizationPercentage: 80\n\n# Node selection\nnodeSelector: {}\ntolerations: []\naffinity: {}\n\n# Monitoring\nserviceMonitor:\n  enabled: false\n  interval: 30s\n```\n\n### values.schema.json (validation)\n```json\n{\n  \"$schema\": \"https://json-schema.org/draft-07/schema#\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"replicaCount\": {\n      \"type\": \"integer\",\n      \"minimum\": 1\n    },\n    \"image\": {\n      \"type\": \"object\",\n      \"required\": [\"repository\"],\n      \"properties\": {\n        \"repository\": {\n          \"type\": \"string\"\n        },\n        \"tag\": {\n          \"type\": \"string\"\n        },\n        \"pullPolicy\": {\n          \"type\": \"string\",\n          \"enum\": [\"Always\", \"IfNotPresent\", \"Never\"]\n        }\n      }\n    }\n  },\n  \"required\": [\"image\"]\n}\n```\n\n## Template Files\n\n### Template Naming Conventions\n\n- **Lowercase with hyphens**: `deployment.yaml`, `service-account.yaml`\n- **Partial templates**: Prefix with underscore `_helpers.tpl`\n- **Tests**: Place in `templates/tests/`\n- **CRDs**: Place in `crds/` (not templated)\n\n### Common Templates\n\n#### _helpers.tpl\n```yaml\n{{/*\nStandard naming helpers\n*/}}\n{{- define \"my-app.name\" -}}\n{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix \"-\" -}}\n{{- end -}}\n\n{{- define \"my-app.fullname\" -}}\n{{- if .Values.fullnameOverride -}}\n{{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" -}}\n{{- else -}}\n{{- $name := default .Chart.Name .Values.nameOverride -}}\n{{- if contains $name .Release.Name -}}\n{{- .Release.Name | trunc 63 | trimSuffix \"-\" -}}\n{{- else -}}\n{{- printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" -}}\n{{- end -}}\n{{- end -}}\n{{- end -}}\n\n{{- define \"my-app.chart\" -}}\n{{- printf \"%s-%s\" .Chart.Name .Chart.Version | replace \"+\" \"_\" | trunc 63 | trimSuffix \"-\" -}}\n{{- end -}}\n\n{{/*\nCommon labels\n*/}}\n{{- define \"my-app.labels\" -}}\nhelm.sh/chart: {{ include \"my-app.chart\" . }}\n{{ include \"my-app.selectorLabels\" . }}\n{{- if .Chart.AppVersion }}\napp.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\n{{- end }}\napp.kubernetes.io/managed-by: {{ .Release.Service }}\n{{- end -}}\n\n{{- define \"my-app.selectorLabels\" -}}\napp.kubernetes.io/name: {{ include \"my-app.name\" . }}\napp.kubernetes.io/instance: {{ .Release.Name }}\n{{- end -}}\n\n{{/*\nImage name helper\n*/}}\n{{- define \"my-app.image\" -}}\n{{- $registry := .Values.global.imageRegistry | default .Values.image.registry -}}\n{{- $repository := .Values.image.repository -}}\n{{- $tag := .Values.image.tag | default .Chart.AppVersion -}}\n{{- printf \"%s/%s:%s\" $registry $repository $tag -}}\n{{- end -}}\n```\n\n#### NOTES.txt\n```\nThank you for installing {{ .Chart.Name }}.\n\nYour release is named {{ .Release.Name }}.\n\nTo learn more about the release, try:\n\n  $ helm status {{ .Release.Name }}\n  $ helm get all {{ .Release.Name }}\n\n{{- if .Values.ingress.enabled }}\n\nApplication URL:\n{{- range .Values.ingress.hosts }}\n  http{{ if $.Values.ingress.tls }}s{{ end }}://{{ .host }}{{ .path }}\n{{- end }}\n{{- else }}\n\nGet the application URL by running:\n  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l \"app.kubernetes.io/name={{ include \"my-app.name\" . }}\" -o jsonpath=\"{.items[0].metadata.name}\")\n  kubectl port-forward $POD_NAME 8080:80\n  echo \"Visit http://127.0.0.1:8080\"\n{{- end }}\n```\n\n## Dependencies Management\n\n### Declaring Dependencies\n\n```yaml\n# Chart.yaml\ndependencies:\n  - name: postgresql\n    version: \"12.0.0\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: postgresql.enabled  # Enable/disable via values\n    tags:                          # Group dependencies\n      - database\n    import-values:                 # Import values from subchart\n      - child: database\n        parent: database\n    alias: db                      # Reference as .Values.db\n```\n\n### Managing Dependencies\n\n```bash\n# Update dependencies\nhelm dependency update\n\n# List dependencies\nhelm dependency list\n\n# Build dependencies\nhelm dependency build\n```\n\n### Chart.lock\n\nGenerated automatically by `helm dependency update`:\n\n```yaml\ndependencies:\n- name: postgresql\n  repository: https://charts.bitnami.com/bitnami\n  version: 12.0.0\ndigest: sha256:abcd1234...\ngenerated: \"2024-01-01T00:00:00Z\"\n```\n\n## .helmignore\n\nExclude files from chart package:\n\n```\n# Development files\n.git/\n.gitignore\n*.md\ndocs/\n\n# Build artifacts\n*.swp\n*.bak\n*.tmp\n*.orig\n\n# CI/CD\n.travis.yml\n.gitlab-ci.yml\nJenkinsfile\n\n# Testing\ntest/\n*.test\n\n# IDE\n.vscode/\n.idea/\n*.iml\n```\n\n## Custom Resource Definitions (CRDs)\n\nPlace CRDs in `crds/` directory:\n\n```\ncrds/\n\u251c\u2500\u2500 my-app-crd.yaml\n\u2514\u2500\u2500 another-crd.yaml\n```\n\n**Important CRD notes:**\n- CRDs are installed before any templates\n- CRDs are NOT templated (no `{{ }}` syntax)\n- CRDs are NOT upgraded or deleted with chart\n- Use `helm install --skip-crds` to skip installation\n\n## Chart Versioning\n\n### Semantic Versioning\n\n- **Chart Version**: Increment when chart changes\n  - MAJOR: Breaking changes\n  - MINOR: New features, backward compatible\n  - PATCH: Bug fixes\n\n- **App Version**: Application version being deployed\n  - Can be any string\n  - Not required to follow SemVer\n\n```yaml\nversion: 2.3.1      # Chart version\nappVersion: \"1.5.0\" # Application version\n```\n\n## Chart Testing\n\n### Test Files\n\n```yaml\n# templates/tests/test-connection.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: \"{{ include \"my-app.fullname\" . }}-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox\n    command: ['wget']\n    args: ['{{ include \"my-app.fullname\" . }}:{{ .Values.service.port }}']\n  restartPolicy: Never\n```\n\n### Running Tests\n\n```bash\nhelm test my-release\nhelm test my-release --logs\n```\n\n## Hooks\n\nHelm hooks allow intervention at specific points:\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"my-app.fullname\" . }}-migration\n  annotations:\n    \"helm.sh/hook\": pre-upgrade,pre-install\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n```\n\n### Hook Types\n\n- `pre-install`: Before templates rendered\n- `post-install`: After all resources loaded\n- `pre-delete`: Before any resources deleted\n- `post-delete`: After all resources deleted\n- `pre-upgrade`: Before upgrade\n- `post-upgrade`: After upgrade\n- `pre-rollback`: Before rollback\n- `post-rollback`: After rollback\n- `test`: Run with `helm test`\n\n### Hook Weight\n\nControls hook execution order (-5 to 5, lower runs first)\n\n### Hook Deletion Policies\n\n- `before-hook-creation`: Delete previous hook before new one\n- `hook-succeeded`: Delete after successful execution\n- `hook-failed`: Delete if hook fails\n\n## Best Practices\n\n1. **Use helpers** for repeated template logic\n2. **Quote strings** in templates: `{{ .Values.name | quote }}`\n3. **Validate values** with values.schema.json\n4. **Document all values** in values.yaml\n5. **Use semantic versioning** for chart versions\n6. **Pin dependency versions** exactly\n7. **Include NOTES.txt** with usage instructions\n8. **Add tests** for critical functionality\n9. **Use hooks** for database migrations\n10. **Keep charts focused** - one application per chart\n\n## Chart Repository Structure\n\n```\nhelm-charts/\n\u251c\u2500\u2500 index.yaml\n\u251c\u2500\u2500 my-app-1.0.0.tgz\n\u251c\u2500\u2500 my-app-1.1.0.tgz\n\u251c\u2500\u2500 my-app-1.2.0.tgz\n\u2514\u2500\u2500 another-chart-2.0.0.tgz\n```\n\n### Creating Repository Index\n\n```bash\nhelm repo index . --url https://charts.example.com\n```\n\n## Related Resources\n\n- [Helm Documentation](https://helm.sh/docs/)\n- [Chart Template Guide](https://helm.sh/docs/chart_template_guide/)\n- [Best Practices](https://helm.sh/docs/chart_best_practices/)\n"
      },
      "assets": {}
    },
    {
      "name": "k8s-manifest-generator",
      "description": "Create production-ready Kubernetes manifests for Deployments, Services, ConfigMaps, and Secrets following best practices and security standards. Use when generating Kubernetes YAML manifests, creating K8s resources, or implementing production-grade Kubernetes configurations.",
      "plugin": "kubernetes-operations",
      "source_path": "plugins/kubernetes-operations/skills/k8s-manifest-generator/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "kubernetes",
        "k8s",
        "containers",
        "helm",
        "argocd",
        "gitops"
      ],
      "content": "---\nname: k8s-manifest-generator\ndescription: Create production-ready Kubernetes manifests for Deployments, Services, ConfigMaps, and Secrets following best practices and security standards. Use when generating Kubernetes YAML manifests, creating K8s resources, or implementing production-grade Kubernetes configurations.\n---\n\n# Kubernetes Manifest Generator\n\nStep-by-step guidance for creating production-ready Kubernetes manifests including Deployments, Services, ConfigMaps, Secrets, and PersistentVolumeClaims.\n\n## Purpose\n\nThis skill provides comprehensive guidance for generating well-structured, secure, and production-ready Kubernetes manifests following cloud-native best practices and Kubernetes conventions.\n\n## When to Use This Skill\n\nUse this skill when you need to:\n- Create new Kubernetes Deployment manifests\n- Define Service resources for network connectivity\n- Generate ConfigMap and Secret resources for configuration management\n- Create PersistentVolumeClaim manifests for stateful workloads\n- Follow Kubernetes best practices and naming conventions\n- Implement resource limits, health checks, and security contexts\n- Design manifests for multi-environment deployments\n\n## Step-by-Step Workflow\n\n### 1. Gather Requirements\n\n**Understand the workload:**\n- Application type (stateless/stateful)\n- Container image and version\n- Environment variables and configuration needs\n- Storage requirements\n- Network exposure requirements (internal/external)\n- Resource requirements (CPU, memory)\n- Scaling requirements\n- Health check endpoints\n\n**Questions to ask:**\n- What is the application name and purpose?\n- What container image and tag will be used?\n- Does the application need persistent storage?\n- What ports does the application expose?\n- Are there any secrets or configuration files needed?\n- What are the CPU and memory requirements?\n- Does the application need to be exposed externally?\n\n### 2. Create Deployment Manifest\n\n**Follow this structure:**\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: <app-name>\n  namespace: <namespace>\n  labels:\n    app: <app-name>\n    version: <version>\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: <app-name>\n  template:\n    metadata:\n      labels:\n        app: <app-name>\n        version: <version>\n    spec:\n      containers:\n      - name: <container-name>\n        image: <image>:<tag>\n        ports:\n        - containerPort: <port>\n          name: http\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: http\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        env:\n        - name: ENV_VAR\n          value: \"value\"\n        envFrom:\n        - configMapRef:\n            name: <app-name>-config\n        - secretRef:\n            name: <app-name>-secret\n```\n\n**Best practices to apply:**\n- Always set resource requests and limits\n- Implement both liveness and readiness probes\n- Use specific image tags (never `:latest`)\n- Apply security context for non-root users\n- Use labels for organization and selection\n- Set appropriate replica count based on availability needs\n\n**Reference:** See `references/deployment-spec.md` for detailed deployment options\n\n### 3. Create Service Manifest\n\n**Choose the appropriate Service type:**\n\n**ClusterIP (internal only):**\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: <app-name>\n  namespace: <namespace>\n  labels:\n    app: <app-name>\nspec:\n  type: ClusterIP\n  selector:\n    app: <app-name>\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    protocol: TCP\n```\n\n**LoadBalancer (external access):**\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: <app-name>\n  namespace: <namespace>\n  labels:\n    app: <app-name>\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\nspec:\n  type: LoadBalancer\n  selector:\n    app: <app-name>\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    protocol: TCP\n```\n\n**Reference:** See `references/service-spec.md` for service types and networking\n\n### 4. Create ConfigMap\n\n**For application configuration:**\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: <app-name>-config\n  namespace: <namespace>\ndata:\n  APP_MODE: production\n  LOG_LEVEL: info\n  DATABASE_HOST: db.example.com\n  # For config files\n  app.properties: |\n    server.port=8080\n    server.host=0.0.0.0\n    logging.level=INFO\n```\n\n**Best practices:**\n- Use ConfigMaps for non-sensitive data only\n- Organize related configuration together\n- Use meaningful names for keys\n- Consider using one ConfigMap per component\n- Version ConfigMaps when making changes\n\n**Reference:** See `assets/configmap-template.yaml` for examples\n\n### 5. Create Secret\n\n**For sensitive data:**\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: <app-name>-secret\n  namespace: <namespace>\ntype: Opaque\nstringData:\n  DATABASE_PASSWORD: \"changeme\"\n  API_KEY: \"secret-api-key\"\n  # For certificate files\n  tls.crt: |\n    -----BEGIN CERTIFICATE-----\n    ...\n    -----END CERTIFICATE-----\n  tls.key: |\n    -----BEGIN PRIVATE KEY-----\n    ...\n    -----END PRIVATE KEY-----\n```\n\n**Security considerations:**\n- Never commit secrets to Git in plain text\n- Use Sealed Secrets, External Secrets Operator, or Vault\n- Rotate secrets regularly\n- Use RBAC to limit secret access\n- Consider using Secret type: `kubernetes.io/tls` for TLS secrets\n\n### 6. Create PersistentVolumeClaim (if needed)\n\n**For stateful applications:**\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: <app-name>-data\n  namespace: <namespace>\nspec:\n  accessModes:\n  - ReadWriteOnce\n  storageClassName: gp3\n  resources:\n    requests:\n      storage: 10Gi\n```\n\n**Mount in Deployment:**\n```yaml\nspec:\n  template:\n    spec:\n      containers:\n      - name: app\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/app\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: <app-name>-data\n```\n\n**Storage considerations:**\n- Choose appropriate StorageClass for performance needs\n- Use ReadWriteOnce for single-pod access\n- Use ReadWriteMany for multi-pod shared storage\n- Consider backup strategies\n- Set appropriate retention policies\n\n### 7. Apply Security Best Practices\n\n**Add security context to Deployment:**\n\n```yaml\nspec:\n  template:\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        fsGroup: 1000\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: app\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n```\n\n**Security checklist:**\n- [ ] Run as non-root user\n- [ ] Drop all capabilities\n- [ ] Use read-only root filesystem\n- [ ] Disable privilege escalation\n- [ ] Set seccomp profile\n- [ ] Use Pod Security Standards\n\n### 8. Add Labels and Annotations\n\n**Standard labels (recommended):**\n\n```yaml\nmetadata:\n  labels:\n    app.kubernetes.io/name: <app-name>\n    app.kubernetes.io/instance: <instance-name>\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/component: backend\n    app.kubernetes.io/part-of: <system-name>\n    app.kubernetes.io/managed-by: kubectl\n```\n\n**Useful annotations:**\n\n```yaml\nmetadata:\n  annotations:\n    description: \"Application description\"\n    contact: \"team@example.com\"\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9090\"\n    prometheus.io/path: \"/metrics\"\n```\n\n### 9. Organize Multi-Resource Manifests\n\n**File organization options:**\n\n**Option 1: Single file with `---` separator**\n```yaml\n# app-name.yaml\n---\napiVersion: v1\nkind: ConfigMap\n...\n---\napiVersion: v1\nkind: Secret\n...\n---\napiVersion: apps/v1\nkind: Deployment\n...\n---\napiVersion: v1\nkind: Service\n...\n```\n\n**Option 2: Separate files**\n```\nmanifests/\n\u251c\u2500\u2500 configmap.yaml\n\u251c\u2500\u2500 secret.yaml\n\u251c\u2500\u2500 deployment.yaml\n\u251c\u2500\u2500 service.yaml\n\u2514\u2500\u2500 pvc.yaml\n```\n\n**Option 3: Kustomize structure**\n```\nbase/\n\u251c\u2500\u2500 kustomization.yaml\n\u251c\u2500\u2500 deployment.yaml\n\u251c\u2500\u2500 service.yaml\n\u2514\u2500\u2500 configmap.yaml\noverlays/\n\u251c\u2500\u2500 dev/\n\u2502   \u2514\u2500\u2500 kustomization.yaml\n\u2514\u2500\u2500 prod/\n    \u2514\u2500\u2500 kustomization.yaml\n```\n\n### 10. Validate and Test\n\n**Validation steps:**\n\n```bash\n# Dry-run validation\nkubectl apply -f manifest.yaml --dry-run=client\n\n# Server-side validation\nkubectl apply -f manifest.yaml --dry-run=server\n\n# Validate with kubeval\nkubeval manifest.yaml\n\n# Validate with kube-score\nkube-score score manifest.yaml\n\n# Check with kube-linter\nkube-linter lint manifest.yaml\n```\n\n**Testing checklist:**\n- [ ] Manifest passes dry-run validation\n- [ ] All required fields are present\n- [ ] Resource limits are reasonable\n- [ ] Health checks are configured\n- [ ] Security context is set\n- [ ] Labels follow conventions\n- [ ] Namespace exists or is created\n\n## Common Patterns\n\n### Pattern 1: Simple Stateless Web Application\n\n**Use case:** Standard web API or microservice\n\n**Components needed:**\n- Deployment (3 replicas for HA)\n- ClusterIP Service\n- ConfigMap for configuration\n- Secret for API keys\n- HorizontalPodAutoscaler (optional)\n\n**Reference:** See `assets/deployment-template.yaml`\n\n### Pattern 2: Stateful Database Application\n\n**Use case:** Database or persistent storage application\n\n**Components needed:**\n- StatefulSet (not Deployment)\n- Headless Service\n- PersistentVolumeClaim template\n- ConfigMap for DB configuration\n- Secret for credentials\n\n### Pattern 3: Background Job or Cron\n\n**Use case:** Scheduled tasks or batch processing\n\n**Components needed:**\n- CronJob or Job\n- ConfigMap for job parameters\n- Secret for credentials\n- ServiceAccount with RBAC\n\n### Pattern 4: Multi-Container Pod\n\n**Use case:** Application with sidecar containers\n\n**Components needed:**\n- Deployment with multiple containers\n- Shared volumes between containers\n- Init containers for setup\n- Service (if needed)\n\n## Templates\n\nThe following templates are available in the `assets/` directory:\n\n- `deployment-template.yaml` - Standard deployment with best practices\n- `service-template.yaml` - Service configurations (ClusterIP, LoadBalancer, NodePort)\n- `configmap-template.yaml` - ConfigMap examples with different data types\n- `secret-template.yaml` - Secret examples (to be generated, not committed)\n- `pvc-template.yaml` - PersistentVolumeClaim templates\n\n## Reference Documentation\n\n- `references/deployment-spec.md` - Detailed Deployment specification\n- `references/service-spec.md` - Service types and networking details\n\n## Best Practices Summary\n\n1. **Always set resource requests and limits** - Prevents resource starvation\n2. **Implement health checks** - Ensures Kubernetes can manage your application\n3. **Use specific image tags** - Avoid unpredictable deployments\n4. **Apply security contexts** - Run as non-root, drop capabilities\n5. **Use ConfigMaps and Secrets** - Separate config from code\n6. **Label everything** - Enables filtering and organization\n7. **Follow naming conventions** - Use standard Kubernetes labels\n8. **Validate before applying** - Use dry-run and validation tools\n9. **Version your manifests** - Keep in Git with version control\n10. **Document with annotations** - Add context for other developers\n\n## Troubleshooting\n\n**Pods not starting:**\n- Check image pull errors: `kubectl describe pod <pod-name>`\n- Verify resource availability: `kubectl get nodes`\n- Check events: `kubectl get events --sort-by='.lastTimestamp'`\n\n**Service not accessible:**\n- Verify selector matches pod labels: `kubectl get endpoints <service-name>`\n- Check service type and port configuration\n- Test from within cluster: `kubectl run debug --rm -it --image=busybox -- sh`\n\n**ConfigMap/Secret not loading:**\n- Verify names match in Deployment\n- Check namespace\n- Ensure resources exist: `kubectl get configmap,secret`\n\n## Next Steps\n\nAfter creating manifests:\n1. Store in Git repository\n2. Set up CI/CD pipeline for deployment\n3. Consider using Helm or Kustomize for templating\n4. Implement GitOps with ArgoCD or Flux\n5. Add monitoring and observability\n\n## Related Skills\n\n- `helm-chart-scaffolding` - For templating and packaging\n- `gitops-workflow` - For automated deployments\n- `k8s-security-policies` - For advanced security configurations\n",
      "references": {
        "deployment-spec.md": "# Kubernetes Deployment Specification Reference\n\nComprehensive reference for Kubernetes Deployment resources, covering all key fields, best practices, and common patterns.\n\n## Overview\n\nA Deployment provides declarative updates for Pods and ReplicaSets. It manages the desired state of your application, handling rollouts, rollbacks, and scaling operations.\n\n## Complete Deployment Specification\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\n  namespace: production\n  labels:\n    app.kubernetes.io/name: my-app\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/component: backend\n    app.kubernetes.io/part-of: my-system\n  annotations:\n    description: \"Main application deployment\"\n    contact: \"backend-team@example.com\"\nspec:\n  # Replica management\n  replicas: 3\n  revisionHistoryLimit: 10\n\n  # Pod selection\n  selector:\n    matchLabels:\n      app: my-app\n      version: v1\n\n  # Update strategy\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n\n  # Minimum time for pod to be ready\n  minReadySeconds: 10\n\n  # Deployment will fail if it doesn't progress in this time\n  progressDeadlineSeconds: 600\n\n  # Pod template\n  template:\n    metadata:\n      labels:\n        app: my-app\n        version: v1\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"9090\"\n    spec:\n      # Service account for RBAC\n      serviceAccountName: my-app\n\n      # Security context for the pod\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        fsGroup: 1000\n        seccompProfile:\n          type: RuntimeDefault\n\n      # Init containers run before main containers\n      initContainers:\n      - name: init-db\n        image: busybox:1.36\n        command: ['sh', '-c', 'until nc -z db-service 5432; do sleep 1; done']\n        securityContext:\n          allowPrivilegeEscalation: false\n          runAsNonRoot: true\n          runAsUser: 1000\n\n      # Main containers\n      containers:\n      - name: app\n        image: myapp:1.0.0\n        imagePullPolicy: IfNotPresent\n\n        # Container ports\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        - name: metrics\n          containerPort: 9090\n          protocol: TCP\n\n        # Environment variables\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: url\n\n        # ConfigMap and Secret references\n        envFrom:\n        - configMapRef:\n            name: app-config\n        - secretRef:\n            name: app-secrets\n\n        # Resource requests and limits\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n\n        # Liveness probe\n        livenessProbe:\n          httpGet:\n            path: /health/live\n            port: http\n            httpHeaders:\n            - name: Custom-Header\n              value: Awesome\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n\n        # Readiness probe\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n\n        # Startup probe (for slow-starting containers)\n        startupProbe:\n          httpGet:\n            path: /health/startup\n            port: http\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 30\n\n        # Volume mounts\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/app\n        - name: config\n          mountPath: /etc/app\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n\n        # Security context for container\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          capabilities:\n            drop:\n            - ALL\n\n        # Lifecycle hooks\n        lifecycle:\n          postStart:\n            exec:\n              command: [\"/bin/sh\", \"-c\", \"echo Container started > /tmp/started\"]\n          preStop:\n            exec:\n              command: [\"/bin/sh\", \"-c\", \"sleep 15\"]\n\n      # Volumes\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: app-data\n      - name: config\n        configMap:\n          name: app-config\n      - name: tmp\n        emptyDir: {}\n\n      # DNS configuration\n      dnsPolicy: ClusterFirst\n      dnsConfig:\n        options:\n        - name: ndots\n          value: \"2\"\n\n      # Scheduling\n      nodeSelector:\n        disktype: ssd\n\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - my-app\n              topologyKey: kubernetes.io/hostname\n\n      tolerations:\n      - key: \"app\"\n        operator: \"Equal\"\n        value: \"my-app\"\n        effect: \"NoSchedule\"\n\n      # Termination\n      terminationGracePeriodSeconds: 30\n\n      # Image pull secrets\n      imagePullSecrets:\n      - name: regcred\n```\n\n## Field Reference\n\n### Metadata Fields\n\n#### Required Fields\n- `apiVersion`: `apps/v1` (current stable version)\n- `kind`: `Deployment`\n- `metadata.name`: Unique name within namespace\n\n#### Recommended Metadata\n- `metadata.namespace`: Target namespace (defaults to `default`)\n- `metadata.labels`: Key-value pairs for organization\n- `metadata.annotations`: Non-identifying metadata\n\n### Spec Fields\n\n#### Replica Management\n\n**`replicas`** (integer, default: 1)\n- Number of desired pod instances\n- Best practice: Use 3+ for production high availability\n- Can be scaled manually or via HorizontalPodAutoscaler\n\n**`revisionHistoryLimit`** (integer, default: 10)\n- Number of old ReplicaSets to retain for rollback\n- Set to 0 to disable rollback capability\n- Reduces storage overhead for long-running deployments\n\n#### Update Strategy\n\n**`strategy.type`** (string)\n- `RollingUpdate` (default): Gradual pod replacement\n- `Recreate`: Delete all pods before creating new ones\n\n**`strategy.rollingUpdate.maxSurge`** (int or percent, default: 25%)\n- Maximum pods above desired replicas during update\n- Example: With 3 replicas and maxSurge=1, up to 4 pods during update\n\n**`strategy.rollingUpdate.maxUnavailable`** (int or percent, default: 25%)\n- Maximum pods below desired replicas during update\n- Set to 0 for zero-downtime deployments\n- Cannot be 0 if maxSurge is 0\n\n**Best practices:**\n```yaml\n# Zero-downtime deployment\nstrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxSurge: 1\n    maxUnavailable: 0\n\n# Fast deployment (can have brief downtime)\nstrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxSurge: 2\n    maxUnavailable: 1\n\n# Complete replacement\nstrategy:\n  type: Recreate\n```\n\n#### Pod Template\n\n**`template.metadata.labels`**\n- Must include labels matching `spec.selector.matchLabels`\n- Add version labels for blue/green deployments\n- Include standard Kubernetes labels\n\n**`template.spec.containers`** (required)\n- Array of container specifications\n- At least one container required\n- Each container needs unique name\n\n#### Container Configuration\n\n**Image Management:**\n```yaml\ncontainers:\n- name: app\n  image: registry.example.com/myapp:1.0.0\n  imagePullPolicy: IfNotPresent  # or Always, Never\n```\n\nImage pull policies:\n- `IfNotPresent`: Pull if not cached (default for tagged images)\n- `Always`: Always pull (default for :latest)\n- `Never`: Never pull, fail if not cached\n\n**Port Declarations:**\n```yaml\nports:\n- name: http      # Named for referencing in Service\n  containerPort: 8080\n  protocol: TCP   # TCP (default), UDP, or SCTP\n  hostPort: 8080  # Optional: Bind to host port (rarely used)\n```\n\n#### Resource Management\n\n**Requests vs Limits:**\n\n```yaml\nresources:\n  requests:\n    memory: \"256Mi\"  # Guaranteed resources\n    cpu: \"250m\"      # 0.25 CPU cores\n  limits:\n    memory: \"512Mi\"  # Maximum allowed\n    cpu: \"500m\"      # 0.5 CPU cores\n```\n\n**QoS Classes (determined automatically):**\n\n1. **Guaranteed**: requests = limits for all containers\n   - Highest priority\n   - Last to be evicted\n\n2. **Burstable**: requests < limits or only requests set\n   - Medium priority\n   - Evicted before Guaranteed\n\n3. **BestEffort**: No requests or limits set\n   - Lowest priority\n   - First to be evicted\n\n**Best practices:**\n- Always set requests in production\n- Set limits to prevent resource monopolization\n- Memory limits should be 1.5-2x requests\n- CPU limits can be higher for bursty workloads\n\n#### Health Checks\n\n**Probe Types:**\n\n1. **startupProbe** - For slow-starting applications\n   ```yaml\n   startupProbe:\n     httpGet:\n       path: /health/startup\n       port: 8080\n     initialDelaySeconds: 0\n     periodSeconds: 10\n     failureThreshold: 30  # 5 minutes to start (10s * 30)\n   ```\n\n2. **livenessProbe** - Restarts unhealthy containers\n   ```yaml\n   livenessProbe:\n     httpGet:\n       path: /health/live\n       port: 8080\n     initialDelaySeconds: 30\n     periodSeconds: 10\n     timeoutSeconds: 5\n     failureThreshold: 3  # Restart after 3 failures\n   ```\n\n3. **readinessProbe** - Controls traffic routing\n   ```yaml\n   readinessProbe:\n     httpGet:\n       path: /health/ready\n       port: 8080\n     initialDelaySeconds: 5\n     periodSeconds: 5\n     failureThreshold: 3  # Remove from service after 3 failures\n   ```\n\n**Probe Mechanisms:**\n\n```yaml\n# HTTP GET\nhttpGet:\n  path: /health\n  port: 8080\n  httpHeaders:\n  - name: Authorization\n    value: Bearer token\n\n# TCP Socket\ntcpSocket:\n  port: 3306\n\n# Command execution\nexec:\n  command:\n  - cat\n  - /tmp/healthy\n\n# gRPC (Kubernetes 1.24+)\ngrpc:\n  port: 9090\n  service: my.service.health.v1.Health\n```\n\n**Probe Timing Parameters:**\n\n- `initialDelaySeconds`: Wait before first probe\n- `periodSeconds`: How often to probe\n- `timeoutSeconds`: Probe timeout\n- `successThreshold`: Successes needed to mark healthy (1 for liveness/startup)\n- `failureThreshold`: Failures before taking action\n\n#### Security Context\n\n**Pod-level security context:**\n```yaml\nspec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n    runAsGroup: 1000\n    fsGroup: 1000\n    fsGroupChangePolicy: OnRootMismatch\n    seccompProfile:\n      type: RuntimeDefault\n```\n\n**Container-level security context:**\n```yaml\ncontainers:\n- name: app\n  securityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    runAsNonRoot: true\n    runAsUser: 1000\n    capabilities:\n      drop:\n      - ALL\n      add:\n      - NET_BIND_SERVICE  # Only if needed\n```\n\n**Security best practices:**\n- Always run as non-root (`runAsNonRoot: true`)\n- Drop all capabilities and add only needed ones\n- Use read-only root filesystem when possible\n- Enable seccomp profile\n- Disable privilege escalation\n\n#### Volumes\n\n**Volume Types:**\n\n```yaml\nvolumes:\n# PersistentVolumeClaim\n- name: data\n  persistentVolumeClaim:\n    claimName: app-data\n\n# ConfigMap\n- name: config\n  configMap:\n    name: app-config\n    items:\n    - key: app.properties\n      path: application.properties\n\n# Secret\n- name: secrets\n  secret:\n    secretName: app-secrets\n    defaultMode: 0400\n\n# EmptyDir (ephemeral)\n- name: cache\n  emptyDir:\n    sizeLimit: 1Gi\n\n# HostPath (avoid in production)\n- name: host-data\n  hostPath:\n    path: /data\n    type: DirectoryOrCreate\n```\n\n#### Scheduling\n\n**Node Selection:**\n\n```yaml\n# Simple node selector\nnodeSelector:\n  disktype: ssd\n  zone: us-west-1a\n\n# Node affinity (more expressive)\naffinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/arch\n          operator: In\n          values:\n          - amd64\n          - arm64\n```\n\n**Pod Affinity/Anti-Affinity:**\n\n```yaml\n# Spread pods across nodes\naffinity:\n  podAntiAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n    - labelSelector:\n        matchLabels:\n          app: my-app\n      topologyKey: kubernetes.io/hostname\n\n# Co-locate with database\naffinity:\n  podAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n    - weight: 100\n      podAffinityTerm:\n        labelSelector:\n          matchLabels:\n            app: database\n        topologyKey: kubernetes.io/hostname\n```\n\n**Tolerations:**\n\n```yaml\ntolerations:\n- key: \"node.kubernetes.io/unreachable\"\n  operator: \"Exists\"\n  effect: \"NoExecute\"\n  tolerationSeconds: 30\n- key: \"dedicated\"\n  operator: \"Equal\"\n  value: \"database\"\n  effect: \"NoSchedule\"\n```\n\n## Common Patterns\n\n### High Availability Deployment\n\n```yaml\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: my-app\n            topologyKey: kubernetes.io/hostname\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: topology.kubernetes.io/zone\n        whenUnsatisfiable: DoNotSchedule\n        labelSelector:\n          matchLabels:\n            app: my-app\n```\n\n### Sidecar Container Pattern\n\n```yaml\nspec:\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myapp:1.0.0\n        volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log\n      - name: log-forwarder\n        image: fluent-bit:2.0\n        volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log\n          readOnly: true\n      volumes:\n      - name: shared-logs\n        emptyDir: {}\n```\n\n### Init Container for Dependencies\n\n```yaml\nspec:\n  template:\n    spec:\n      initContainers:\n      - name: wait-for-db\n        image: busybox:1.36\n        command:\n        - sh\n        - -c\n        - |\n          until nc -z database-service 5432; do\n            echo \"Waiting for database...\"\n            sleep 2\n          done\n      - name: run-migrations\n        image: myapp:1.0.0\n        command: [\"./migrate\", \"up\"]\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: url\n      containers:\n      - name: app\n        image: myapp:1.0.0\n```\n\n## Best Practices\n\n### Production Checklist\n\n- [ ] Set resource requests and limits\n- [ ] Implement all three probe types (startup, liveness, readiness)\n- [ ] Use specific image tags (not :latest)\n- [ ] Configure security context (non-root, read-only filesystem)\n- [ ] Set replica count >= 3 for HA\n- [ ] Configure pod anti-affinity for spread\n- [ ] Set appropriate update strategy (maxUnavailable: 0 for zero-downtime)\n- [ ] Use ConfigMaps and Secrets for configuration\n- [ ] Add standard labels and annotations\n- [ ] Configure graceful shutdown (preStop hook, terminationGracePeriodSeconds)\n- [ ] Set revisionHistoryLimit for rollback capability\n- [ ] Use ServiceAccount with minimal RBAC permissions\n\n### Performance Tuning\n\n**Fast startup:**\n```yaml\nspec:\n  minReadySeconds: 5\n  strategy:\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n```\n\n**Zero-downtime updates:**\n```yaml\nspec:\n  minReadySeconds: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n```\n\n**Graceful shutdown:**\n```yaml\nspec:\n  template:\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: app\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\", \"-c\", \"sleep 15 && kill -SIGTERM 1\"]\n```\n\n## Troubleshooting\n\n### Common Issues\n\n**Pods not starting:**\n```bash\nkubectl describe deployment <name>\nkubectl get pods -l app=<app-name>\nkubectl describe pod <pod-name>\nkubectl logs <pod-name>\n```\n\n**ImagePullBackOff:**\n- Check image name and tag\n- Verify imagePullSecrets\n- Check registry credentials\n\n**CrashLoopBackOff:**\n- Check container logs\n- Verify liveness probe is not too aggressive\n- Check resource limits\n- Verify application dependencies\n\n**Deployment stuck in progress:**\n- Check progressDeadlineSeconds\n- Verify readiness probes\n- Check resource availability\n\n## Related Resources\n\n- [Kubernetes Deployment API Reference](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#deployment-v1-apps)\n- [Pod Security Standards](https://kubernetes.io/docs/concepts/security/pod-security-standards/)\n- [Resource Management](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)\n",
        "service-spec.md": "# Kubernetes Service Specification Reference\n\nComprehensive reference for Kubernetes Service resources, covering service types, networking, load balancing, and service discovery patterns.\n\n## Overview\n\nA Service provides stable network endpoints for accessing Pods. Services enable loose coupling between microservices by providing service discovery and load balancing.\n\n## Service Types\n\n### 1. ClusterIP (Default)\n\nExposes the service on an internal cluster IP. Only reachable from within the cluster.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend-service\n  namespace: production\nspec:\n  type: ClusterIP\n  selector:\n    app: backend\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    protocol: TCP\n  sessionAffinity: None\n```\n\n**Use cases:**\n- Internal microservice communication\n- Database services\n- Internal APIs\n- Message queues\n\n### 2. NodePort\n\nExposes the service on each Node's IP at a static port (30000-32767 range).\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend-service\nspec:\n  type: NodePort\n  selector:\n    app: frontend\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    nodePort: 30080  # Optional, auto-assigned if omitted\n    protocol: TCP\n```\n\n**Use cases:**\n- Development/testing external access\n- Small deployments without load balancer\n- Direct node access requirements\n\n**Limitations:**\n- Limited port range (30000-32767)\n- Must handle node failures\n- No built-in load balancing across nodes\n\n### 3. LoadBalancer\n\nExposes the service using a cloud provider's load balancer.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: public-api\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n    service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: api\n  ports:\n  - name: https\n    port: 443\n    targetPort: 8443\n    protocol: TCP\n  loadBalancerSourceRanges:\n  - 203.0.113.0/24\n```\n\n**Cloud-specific annotations:**\n\n**AWS:**\n```yaml\nannotations:\n  service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"  # or \"external\"\n  service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\n  service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \"true\"\n  service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:...\"\n  service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n```\n\n**Azure:**\n```yaml\nannotations:\n  service.beta.kubernetes.io/azure-load-balancer-internal: \"true\"\n  service.beta.kubernetes.io/azure-pip-name: \"my-public-ip\"\n```\n\n**GCP:**\n```yaml\nannotations:\n  cloud.google.com/load-balancer-type: \"Internal\"\n  cloud.google.com/backend-config: '{\"default\": \"my-backend-config\"}'\n```\n\n### 4. ExternalName\n\nMaps service to external DNS name (CNAME record).\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: external-db\nspec:\n  type: ExternalName\n  externalName: db.external.example.com\n  ports:\n  - port: 5432\n```\n\n**Use cases:**\n- Accessing external services\n- Service migration scenarios\n- Multi-cluster service references\n\n## Complete Service Specification\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  namespace: production\n  labels:\n    app: my-app\n    tier: backend\n  annotations:\n    description: \"Main application service\"\n    prometheus.io/scrape: \"true\"\nspec:\n  # Service type\n  type: ClusterIP\n\n  # Pod selector\n  selector:\n    app: my-app\n    version: v1\n\n  # Ports configuration\n  ports:\n  - name: http\n    port: 80           # Service port\n    targetPort: 8080   # Container port (or named port)\n    protocol: TCP      # TCP, UDP, or SCTP\n\n  # Session affinity\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 10800\n\n  # IP configuration\n  clusterIP: 10.0.0.10  # Optional: specific IP\n  clusterIPs:\n  - 10.0.0.10\n  ipFamilies:\n  - IPv4\n  ipFamilyPolicy: SingleStack\n\n  # External traffic policy\n  externalTrafficPolicy: Local\n\n  # Internal traffic policy\n  internalTrafficPolicy: Local\n\n  # Health check\n  healthCheckNodePort: 30000\n\n  # Load balancer config (for type: LoadBalancer)\n  loadBalancerIP: 203.0.113.100\n  loadBalancerSourceRanges:\n  - 203.0.113.0/24\n\n  # External IPs\n  externalIPs:\n  - 80.11.12.10\n\n  # Publishing strategy\n  publishNotReadyAddresses: false\n```\n\n## Port Configuration\n\n### Named Ports\n\nUse named ports in Pods for flexibility:\n\n**Deployment:**\n```yaml\nspec:\n  template:\n    spec:\n      containers:\n      - name: app\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 9090\n```\n\n**Service:**\n```yaml\nspec:\n  ports:\n  - name: http\n    port: 80\n    targetPort: http  # References named port\n  - name: metrics\n    port: 9090\n    targetPort: metrics\n```\n\n### Multiple Ports\n\n```yaml\nspec:\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    protocol: TCP\n  - name: https\n    port: 443\n    targetPort: 8443\n    protocol: TCP\n  - name: grpc\n    port: 9090\n    targetPort: 9090\n    protocol: TCP\n```\n\n## Session Affinity\n\n### None (Default)\n\nDistributes requests randomly across pods.\n\n```yaml\nspec:\n  sessionAffinity: None\n```\n\n### ClientIP\n\nRoutes requests from same client IP to same pod.\n\n```yaml\nspec:\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 10800  # 3 hours\n```\n\n**Use cases:**\n- Stateful applications\n- Session-based applications\n- WebSocket connections\n\n## Traffic Policies\n\n### External Traffic Policy\n\n**Cluster (Default):**\n```yaml\nspec:\n  externalTrafficPolicy: Cluster\n```\n- Load balances across all nodes\n- May add extra network hop\n- Source IP is masked\n\n**Local:**\n```yaml\nspec:\n  externalTrafficPolicy: Local\n```\n- Traffic goes only to pods on receiving node\n- Preserves client source IP\n- Better performance (no extra hop)\n- May cause imbalanced load\n\n### Internal Traffic Policy\n\n```yaml\nspec:\n  internalTrafficPolicy: Local  # or Cluster\n```\n\nControls traffic routing for cluster-internal clients.\n\n## Headless Services\n\nService without cluster IP for direct pod access.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: database\nspec:\n  clusterIP: None  # Headless\n  selector:\n    app: database\n  ports:\n  - port: 5432\n    targetPort: 5432\n```\n\n**Use cases:**\n- StatefulSet pod discovery\n- Direct pod-to-pod communication\n- Custom load balancing\n- Database clusters\n\n**DNS returns:**\n- Individual pod IPs instead of service IP\n- Format: `<pod-name>.<service-name>.<namespace>.svc.cluster.local`\n\n## Service Discovery\n\n### DNS\n\n**ClusterIP Service:**\n```\n<service-name>.<namespace>.svc.cluster.local\n```\n\nExample:\n```bash\ncurl http://backend-service.production.svc.cluster.local\n```\n\n**Within same namespace:**\n```bash\ncurl http://backend-service\n```\n\n**Headless Service (returns pod IPs):**\n```\n<pod-name>.<service-name>.<namespace>.svc.cluster.local\n```\n\n### Environment Variables\n\nKubernetes injects service info into pods:\n\n```bash\n# Service host and port\nBACKEND_SERVICE_SERVICE_HOST=10.0.0.100\nBACKEND_SERVICE_SERVICE_PORT=80\n\n# For named ports\nBACKEND_SERVICE_SERVICE_PORT_HTTP=80\n```\n\n**Note:** Pods must be created after the service for env vars to be injected.\n\n## Load Balancing\n\n### Algorithms\n\nKubernetes uses random selection by default. For advanced load balancing:\n\n**Service Mesh (Istio example):**\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: my-destination-rule\nspec:\n  host: my-service\n  trafficPolicy:\n    loadBalancer:\n      simple: LEAST_REQUEST  # or ROUND_ROBIN, RANDOM, PASSTHROUGH\n    connectionPool:\n      tcp:\n        maxConnections: 100\n```\n\n### Connection Limits\n\nUse pod disruption budgets and resource limits:\n\n```yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: my-app-pdb\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: my-app\n```\n\n## Service Mesh Integration\n\n### Istio Virtual Service\n\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: my-service\nspec:\n  hosts:\n  - my-service\n  http:\n  - match:\n    - headers:\n        version:\n          exact: v2\n    route:\n    - destination:\n        host: my-service\n        subset: v2\n  - route:\n    - destination:\n        host: my-service\n        subset: v1\n      weight: 90\n    - destination:\n        host: my-service\n        subset: v2\n      weight: 10\n```\n\n## Common Patterns\n\n### Pattern 1: Internal Microservice\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-service\n  namespace: backend\n  labels:\n    app: user-service\n    tier: backend\nspec:\n  type: ClusterIP\n  selector:\n    app: user-service\n  ports:\n  - name: http\n    port: 8080\n    targetPort: http\n    protocol: TCP\n  - name: grpc\n    port: 9090\n    targetPort: grpc\n    protocol: TCP\n```\n\n### Pattern 2: Public API with Load Balancer\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-gateway\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:...\"\nspec:\n  type: LoadBalancer\n  externalTrafficPolicy: Local\n  selector:\n    app: api-gateway\n  ports:\n  - name: https\n    port: 443\n    targetPort: 8443\n    protocol: TCP\n  loadBalancerSourceRanges:\n  - 0.0.0.0/0\n```\n\n### Pattern 3: StatefulSet with Headless Service\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: cassandra\nspec:\n  clusterIP: None\n  selector:\n    app: cassandra\n  ports:\n  - port: 9042\n    targetPort: 9042\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: cassandra\nspec:\n  serviceName: cassandra\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cassandra\n  template:\n    metadata:\n      labels:\n        app: cassandra\n    spec:\n      containers:\n      - name: cassandra\n        image: cassandra:4.0\n```\n\n### Pattern 4: External Service Mapping\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: external-database\nspec:\n  type: ExternalName\n  externalName: prod-db.cxyz.us-west-2.rds.amazonaws.com\n---\n# Or with Endpoints for IP-based external service\napiVersion: v1\nkind: Service\nmetadata:\n  name: external-api\nspec:\n  ports:\n  - port: 443\n    targetPort: 443\n    protocol: TCP\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: external-api\nsubsets:\n- addresses:\n  - ip: 203.0.113.100\n  ports:\n  - port: 443\n```\n\n### Pattern 5: Multi-Port Service with Metrics\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-app\n  annotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9090\"\n    prometheus.io/path: \"/metrics\"\nspec:\n  type: ClusterIP\n  selector:\n    app: web-app\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n  - name: metrics\n    port: 9090\n    targetPort: 9090\n```\n\n## Network Policies\n\nControl traffic to services:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n```\n\n## Best Practices\n\n### Service Configuration\n\n1. **Use named ports** for flexibility\n2. **Set appropriate service type** based on exposure needs\n3. **Use labels and selectors consistently** across Deployments and Services\n4. **Configure session affinity** for stateful apps\n5. **Set external traffic policy to Local** for IP preservation\n6. **Use headless services** for StatefulSets\n7. **Implement network policies** for security\n8. **Add monitoring annotations** for observability\n\n### Production Checklist\n\n- [ ] Service type appropriate for use case\n- [ ] Selector matches pod labels\n- [ ] Named ports used for clarity\n- [ ] Session affinity configured if needed\n- [ ] Traffic policy set appropriately\n- [ ] Load balancer annotations configured (if applicable)\n- [ ] Source IP ranges restricted (for public services)\n- [ ] Health check configuration validated\n- [ ] Monitoring annotations added\n- [ ] Network policies defined\n\n### Performance Tuning\n\n**For high traffic:**\n```yaml\nspec:\n  externalTrafficPolicy: Local\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 3600\n```\n\n**For WebSocket/long connections:**\n```yaml\nspec:\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 86400  # 24 hours\n```\n\n## Troubleshooting\n\n### Service not accessible\n\n```bash\n# Check service exists\nkubectl get service <service-name>\n\n# Check endpoints (should show pod IPs)\nkubectl get endpoints <service-name>\n\n# Describe service\nkubectl describe service <service-name>\n\n# Check if pods match selector\nkubectl get pods -l app=<app-name>\n```\n\n**Common issues:**\n- Selector doesn't match pod labels\n- No pods running (endpoints empty)\n- Ports misconfigured\n- Network policy blocking traffic\n\n### DNS resolution failing\n\n```bash\n# Test DNS from pod\nkubectl run debug --rm -it --image=busybox -- nslookup <service-name>\n\n# Check CoreDNS\nkubectl get pods -n kube-system -l k8s-app=kube-dns\nkubectl logs -n kube-system -l k8s-app=kube-dns\n```\n\n### Load balancer issues\n\n```bash\n# Check load balancer status\nkubectl describe service <service-name>\n\n# Check events\nkubectl get events --sort-by='.lastTimestamp'\n\n# Verify cloud provider configuration\nkubectl describe node\n```\n\n## Related Resources\n\n- [Kubernetes Service API Reference](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#service-v1-core)\n- [Service Networking](https://kubernetes.io/docs/concepts/services-networking/service/)\n- [DNS for Services and Pods](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)\n"
      },
      "assets": {}
    },
    {
      "name": "k8s-security-policies",
      "description": "Implement Kubernetes security policies including NetworkPolicy, PodSecurityPolicy, and RBAC for production-grade security. Use when securing Kubernetes clusters, implementing network isolation, or enforcing pod security standards.",
      "plugin": "kubernetes-operations",
      "source_path": "plugins/kubernetes-operations/skills/k8s-security-policies/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "kubernetes",
        "k8s",
        "containers",
        "helm",
        "argocd",
        "gitops"
      ],
      "content": "---\nname: k8s-security-policies\ndescription: Implement Kubernetes security policies including NetworkPolicy, PodSecurityPolicy, and RBAC for production-grade security. Use when securing Kubernetes clusters, implementing network isolation, or enforcing pod security standards.\n---\n\n# Kubernetes Security Policies\n\nComprehensive guide for implementing NetworkPolicy, PodSecurityPolicy, RBAC, and Pod Security Standards in Kubernetes.\n\n## Purpose\n\nImplement defense-in-depth security for Kubernetes clusters using network policies, pod security standards, and RBAC.\n\n## When to Use This Skill\n\n- Implement network segmentation\n- Configure pod security standards\n- Set up RBAC for least-privilege access\n- Create security policies for compliance\n- Implement admission control\n- Secure multi-tenant clusters\n\n## Pod Security Standards\n\n### 1. Privileged (Unrestricted)\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: privileged-ns\n  labels:\n    pod-security.kubernetes.io/enforce: privileged\n    pod-security.kubernetes.io/audit: privileged\n    pod-security.kubernetes.io/warn: privileged\n```\n\n### 2. Baseline (Minimally restrictive)\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: baseline-ns\n  labels:\n    pod-security.kubernetes.io/enforce: baseline\n    pod-security.kubernetes.io/audit: baseline\n    pod-security.kubernetes.io/warn: baseline\n```\n\n### 3. Restricted (Most restrictive)\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: restricted-ns\n  labels:\n    pod-security.kubernetes.io/enforce: restricted\n    pod-security.kubernetes.io/audit: restricted\n    pod-security.kubernetes.io/warn: restricted\n```\n\n## Network Policies\n\n### Default Deny All\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n```\n\n### Allow Frontend to Backend\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n```\n\n### Allow DNS\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-dns\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n    ports:\n    - protocol: UDP\n      port: 53\n```\n\n**Reference:** See `assets/network-policy-template.yaml`\n\n## RBAC Configuration\n\n### Role (Namespace-scoped)\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-reader\n  namespace: production\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n```\n\n### ClusterRole (Cluster-wide)\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: secret-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n```\n\n### RoleBinding\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: production\nsubjects:\n- kind: User\n  name: jane\n  apiGroup: rbac.authorization.k8s.io\n- kind: ServiceAccount\n  name: default\n  namespace: production\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n```\n\n**Reference:** See `references/rbac-patterns.md`\n\n## Pod Security Context\n\n### Restricted Pod\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secure-pod\nspec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n    fsGroup: 1000\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n  - name: app\n    image: myapp:1.0\n    securityContext:\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      capabilities:\n        drop:\n        - ALL\n```\n\n## Policy Enforcement with OPA Gatekeeper\n\n### ConstraintTemplate\n```yaml\napiVersion: templates.gatekeeper.sh/v1\nkind: ConstraintTemplate\nmetadata:\n  name: k8srequiredlabels\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sRequiredLabels\n      validation:\n        openAPIV3Schema:\n          type: object\n          properties:\n            labels:\n              type: array\n              items:\n                type: string\n  targets:\n    - target: admission.k8s.gatekeeper.sh\n      rego: |\n        package k8srequiredlabels\n        violation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] {\n          provided := {label | input.review.object.metadata.labels[label]}\n          required := {label | label := input.parameters.labels[_]}\n          missing := required - provided\n          count(missing) > 0\n          msg := sprintf(\"missing required labels: %v\", [missing])\n        }\n```\n\n### Constraint\n```yaml\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sRequiredLabels\nmetadata:\n  name: require-app-label\nspec:\n  match:\n    kinds:\n      - apiGroups: [\"apps\"]\n        kinds: [\"Deployment\"]\n  parameters:\n    labels: [\"app\", \"environment\"]\n```\n\n## Service Mesh Security (Istio)\n\n### PeerAuthentication (mTLS)\n```yaml\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: production\nspec:\n  mtls:\n    mode: STRICT\n```\n\n### AuthorizationPolicy\n```yaml\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: allow-frontend\n  namespace: production\nspec:\n  selector:\n    matchLabels:\n      app: backend\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/production/sa/frontend\"]\n```\n\n## Best Practices\n\n1. **Implement Pod Security Standards** at namespace level\n2. **Use Network Policies** for network segmentation\n3. **Apply least-privilege RBAC** for all service accounts\n4. **Enable admission control** (OPA Gatekeeper/Kyverno)\n5. **Run containers as non-root**\n6. **Use read-only root filesystem**\n7. **Drop all capabilities** unless needed\n8. **Implement resource quotas** and limit ranges\n9. **Enable audit logging** for security events\n10. **Regular security scanning** of images\n\n## Compliance Frameworks\n\n### CIS Kubernetes Benchmark\n- Use RBAC authorization\n- Enable audit logging\n- Use Pod Security Standards\n- Configure network policies\n- Implement secrets encryption at rest\n- Enable node authentication\n\n### NIST Cybersecurity Framework\n- Implement defense in depth\n- Use network segmentation\n- Configure security monitoring\n- Implement access controls\n- Enable logging and monitoring\n\n## Troubleshooting\n\n**NetworkPolicy not working:**\n```bash\n# Check if CNI supports NetworkPolicy\nkubectl get nodes -o wide\nkubectl describe networkpolicy <name>\n```\n\n**RBAC permission denied:**\n```bash\n# Check effective permissions\nkubectl auth can-i list pods --as system:serviceaccount:default:my-sa\nkubectl auth can-i '*' '*' --as system:serviceaccount:default:my-sa\n```\n\n## Reference Files\n\n- `assets/network-policy-template.yaml` - Network policy examples\n- `assets/pod-security-template.yaml` - Pod security policies\n- `references/rbac-patterns.md` - RBAC configuration patterns\n\n## Related Skills\n\n- `k8s-manifest-generator` - For creating secure manifests\n- `gitops-workflow` - For automated policy deployment\n",
      "references": {
        "rbac-patterns.md": "# RBAC Patterns and Best Practices\n\n## Common RBAC Patterns\n\n### Pattern 1: Read-Only Access\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: read-only\nrules:\n- apiGroups: [\"\", \"apps\", \"batch\"]\n  resources: [\"*\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n```\n\n### Pattern 2: Namespace Admin\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: namespace-admin\n  namespace: production\nrules:\n- apiGroups: [\"\", \"apps\", \"batch\", \"extensions\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n```\n\n### Pattern 3: Deployment Manager\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: deployment-manager\n  namespace: production\nrules:\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n```\n\n### Pattern 4: Secret Reader (ServiceAccount)\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: secret-reader\n  namespace: production\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\"]\n  resourceNames: [\"app-secrets\"]  # Specific secret only\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: app-secret-reader\n  namespace: production\nsubjects:\n- kind: ServiceAccount\n  name: my-app\n  namespace: production\nroleRef:\n  kind: Role\n  name: secret-reader\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Pattern 5: CI/CD Pipeline Access\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cicd-deployer\nrules:\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"replicasets\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\"]\n- apiGroups: [\"\"]\n  resources: [\"services\", \"configmaps\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n```\n\n## ServiceAccount Best Practices\n\n### Create Dedicated ServiceAccounts\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-app\n  namespace: production\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  template:\n    spec:\n      serviceAccountName: my-app\n      automountServiceAccountToken: false  # Disable if not needed\n```\n\n### Least-Privilege ServiceAccount\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: my-app-role\n  namespace: production\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"get\"]\n  resourceNames: [\"my-app-config\"]\n```\n\n## Security Best Practices\n\n1. **Use Roles over ClusterRoles** when possible\n2. **Specify resourceNames** for fine-grained access\n3. **Avoid wildcard permissions** (`*`) in production\n4. **Create dedicated ServiceAccounts** for each app\n5. **Disable token auto-mounting** if not needed\n6. **Regular RBAC audits** to remove unused permissions\n7. **Use groups** for user management\n8. **Implement namespace isolation**\n9. **Monitor RBAC usage** with audit logs\n10. **Document role purposes** in metadata\n\n## Troubleshooting RBAC\n\n### Check User Permissions\n```bash\nkubectl auth can-i list pods --as john@example.com\nkubectl auth can-i '*' '*' --as system:serviceaccount:default:my-app\n```\n\n### View Effective Permissions\n```bash\nkubectl describe clusterrole cluster-admin\nkubectl describe rolebinding -n production\n```\n\n### Debug Access Issues\n```bash\nkubectl get rolebindings,clusterrolebindings --all-namespaces -o wide | grep my-user\n```\n\n## Common RBAC Verbs\n\n- `get` - Read a specific resource\n- `list` - List all resources of a type\n- `watch` - Watch for resource changes\n- `create` - Create new resources\n- `update` - Update existing resources\n- `patch` - Partially update resources\n- `delete` - Delete resources\n- `deletecollection` - Delete multiple resources\n- `*` - All verbs (avoid in production)\n\n## Resource Scope\n\n### Cluster-Scoped Resources\n- Nodes\n- PersistentVolumes\n- ClusterRoles\n- ClusterRoleBindings\n- Namespaces\n\n### Namespace-Scoped Resources\n- Pods\n- Services\n- Deployments\n- ConfigMaps\n- Secrets\n- Roles\n- RoleBindings\n"
      },
      "assets": {}
    }
  ],
  "counts": {
    "agents": 1,
    "commands": 0,
    "skills": 4
  }
}