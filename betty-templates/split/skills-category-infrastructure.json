{
  "total_count": 12,
  "category": "infrastructure",
  "skills": [
    {
      "name": "gitops-workflow",
      "description": "Implement GitOps workflows with ArgoCD and Flux for automated, declarative Kubernetes deployments with continuous reconciliation. Use when implementing GitOps practices, automating Kubernetes deployments, or setting up declarative infrastructure management.",
      "plugin": "kubernetes-operations",
      "source_path": "plugins/kubernetes-operations/skills/gitops-workflow/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "kubernetes",
        "k8s",
        "containers",
        "helm",
        "argocd",
        "gitops"
      ],
      "content": "---\nname: gitops-workflow\ndescription: Implement GitOps workflows with ArgoCD and Flux for automated, declarative Kubernetes deployments with continuous reconciliation. Use when implementing GitOps practices, automating Kubernetes deployments, or setting up declarative infrastructure management.\n---\n\n# GitOps Workflow\n\nComplete guide to implementing GitOps workflows with ArgoCD and Flux for automated Kubernetes deployments.\n\n## Purpose\n\nImplement declarative, Git-based continuous delivery for Kubernetes using ArgoCD or Flux CD, following OpenGitOps principles.\n\n## When to Use This Skill\n\n- Set up GitOps for Kubernetes clusters\n- Automate application deployments from Git\n- Implement progressive delivery strategies\n- Manage multi-cluster deployments\n- Configure automated sync policies\n- Set up secret management in GitOps\n\n## OpenGitOps Principles\n\n1. **Declarative** - Entire system described declaratively\n2. **Versioned and Immutable** - Desired state stored in Git\n3. **Pulled Automatically** - Software agents pull desired state\n4. **Continuously Reconciled** - Agents reconcile actual vs desired state\n\n## ArgoCD Setup\n\n### 1. Installation\n\n```bash\n# Create namespace\nkubectl create namespace argocd\n\n# Install ArgoCD\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n\n# Get admin password\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n```\n\n**Reference:** See `references/argocd-setup.md` for detailed setup\n\n### 2. Repository Structure\n\n```\ngitops-repo/\n\u251c\u2500\u2500 apps/\n\u2502   \u251c\u2500\u2500 production/\n\u2502   \u2502   \u251c\u2500\u2500 app1/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 deployment.yaml\n\u2502   \u2502   \u2514\u2500\u2500 app2/\n\u2502   \u2514\u2500\u2500 staging/\n\u251c\u2500\u2500 infrastructure/\n\u2502   \u251c\u2500\u2500 ingress-nginx/\n\u2502   \u251c\u2500\u2500 cert-manager/\n\u2502   \u2514\u2500\u2500 monitoring/\n\u2514\u2500\u2500 argocd/\n    \u251c\u2500\u2500 applications/\n    \u2514\u2500\u2500 projects/\n```\n\n### 3. Create Application\n\n```yaml\n# argocd/applications/my-app.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-app\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/org/gitops-repo\n    targetRevision: main\n    path: apps/production/my-app\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: production\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n    - CreateNamespace=true\n```\n\n### 4. App of Apps Pattern\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: applications\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/org/gitops-repo\n    targetRevision: main\n    path: argocd/applications\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: argocd\n  syncPolicy:\n    automated: {}\n```\n\n## Flux CD Setup\n\n### 1. Installation\n\n```bash\n# Install Flux CLI\ncurl -s https://fluxcd.io/install.sh | sudo bash\n\n# Bootstrap Flux\nflux bootstrap github \\\n  --owner=org \\\n  --repository=gitops-repo \\\n  --branch=main \\\n  --path=clusters/production \\\n  --personal\n```\n\n### 2. Create GitRepository\n\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  interval: 1m\n  url: https://github.com/org/my-app\n  ref:\n    branch: main\n```\n\n### 3. Create Kustomization\n\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  interval: 5m\n  path: ./deploy\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: my-app\n```\n\n## Sync Policies\n\n### Auto-Sync Configuration\n\n**ArgoCD:**\n```yaml\nsyncPolicy:\n  automated:\n    prune: true      # Delete resources not in Git\n    selfHeal: true   # Reconcile manual changes\n    allowEmpty: false\n  retry:\n    limit: 5\n    backoff:\n      duration: 5s\n      factor: 2\n      maxDuration: 3m\n```\n\n**Flux:**\n```yaml\nspec:\n  interval: 1m\n  prune: true\n  wait: true\n  timeout: 5m\n```\n\n**Reference:** See `references/sync-policies.md`\n\n## Progressive Delivery\n\n### Canary Deployment with ArgoCD Rollouts\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: my-app\nspec:\n  replicas: 5\n  strategy:\n    canary:\n      steps:\n      - setWeight: 20\n      - pause: {duration: 1m}\n      - setWeight: 50\n      - pause: {duration: 2m}\n      - setWeight: 100\n```\n\n### Blue-Green Deployment\n\n```yaml\nstrategy:\n  blueGreen:\n    activeService: my-app\n    previewService: my-app-preview\n    autoPromotionEnabled: false\n```\n\n## Secret Management\n\n### External Secrets Operator\n\n```yaml\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: db-credentials\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: aws-secrets-manager\n    kind: SecretStore\n  target:\n    name: db-credentials\n  data:\n  - secretKey: password\n    remoteRef:\n      key: prod/db/password\n```\n\n### Sealed Secrets\n\n```bash\n# Encrypt secret\nkubeseal --format yaml < secret.yaml > sealed-secret.yaml\n\n# Commit sealed-secret.yaml to Git\n```\n\n## Best Practices\n\n1. **Use separate repos or branches** for different environments\n2. **Implement RBAC** for Git repositories\n3. **Enable notifications** for sync failures\n4. **Use health checks** for custom resources\n5. **Implement approval gates** for production\n6. **Keep secrets out of Git** (use External Secrets)\n7. **Use App of Apps pattern** for organization\n8. **Tag releases** for easy rollback\n9. **Monitor sync status** with alerts\n10. **Test changes** in staging first\n\n## Troubleshooting\n\n**Sync failures:**\n```bash\nargocd app get my-app\nargocd app sync my-app --prune\n```\n\n**Out of sync status:**\n```bash\nargocd app diff my-app\nargocd app sync my-app --force\n```\n\n## Related Skills\n\n- `k8s-manifest-generator` - For creating manifests\n- `helm-chart-scaffolding` - For packaging applications\n",
      "references": {
        "sync-policies.md": "# GitOps Sync Policies\n\n## ArgoCD Sync Policies\n\n### Automated Sync\n```yaml\nsyncPolicy:\n  automated:\n    prune: true       # Delete resources removed from Git\n    selfHeal: true    # Reconcile manual changes\n    allowEmpty: false # Prevent empty sync\n```\n\n### Manual Sync\n```yaml\nsyncPolicy:\n  syncOptions:\n  - PrunePropagationPolicy=foreground\n  - CreateNamespace=true\n```\n\n### Sync Windows\n```yaml\nsyncWindows:\n- kind: allow\n  schedule: \"0 8 * * *\"\n  duration: 1h\n  applications:\n  - my-app\n- kind: deny\n  schedule: \"0 22 * * *\"\n  duration: 8h\n  applications:\n  - '*'\n```\n\n### Retry Policy\n```yaml\nsyncPolicy:\n  retry:\n    limit: 5\n    backoff:\n      duration: 5s\n      factor: 2\n      maxDuration: 3m\n```\n\n## Flux Sync Policies\n\n### Kustomization Sync\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: my-app\nspec:\n  interval: 5m\n  prune: true\n  wait: true\n  timeout: 5m\n  retryInterval: 1m\n  force: false\n```\n\n### Source Sync Interval\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: my-app\nspec:\n  interval: 1m\n  timeout: 60s\n```\n\n## Health Assessment\n\n### Custom Health Checks\n```yaml\n# ArgoCD\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\n  namespace: argocd\ndata:\n  resource.customizations.health.MyCustomResource: |\n    hs = {}\n    if obj.status ~= nil then\n      if obj.status.conditions ~= nil then\n        for i, condition in ipairs(obj.status.conditions) do\n          if condition.type == \"Ready\" and condition.status == \"False\" then\n            hs.status = \"Degraded\"\n            hs.message = condition.message\n            return hs\n          end\n          if condition.type == \"Ready\" and condition.status == \"True\" then\n            hs.status = \"Healthy\"\n            hs.message = condition.message\n            return hs\n          end\n        end\n      end\n    end\n    hs.status = \"Progressing\"\n    hs.message = \"Waiting for status\"\n    return hs\n```\n\n## Sync Options\n\n### Common Sync Options\n- `PrunePropagationPolicy=foreground` - Wait for pruned resources to be deleted\n- `CreateNamespace=true` - Auto-create namespace\n- `Validate=false` - Skip kubectl validation\n- `PruneLast=true` - Prune resources after sync\n- `RespectIgnoreDifferences=true` - Honor ignore differences\n- `ApplyOutOfSyncOnly=true` - Only apply out-of-sync resources\n\n## Best Practices\n\n1. Use automated sync for non-production\n2. Require manual approval for production\n3. Configure sync windows for maintenance\n4. Implement health checks for custom resources\n5. Use selective sync for large applications\n6. Configure appropriate retry policies\n7. Monitor sync failures with alerts\n8. Use prune with caution in production\n9. Test sync policies in staging\n10. Document sync behavior for teams\n",
        "argocd-setup.md": "# ArgoCD Setup and Configuration\n\n## Installation Methods\n\n### 1. Standard Installation\n```bash\nkubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n```\n\n### 2. High Availability Installation\n```bash\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/ha/install.yaml\n```\n\n### 3. Helm Installation\n```bash\nhelm repo add argo https://argoproj.github.io/argo-helm\nhelm install argocd argo/argo-cd -n argocd --create-namespace\n```\n\n## Initial Configuration\n\n### Access ArgoCD UI\n```bash\n# Port forward\nkubectl port-forward svc/argocd-server -n argocd 8080:443\n\n# Get initial admin password\nargocd admin initial-password -n argocd\n```\n\n### Configure Ingress\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argocd-server-ingress\n  namespace: argocd\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: argocd.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: argocd-server\n            port:\n              number: 443\n  tls:\n  - hosts:\n    - argocd.example.com\n    secretName: argocd-secret\n```\n\n## CLI Configuration\n\n### Login\n```bash\nargocd login argocd.example.com --username admin\n```\n\n### Add Repository\n```bash\nargocd repo add https://github.com/org/repo --username user --password token\n```\n\n### Create Application\n```bash\nargocd app create my-app \\\n  --repo https://github.com/org/repo \\\n  --path apps/my-app \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace production\n```\n\n## SSO Configuration\n\n### GitHub OAuth\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\n  namespace: argocd\ndata:\n  url: https://argocd.example.com\n  dex.config: |\n    connectors:\n      - type: github\n        id: github\n        name: GitHub\n        config:\n          clientID: $GITHUB_CLIENT_ID\n          clientSecret: $GITHUB_CLIENT_SECRET\n          orgs:\n          - name: my-org\n```\n\n## RBAC Configuration\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-rbac-cm\n  namespace: argocd\ndata:\n  policy.default: role:readonly\n  policy.csv: |\n    p, role:developers, applications, *, */dev, allow\n    p, role:operators, applications, *, */*, allow\n    g, my-org:devs, role:developers\n    g, my-org:ops, role:operators\n```\n\n## Best Practices\n\n1. Enable SSO for production\n2. Implement RBAC policies\n3. Use separate projects for teams\n4. Enable audit logging\n5. Configure notifications\n6. Use ApplicationSets for multi-cluster\n7. Implement resource hooks\n8. Configure health checks\n9. Use sync windows for maintenance\n10. Monitor with Prometheus metrics\n"
      },
      "assets": {}
    },
    {
      "name": "helm-chart-scaffolding",
      "description": "Design, organize, and manage Helm charts for templating and packaging Kubernetes applications with reusable configurations. Use when creating Helm charts, packaging Kubernetes applications, or implementing templated deployments.",
      "plugin": "kubernetes-operations",
      "source_path": "plugins/kubernetes-operations/skills/helm-chart-scaffolding/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "kubernetes",
        "k8s",
        "containers",
        "helm",
        "argocd",
        "gitops"
      ],
      "content": "---\nname: helm-chart-scaffolding\ndescription: Design, organize, and manage Helm charts for templating and packaging Kubernetes applications with reusable configurations. Use when creating Helm charts, packaging Kubernetes applications, or implementing templated deployments.\n---\n\n# Helm Chart Scaffolding\n\nComprehensive guidance for creating, organizing, and managing Helm charts for packaging and deploying Kubernetes applications.\n\n## Purpose\n\nThis skill provides step-by-step instructions for building production-ready Helm charts, including chart structure, templating patterns, values management, and validation strategies.\n\n## When to Use This Skill\n\nUse this skill when you need to:\n- Create new Helm charts from scratch\n- Package Kubernetes applications for distribution\n- Manage multi-environment deployments with Helm\n- Implement templating for reusable Kubernetes manifests\n- Set up Helm chart repositories\n- Follow Helm best practices and conventions\n\n## Helm Overview\n\n**Helm** is the package manager for Kubernetes that:\n- Templates Kubernetes manifests for reusability\n- Manages application releases and rollbacks\n- Handles dependencies between charts\n- Provides version control for deployments\n- Simplifies configuration management across environments\n\n## Step-by-Step Workflow\n\n### 1. Initialize Chart Structure\n\n**Create new chart:**\n```bash\nhelm create my-app\n```\n\n**Standard chart structure:**\n```\nmy-app/\n\u251c\u2500\u2500 Chart.yaml           # Chart metadata\n\u251c\u2500\u2500 values.yaml          # Default configuration values\n\u251c\u2500\u2500 charts/              # Chart dependencies\n\u251c\u2500\u2500 templates/           # Kubernetes manifest templates\n\u2502   \u251c\u2500\u2500 NOTES.txt       # Post-install notes\n\u2502   \u251c\u2500\u2500 _helpers.tpl    # Template helpers\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u251c\u2500\u2500 ingress.yaml\n\u2502   \u251c\u2500\u2500 serviceaccount.yaml\n\u2502   \u251c\u2500\u2500 hpa.yaml\n\u2502   \u2514\u2500\u2500 tests/\n\u2502       \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 .helmignore         # Files to ignore\n```\n\n### 2. Configure Chart.yaml\n\n**Chart metadata defines the package:**\n\n```yaml\napiVersion: v2\nname: my-app\ndescription: A Helm chart for My Application\ntype: application\nversion: 1.0.0      # Chart version\nappVersion: \"2.1.0\" # Application version\n\n# Keywords for chart discovery\nkeywords:\n  - web\n  - api\n  - backend\n\n# Maintainer information\nmaintainers:\n  - name: DevOps Team\n    email: devops@example.com\n    url: https://github.com/example/my-app\n\n# Source code repository\nsources:\n  - https://github.com/example/my-app\n\n# Homepage\nhome: https://example.com\n\n# Chart icon\nicon: https://example.com/icon.png\n\n# Dependencies\ndependencies:\n  - name: postgresql\n    version: \"12.0.0\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: postgresql.enabled\n  - name: redis\n    version: \"17.0.0\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: redis.enabled\n```\n\n**Reference:** See `assets/Chart.yaml.template` for complete example\n\n### 3. Design values.yaml Structure\n\n**Organize values hierarchically:**\n\n```yaml\n# Image configuration\nimage:\n  repository: myapp\n  tag: \"1.0.0\"\n  pullPolicy: IfNotPresent\n\n# Number of replicas\nreplicaCount: 3\n\n# Service configuration\nservice:\n  type: ClusterIP\n  port: 80\n  targetPort: 8080\n\n# Ingress configuration\ningress:\n  enabled: false\n  className: nginx\n  hosts:\n    - host: app.example.com\n      paths:\n        - path: /\n          pathType: Prefix\n\n# Resources\nresources:\n  requests:\n    memory: \"256Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"512Mi\"\n    cpu: \"500m\"\n\n# Autoscaling\nautoscaling:\n  enabled: false\n  minReplicas: 2\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 80\n\n# Environment variables\nenv:\n  - name: LOG_LEVEL\n    value: \"info\"\n\n# ConfigMap data\nconfigMap:\n  data:\n    APP_MODE: production\n\n# Dependencies\npostgresql:\n  enabled: true\n  auth:\n    database: myapp\n    username: myapp\n\nredis:\n  enabled: false\n```\n\n**Reference:** See `assets/values.yaml.template` for complete structure\n\n### 4. Create Template Files\n\n**Use Go templating with Helm functions:**\n\n**templates/deployment.yaml:**\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"my-app.fullname\" . }}\n  labels:\n    {{- include \"my-app.labels\" . | nindent 4 }}\nspec:\n  {{- if not .Values.autoscaling.enabled }}\n  replicas: {{ .Values.replicaCount }}\n  {{- end }}\n  selector:\n    matchLabels:\n      {{- include \"my-app.selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      labels:\n        {{- include \"my-app.selectorLabels\" . | nindent 8 }}\n    spec:\n      containers:\n      - name: {{ .Chart.Name }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n        imagePullPolicy: {{ .Values.image.pullPolicy }}\n        ports:\n        - name: http\n          containerPort: {{ .Values.service.targetPort }}\n        resources:\n          {{- toYaml .Values.resources | nindent 12 }}\n        env:\n          {{- toYaml .Values.env | nindent 12 }}\n```\n\n### 5. Create Template Helpers\n\n**templates/_helpers.tpl:**\n```yaml\n{{/*\nExpand the name of the chart.\n*/}}\n{{- define \"my-app.name\" -}}\n{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n\n{{/*\nCreate a default fully qualified app name.\n*/}}\n{{- define \"my-app.fullname\" -}}\n{{- if .Values.fullnameOverride }}\n{{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- $name := default .Chart.Name .Values.nameOverride }}\n{{- if contains $name .Release.Name }}\n{{- .Release.Name | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n{{- end }}\n{{- end }}\n\n{{/*\nCommon labels\n*/}}\n{{- define \"my-app.labels\" -}}\nhelm.sh/chart: {{ include \"my-app.chart\" . }}\n{{ include \"my-app.selectorLabels\" . }}\n{{- if .Chart.AppVersion }}\napp.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\n{{- end }}\napp.kubernetes.io/managed-by: {{ .Release.Service }}\n{{- end }}\n\n{{/*\nSelector labels\n*/}}\n{{- define \"my-app.selectorLabels\" -}}\napp.kubernetes.io/name: {{ include \"my-app.name\" . }}\napp.kubernetes.io/instance: {{ .Release.Name }}\n{{- end }}\n```\n\n### 6. Manage Dependencies\n\n**Add dependencies in Chart.yaml:**\n```yaml\ndependencies:\n  - name: postgresql\n    version: \"12.0.0\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: postgresql.enabled\n```\n\n**Update dependencies:**\n```bash\nhelm dependency update\nhelm dependency build\n```\n\n**Override dependency values:**\n```yaml\n# values.yaml\npostgresql:\n  enabled: true\n  auth:\n    database: myapp\n    username: myapp\n    password: changeme\n  primary:\n    persistence:\n      enabled: true\n      size: 10Gi\n```\n\n### 7. Test and Validate\n\n**Validation commands:**\n```bash\n# Lint the chart\nhelm lint my-app/\n\n# Dry-run installation\nhelm install my-app ./my-app --dry-run --debug\n\n# Template rendering\nhelm template my-app ./my-app\n\n# Template with values\nhelm template my-app ./my-app -f values-prod.yaml\n\n# Show computed values\nhelm show values ./my-app\n```\n\n**Validation script:**\n```bash\n#!/bin/bash\nset -e\n\necho \"Linting chart...\"\nhelm lint .\n\necho \"Testing template rendering...\"\nhelm template test-release . --dry-run\n\necho \"Checking for required values...\"\nhelm template test-release . --validate\n\necho \"All validations passed!\"\n```\n\n**Reference:** See `scripts/validate-chart.sh`\n\n### 8. Package and Distribute\n\n**Package the chart:**\n```bash\nhelm package my-app/\n# Creates: my-app-1.0.0.tgz\n```\n\n**Create chart repository:**\n```bash\n# Create index\nhelm repo index .\n\n# Upload to repository\n# AWS S3 example\naws s3 sync . s3://my-helm-charts/ --exclude \"*\" --include \"*.tgz\" --include \"index.yaml\"\n```\n\n**Use the chart:**\n```bash\nhelm repo add my-repo https://charts.example.com\nhelm repo update\nhelm install my-app my-repo/my-app\n```\n\n### 9. Multi-Environment Configuration\n\n**Environment-specific values files:**\n\n```\nmy-app/\n\u251c\u2500\u2500 values.yaml          # Defaults\n\u251c\u2500\u2500 values-dev.yaml      # Development\n\u251c\u2500\u2500 values-staging.yaml  # Staging\n\u2514\u2500\u2500 values-prod.yaml     # Production\n```\n\n**values-prod.yaml:**\n```yaml\nreplicaCount: 5\n\nimage:\n  tag: \"2.1.0\"\n\nresources:\n  requests:\n    memory: \"512Mi\"\n    cpu: \"500m\"\n  limits:\n    memory: \"1Gi\"\n    cpu: \"1000m\"\n\nautoscaling:\n  enabled: true\n  minReplicas: 3\n  maxReplicas: 20\n\ningress:\n  enabled: true\n  hosts:\n    - host: app.example.com\n      paths:\n        - path: /\n          pathType: Prefix\n\npostgresql:\n  enabled: true\n  primary:\n    persistence:\n      size: 100Gi\n```\n\n**Install with environment:**\n```bash\nhelm install my-app ./my-app -f values-prod.yaml --namespace production\n```\n\n### 10. Implement Hooks and Tests\n\n**Pre-install hook:**\n```yaml\n# templates/pre-install-job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"my-app.fullname\" . }}-db-setup\n  annotations:\n    \"helm.sh/hook\": pre-install\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n      - name: db-setup\n        image: postgres:15\n        command: [\"psql\", \"-c\", \"CREATE DATABASE myapp\"]\n      restartPolicy: Never\n```\n\n**Test connection:**\n```yaml\n# templates/tests/test-connection.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: \"{{ include \"my-app.fullname\" . }}-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n  - name: wget\n    image: busybox\n    command: ['wget']\n    args: ['{{ include \"my-app.fullname\" . }}:{{ .Values.service.port }}']\n  restartPolicy: Never\n```\n\n**Run tests:**\n```bash\nhelm test my-app\n```\n\n## Common Patterns\n\n### Pattern 1: Conditional Resources\n\n```yaml\n{{- if .Values.ingress.enabled }}\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ include \"my-app.fullname\" . }}\nspec:\n  # ...\n{{- end }}\n```\n\n### Pattern 2: Iterating Over Lists\n\n```yaml\nenv:\n{{- range .Values.env }}\n- name: {{ .name }}\n  value: {{ .value | quote }}\n{{- end }}\n```\n\n### Pattern 3: Including Files\n\n```yaml\ndata:\n  config.yaml: |\n    {{- .Files.Get \"config/application.yaml\" | nindent 4 }}\n```\n\n### Pattern 4: Global Values\n\n```yaml\nglobal:\n  imageRegistry: docker.io\n  imagePullSecrets:\n    - name: regcred\n\n# Use in templates:\nimage: {{ .Values.global.imageRegistry }}/{{ .Values.image.repository }}\n```\n\n## Best Practices\n\n1. **Use semantic versioning** for chart and app versions\n2. **Document all values** in values.yaml with comments\n3. **Use template helpers** for repeated logic\n4. **Validate charts** before packaging\n5. **Pin dependency versions** explicitly\n6. **Use conditions** for optional resources\n7. **Follow naming conventions** (lowercase, hyphens)\n8. **Include NOTES.txt** with usage instructions\n9. **Add labels** consistently using helpers\n10. **Test installations** in all environments\n\n## Troubleshooting\n\n**Template rendering errors:**\n```bash\nhelm template my-app ./my-app --debug\n```\n\n**Dependency issues:**\n```bash\nhelm dependency update\nhelm dependency list\n```\n\n**Installation failures:**\n```bash\nhelm install my-app ./my-app --dry-run --debug\nkubectl get events --sort-by='.lastTimestamp'\n```\n\n## Reference Files\n\n- `assets/Chart.yaml.template` - Chart metadata template\n- `assets/values.yaml.template` - Values structure template\n- `scripts/validate-chart.sh` - Validation script\n- `references/chart-structure.md` - Detailed chart organization\n\n## Related Skills\n\n- `k8s-manifest-generator` - For creating base Kubernetes manifests\n- `gitops-workflow` - For automated Helm chart deployments\n",
      "references": {
        "chart-structure.md": "# Helm Chart Structure Reference\n\nComplete guide to Helm chart organization, file conventions, and best practices.\n\n## Standard Chart Directory Structure\n\n```\nmy-app/\n\u251c\u2500\u2500 Chart.yaml              # Chart metadata (required)\n\u251c\u2500\u2500 Chart.lock              # Dependency lock file (generated)\n\u251c\u2500\u2500 values.yaml             # Default configuration values (required)\n\u251c\u2500\u2500 values.schema.json      # JSON schema for values validation\n\u251c\u2500\u2500 .helmignore             # Patterns to ignore when packaging\n\u251c\u2500\u2500 README.md               # Chart documentation\n\u251c\u2500\u2500 LICENSE                 # Chart license\n\u251c\u2500\u2500 charts/                 # Chart dependencies (bundled)\n\u2502   \u2514\u2500\u2500 postgresql-12.0.0.tgz\n\u251c\u2500\u2500 crds/                   # Custom Resource Definitions\n\u2502   \u2514\u2500\u2500 my-crd.yaml\n\u251c\u2500\u2500 templates/              # Kubernetes manifest templates (required)\n\u2502   \u251c\u2500\u2500 NOTES.txt          # Post-install instructions\n\u2502   \u251c\u2500\u2500 _helpers.tpl       # Template helper functions\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u251c\u2500\u2500 ingress.yaml\n\u2502   \u251c\u2500\u2500 configmap.yaml\n\u2502   \u251c\u2500\u2500 secret.yaml\n\u2502   \u251c\u2500\u2500 serviceaccount.yaml\n\u2502   \u251c\u2500\u2500 hpa.yaml\n\u2502   \u251c\u2500\u2500 pdb.yaml\n\u2502   \u251c\u2500\u2500 networkpolicy.yaml\n\u2502   \u2514\u2500\u2500 tests/\n\u2502       \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 files/                  # Additional files to include\n    \u2514\u2500\u2500 config/\n        \u2514\u2500\u2500 app.conf\n```\n\n## Chart.yaml Specification\n\n### API Version v2 (Helm 3+)\n\n```yaml\napiVersion: v2                    # Required: API version\nname: my-application              # Required: Chart name\nversion: 1.2.3                    # Required: Chart version (SemVer)\nappVersion: \"2.5.0\"              # Application version\ndescription: A Helm chart for my application  # Required\ntype: application                 # Chart type: application or library\nkeywords:                         # Search keywords\n  - web\n  - api\n  - backend\nhome: https://example.com         # Project home page\nsources:                          # Source code URLs\n  - https://github.com/example/my-app\nmaintainers:                      # Maintainer list\n  - name: John Doe\n    email: john@example.com\n    url: https://github.com/johndoe\nicon: https://example.com/icon.png  # Chart icon URL\nkubeVersion: \">=1.24.0\"          # Compatible Kubernetes versions\ndeprecated: false                 # Mark chart as deprecated\nannotations:                      # Arbitrary annotations\n  example.com/release-notes: https://example.com/releases/v1.2.3\ndependencies:                     # Chart dependencies\n  - name: postgresql\n    version: \"12.0.0\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: postgresql.enabled\n    tags:\n      - database\n    import-values:\n      - child: database\n        parent: database\n    alias: db\n```\n\n## Chart Types\n\n### Application Chart\n```yaml\ntype: application\n```\n- Standard Kubernetes applications\n- Can be installed and managed\n- Contains templates for K8s resources\n\n### Library Chart\n```yaml\ntype: library\n```\n- Shared template helpers\n- Cannot be installed directly\n- Used as dependency by other charts\n- No templates/ directory\n\n## Values Files Organization\n\n### values.yaml (defaults)\n```yaml\n# Global values (shared with subcharts)\nglobal:\n  imageRegistry: docker.io\n  imagePullSecrets: []\n\n# Image configuration\nimage:\n  registry: docker.io\n  repository: myapp/web\n  tag: \"\"  # Defaults to .Chart.AppVersion\n  pullPolicy: IfNotPresent\n\n# Deployment settings\nreplicaCount: 1\nrevisionHistoryLimit: 10\n\n# Pod configuration\npodAnnotations: {}\npodSecurityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n  fsGroup: 1000\n\n# Container security\nsecurityContext:\n  allowPrivilegeEscalation: false\n  readOnlyRootFilesystem: true\n  capabilities:\n    drop:\n    - ALL\n\n# Service\nservice:\n  type: ClusterIP\n  port: 80\n  targetPort: http\n  annotations: {}\n\n# Resources\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n\n# Autoscaling\nautoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 100\n  targetCPUUtilizationPercentage: 80\n\n# Node selection\nnodeSelector: {}\ntolerations: []\naffinity: {}\n\n# Monitoring\nserviceMonitor:\n  enabled: false\n  interval: 30s\n```\n\n### values.schema.json (validation)\n```json\n{\n  \"$schema\": \"https://json-schema.org/draft-07/schema#\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"replicaCount\": {\n      \"type\": \"integer\",\n      \"minimum\": 1\n    },\n    \"image\": {\n      \"type\": \"object\",\n      \"required\": [\"repository\"],\n      \"properties\": {\n        \"repository\": {\n          \"type\": \"string\"\n        },\n        \"tag\": {\n          \"type\": \"string\"\n        },\n        \"pullPolicy\": {\n          \"type\": \"string\",\n          \"enum\": [\"Always\", \"IfNotPresent\", \"Never\"]\n        }\n      }\n    }\n  },\n  \"required\": [\"image\"]\n}\n```\n\n## Template Files\n\n### Template Naming Conventions\n\n- **Lowercase with hyphens**: `deployment.yaml`, `service-account.yaml`\n- **Partial templates**: Prefix with underscore `_helpers.tpl`\n- **Tests**: Place in `templates/tests/`\n- **CRDs**: Place in `crds/` (not templated)\n\n### Common Templates\n\n#### _helpers.tpl\n```yaml\n{{/*\nStandard naming helpers\n*/}}\n{{- define \"my-app.name\" -}}\n{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix \"-\" -}}\n{{- end -}}\n\n{{- define \"my-app.fullname\" -}}\n{{- if .Values.fullnameOverride -}}\n{{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" -}}\n{{- else -}}\n{{- $name := default .Chart.Name .Values.nameOverride -}}\n{{- if contains $name .Release.Name -}}\n{{- .Release.Name | trunc 63 | trimSuffix \"-\" -}}\n{{- else -}}\n{{- printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" -}}\n{{- end -}}\n{{- end -}}\n{{- end -}}\n\n{{- define \"my-app.chart\" -}}\n{{- printf \"%s-%s\" .Chart.Name .Chart.Version | replace \"+\" \"_\" | trunc 63 | trimSuffix \"-\" -}}\n{{- end -}}\n\n{{/*\nCommon labels\n*/}}\n{{- define \"my-app.labels\" -}}\nhelm.sh/chart: {{ include \"my-app.chart\" . }}\n{{ include \"my-app.selectorLabels\" . }}\n{{- if .Chart.AppVersion }}\napp.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\n{{- end }}\napp.kubernetes.io/managed-by: {{ .Release.Service }}\n{{- end -}}\n\n{{- define \"my-app.selectorLabels\" -}}\napp.kubernetes.io/name: {{ include \"my-app.name\" . }}\napp.kubernetes.io/instance: {{ .Release.Name }}\n{{- end -}}\n\n{{/*\nImage name helper\n*/}}\n{{- define \"my-app.image\" -}}\n{{- $registry := .Values.global.imageRegistry | default .Values.image.registry -}}\n{{- $repository := .Values.image.repository -}}\n{{- $tag := .Values.image.tag | default .Chart.AppVersion -}}\n{{- printf \"%s/%s:%s\" $registry $repository $tag -}}\n{{- end -}}\n```\n\n#### NOTES.txt\n```\nThank you for installing {{ .Chart.Name }}.\n\nYour release is named {{ .Release.Name }}.\n\nTo learn more about the release, try:\n\n  $ helm status {{ .Release.Name }}\n  $ helm get all {{ .Release.Name }}\n\n{{- if .Values.ingress.enabled }}\n\nApplication URL:\n{{- range .Values.ingress.hosts }}\n  http{{ if $.Values.ingress.tls }}s{{ end }}://{{ .host }}{{ .path }}\n{{- end }}\n{{- else }}\n\nGet the application URL by running:\n  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l \"app.kubernetes.io/name={{ include \"my-app.name\" . }}\" -o jsonpath=\"{.items[0].metadata.name}\")\n  kubectl port-forward $POD_NAME 8080:80\n  echo \"Visit http://127.0.0.1:8080\"\n{{- end }}\n```\n\n## Dependencies Management\n\n### Declaring Dependencies\n\n```yaml\n# Chart.yaml\ndependencies:\n  - name: postgresql\n    version: \"12.0.0\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: postgresql.enabled  # Enable/disable via values\n    tags:                          # Group dependencies\n      - database\n    import-values:                 # Import values from subchart\n      - child: database\n        parent: database\n    alias: db                      # Reference as .Values.db\n```\n\n### Managing Dependencies\n\n```bash\n# Update dependencies\nhelm dependency update\n\n# List dependencies\nhelm dependency list\n\n# Build dependencies\nhelm dependency build\n```\n\n### Chart.lock\n\nGenerated automatically by `helm dependency update`:\n\n```yaml\ndependencies:\n- name: postgresql\n  repository: https://charts.bitnami.com/bitnami\n  version: 12.0.0\ndigest: sha256:abcd1234...\ngenerated: \"2024-01-01T00:00:00Z\"\n```\n\n## .helmignore\n\nExclude files from chart package:\n\n```\n# Development files\n.git/\n.gitignore\n*.md\ndocs/\n\n# Build artifacts\n*.swp\n*.bak\n*.tmp\n*.orig\n\n# CI/CD\n.travis.yml\n.gitlab-ci.yml\nJenkinsfile\n\n# Testing\ntest/\n*.test\n\n# IDE\n.vscode/\n.idea/\n*.iml\n```\n\n## Custom Resource Definitions (CRDs)\n\nPlace CRDs in `crds/` directory:\n\n```\ncrds/\n\u251c\u2500\u2500 my-app-crd.yaml\n\u2514\u2500\u2500 another-crd.yaml\n```\n\n**Important CRD notes:**\n- CRDs are installed before any templates\n- CRDs are NOT templated (no `{{ }}` syntax)\n- CRDs are NOT upgraded or deleted with chart\n- Use `helm install --skip-crds` to skip installation\n\n## Chart Versioning\n\n### Semantic Versioning\n\n- **Chart Version**: Increment when chart changes\n  - MAJOR: Breaking changes\n  - MINOR: New features, backward compatible\n  - PATCH: Bug fixes\n\n- **App Version**: Application version being deployed\n  - Can be any string\n  - Not required to follow SemVer\n\n```yaml\nversion: 2.3.1      # Chart version\nappVersion: \"1.5.0\" # Application version\n```\n\n## Chart Testing\n\n### Test Files\n\n```yaml\n# templates/tests/test-connection.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: \"{{ include \"my-app.fullname\" . }}-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox\n    command: ['wget']\n    args: ['{{ include \"my-app.fullname\" . }}:{{ .Values.service.port }}']\n  restartPolicy: Never\n```\n\n### Running Tests\n\n```bash\nhelm test my-release\nhelm test my-release --logs\n```\n\n## Hooks\n\nHelm hooks allow intervention at specific points:\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"my-app.fullname\" . }}-migration\n  annotations:\n    \"helm.sh/hook\": pre-upgrade,pre-install\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n```\n\n### Hook Types\n\n- `pre-install`: Before templates rendered\n- `post-install`: After all resources loaded\n- `pre-delete`: Before any resources deleted\n- `post-delete`: After all resources deleted\n- `pre-upgrade`: Before upgrade\n- `post-upgrade`: After upgrade\n- `pre-rollback`: Before rollback\n- `post-rollback`: After rollback\n- `test`: Run with `helm test`\n\n### Hook Weight\n\nControls hook execution order (-5 to 5, lower runs first)\n\n### Hook Deletion Policies\n\n- `before-hook-creation`: Delete previous hook before new one\n- `hook-succeeded`: Delete after successful execution\n- `hook-failed`: Delete if hook fails\n\n## Best Practices\n\n1. **Use helpers** for repeated template logic\n2. **Quote strings** in templates: `{{ .Values.name | quote }}`\n3. **Validate values** with values.schema.json\n4. **Document all values** in values.yaml\n5. **Use semantic versioning** for chart versions\n6. **Pin dependency versions** exactly\n7. **Include NOTES.txt** with usage instructions\n8. **Add tests** for critical functionality\n9. **Use hooks** for database migrations\n10. **Keep charts focused** - one application per chart\n\n## Chart Repository Structure\n\n```\nhelm-charts/\n\u251c\u2500\u2500 index.yaml\n\u251c\u2500\u2500 my-app-1.0.0.tgz\n\u251c\u2500\u2500 my-app-1.1.0.tgz\n\u251c\u2500\u2500 my-app-1.2.0.tgz\n\u2514\u2500\u2500 another-chart-2.0.0.tgz\n```\n\n### Creating Repository Index\n\n```bash\nhelm repo index . --url https://charts.example.com\n```\n\n## Related Resources\n\n- [Helm Documentation](https://helm.sh/docs/)\n- [Chart Template Guide](https://helm.sh/docs/chart_template_guide/)\n- [Best Practices](https://helm.sh/docs/chart_best_practices/)\n"
      },
      "assets": {}
    },
    {
      "name": "k8s-manifest-generator",
      "description": "Create production-ready Kubernetes manifests for Deployments, Services, ConfigMaps, and Secrets following best practices and security standards. Use when generating Kubernetes YAML manifests, creating K8s resources, or implementing production-grade Kubernetes configurations.",
      "plugin": "kubernetes-operations",
      "source_path": "plugins/kubernetes-operations/skills/k8s-manifest-generator/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "kubernetes",
        "k8s",
        "containers",
        "helm",
        "argocd",
        "gitops"
      ],
      "content": "---\nname: k8s-manifest-generator\ndescription: Create production-ready Kubernetes manifests for Deployments, Services, ConfigMaps, and Secrets following best practices and security standards. Use when generating Kubernetes YAML manifests, creating K8s resources, or implementing production-grade Kubernetes configurations.\n---\n\n# Kubernetes Manifest Generator\n\nStep-by-step guidance for creating production-ready Kubernetes manifests including Deployments, Services, ConfigMaps, Secrets, and PersistentVolumeClaims.\n\n## Purpose\n\nThis skill provides comprehensive guidance for generating well-structured, secure, and production-ready Kubernetes manifests following cloud-native best practices and Kubernetes conventions.\n\n## When to Use This Skill\n\nUse this skill when you need to:\n- Create new Kubernetes Deployment manifests\n- Define Service resources for network connectivity\n- Generate ConfigMap and Secret resources for configuration management\n- Create PersistentVolumeClaim manifests for stateful workloads\n- Follow Kubernetes best practices and naming conventions\n- Implement resource limits, health checks, and security contexts\n- Design manifests for multi-environment deployments\n\n## Step-by-Step Workflow\n\n### 1. Gather Requirements\n\n**Understand the workload:**\n- Application type (stateless/stateful)\n- Container image and version\n- Environment variables and configuration needs\n- Storage requirements\n- Network exposure requirements (internal/external)\n- Resource requirements (CPU, memory)\n- Scaling requirements\n- Health check endpoints\n\n**Questions to ask:**\n- What is the application name and purpose?\n- What container image and tag will be used?\n- Does the application need persistent storage?\n- What ports does the application expose?\n- Are there any secrets or configuration files needed?\n- What are the CPU and memory requirements?\n- Does the application need to be exposed externally?\n\n### 2. Create Deployment Manifest\n\n**Follow this structure:**\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: <app-name>\n  namespace: <namespace>\n  labels:\n    app: <app-name>\n    version: <version>\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: <app-name>\n  template:\n    metadata:\n      labels:\n        app: <app-name>\n        version: <version>\n    spec:\n      containers:\n      - name: <container-name>\n        image: <image>:<tag>\n        ports:\n        - containerPort: <port>\n          name: http\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: http\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        env:\n        - name: ENV_VAR\n          value: \"value\"\n        envFrom:\n        - configMapRef:\n            name: <app-name>-config\n        - secretRef:\n            name: <app-name>-secret\n```\n\n**Best practices to apply:**\n- Always set resource requests and limits\n- Implement both liveness and readiness probes\n- Use specific image tags (never `:latest`)\n- Apply security context for non-root users\n- Use labels for organization and selection\n- Set appropriate replica count based on availability needs\n\n**Reference:** See `references/deployment-spec.md` for detailed deployment options\n\n### 3. Create Service Manifest\n\n**Choose the appropriate Service type:**\n\n**ClusterIP (internal only):**\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: <app-name>\n  namespace: <namespace>\n  labels:\n    app: <app-name>\nspec:\n  type: ClusterIP\n  selector:\n    app: <app-name>\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    protocol: TCP\n```\n\n**LoadBalancer (external access):**\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: <app-name>\n  namespace: <namespace>\n  labels:\n    app: <app-name>\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\nspec:\n  type: LoadBalancer\n  selector:\n    app: <app-name>\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    protocol: TCP\n```\n\n**Reference:** See `references/service-spec.md` for service types and networking\n\n### 4. Create ConfigMap\n\n**For application configuration:**\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: <app-name>-config\n  namespace: <namespace>\ndata:\n  APP_MODE: production\n  LOG_LEVEL: info\n  DATABASE_HOST: db.example.com\n  # For config files\n  app.properties: |\n    server.port=8080\n    server.host=0.0.0.0\n    logging.level=INFO\n```\n\n**Best practices:**\n- Use ConfigMaps for non-sensitive data only\n- Organize related configuration together\n- Use meaningful names for keys\n- Consider using one ConfigMap per component\n- Version ConfigMaps when making changes\n\n**Reference:** See `assets/configmap-template.yaml` for examples\n\n### 5. Create Secret\n\n**For sensitive data:**\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: <app-name>-secret\n  namespace: <namespace>\ntype: Opaque\nstringData:\n  DATABASE_PASSWORD: \"changeme\"\n  API_KEY: \"secret-api-key\"\n  # For certificate files\n  tls.crt: |\n    -----BEGIN CERTIFICATE-----\n    ...\n    -----END CERTIFICATE-----\n  tls.key: |\n    -----BEGIN PRIVATE KEY-----\n    ...\n    -----END PRIVATE KEY-----\n```\n\n**Security considerations:**\n- Never commit secrets to Git in plain text\n- Use Sealed Secrets, External Secrets Operator, or Vault\n- Rotate secrets regularly\n- Use RBAC to limit secret access\n- Consider using Secret type: `kubernetes.io/tls` for TLS secrets\n\n### 6. Create PersistentVolumeClaim (if needed)\n\n**For stateful applications:**\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: <app-name>-data\n  namespace: <namespace>\nspec:\n  accessModes:\n  - ReadWriteOnce\n  storageClassName: gp3\n  resources:\n    requests:\n      storage: 10Gi\n```\n\n**Mount in Deployment:**\n```yaml\nspec:\n  template:\n    spec:\n      containers:\n      - name: app\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/app\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: <app-name>-data\n```\n\n**Storage considerations:**\n- Choose appropriate StorageClass for performance needs\n- Use ReadWriteOnce for single-pod access\n- Use ReadWriteMany for multi-pod shared storage\n- Consider backup strategies\n- Set appropriate retention policies\n\n### 7. Apply Security Best Practices\n\n**Add security context to Deployment:**\n\n```yaml\nspec:\n  template:\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        fsGroup: 1000\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: app\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n```\n\n**Security checklist:**\n- [ ] Run as non-root user\n- [ ] Drop all capabilities\n- [ ] Use read-only root filesystem\n- [ ] Disable privilege escalation\n- [ ] Set seccomp profile\n- [ ] Use Pod Security Standards\n\n### 8. Add Labels and Annotations\n\n**Standard labels (recommended):**\n\n```yaml\nmetadata:\n  labels:\n    app.kubernetes.io/name: <app-name>\n    app.kubernetes.io/instance: <instance-name>\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/component: backend\n    app.kubernetes.io/part-of: <system-name>\n    app.kubernetes.io/managed-by: kubectl\n```\n\n**Useful annotations:**\n\n```yaml\nmetadata:\n  annotations:\n    description: \"Application description\"\n    contact: \"team@example.com\"\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9090\"\n    prometheus.io/path: \"/metrics\"\n```\n\n### 9. Organize Multi-Resource Manifests\n\n**File organization options:**\n\n**Option 1: Single file with `---` separator**\n```yaml\n# app-name.yaml\n---\napiVersion: v1\nkind: ConfigMap\n...\n---\napiVersion: v1\nkind: Secret\n...\n---\napiVersion: apps/v1\nkind: Deployment\n...\n---\napiVersion: v1\nkind: Service\n...\n```\n\n**Option 2: Separate files**\n```\nmanifests/\n\u251c\u2500\u2500 configmap.yaml\n\u251c\u2500\u2500 secret.yaml\n\u251c\u2500\u2500 deployment.yaml\n\u251c\u2500\u2500 service.yaml\n\u2514\u2500\u2500 pvc.yaml\n```\n\n**Option 3: Kustomize structure**\n```\nbase/\n\u251c\u2500\u2500 kustomization.yaml\n\u251c\u2500\u2500 deployment.yaml\n\u251c\u2500\u2500 service.yaml\n\u2514\u2500\u2500 configmap.yaml\noverlays/\n\u251c\u2500\u2500 dev/\n\u2502   \u2514\u2500\u2500 kustomization.yaml\n\u2514\u2500\u2500 prod/\n    \u2514\u2500\u2500 kustomization.yaml\n```\n\n### 10. Validate and Test\n\n**Validation steps:**\n\n```bash\n# Dry-run validation\nkubectl apply -f manifest.yaml --dry-run=client\n\n# Server-side validation\nkubectl apply -f manifest.yaml --dry-run=server\n\n# Validate with kubeval\nkubeval manifest.yaml\n\n# Validate with kube-score\nkube-score score manifest.yaml\n\n# Check with kube-linter\nkube-linter lint manifest.yaml\n```\n\n**Testing checklist:**\n- [ ] Manifest passes dry-run validation\n- [ ] All required fields are present\n- [ ] Resource limits are reasonable\n- [ ] Health checks are configured\n- [ ] Security context is set\n- [ ] Labels follow conventions\n- [ ] Namespace exists or is created\n\n## Common Patterns\n\n### Pattern 1: Simple Stateless Web Application\n\n**Use case:** Standard web API or microservice\n\n**Components needed:**\n- Deployment (3 replicas for HA)\n- ClusterIP Service\n- ConfigMap for configuration\n- Secret for API keys\n- HorizontalPodAutoscaler (optional)\n\n**Reference:** See `assets/deployment-template.yaml`\n\n### Pattern 2: Stateful Database Application\n\n**Use case:** Database or persistent storage application\n\n**Components needed:**\n- StatefulSet (not Deployment)\n- Headless Service\n- PersistentVolumeClaim template\n- ConfigMap for DB configuration\n- Secret for credentials\n\n### Pattern 3: Background Job or Cron\n\n**Use case:** Scheduled tasks or batch processing\n\n**Components needed:**\n- CronJob or Job\n- ConfigMap for job parameters\n- Secret for credentials\n- ServiceAccount with RBAC\n\n### Pattern 4: Multi-Container Pod\n\n**Use case:** Application with sidecar containers\n\n**Components needed:**\n- Deployment with multiple containers\n- Shared volumes between containers\n- Init containers for setup\n- Service (if needed)\n\n## Templates\n\nThe following templates are available in the `assets/` directory:\n\n- `deployment-template.yaml` - Standard deployment with best practices\n- `service-template.yaml` - Service configurations (ClusterIP, LoadBalancer, NodePort)\n- `configmap-template.yaml` - ConfigMap examples with different data types\n- `secret-template.yaml` - Secret examples (to be generated, not committed)\n- `pvc-template.yaml` - PersistentVolumeClaim templates\n\n## Reference Documentation\n\n- `references/deployment-spec.md` - Detailed Deployment specification\n- `references/service-spec.md` - Service types and networking details\n\n## Best Practices Summary\n\n1. **Always set resource requests and limits** - Prevents resource starvation\n2. **Implement health checks** - Ensures Kubernetes can manage your application\n3. **Use specific image tags** - Avoid unpredictable deployments\n4. **Apply security contexts** - Run as non-root, drop capabilities\n5. **Use ConfigMaps and Secrets** - Separate config from code\n6. **Label everything** - Enables filtering and organization\n7. **Follow naming conventions** - Use standard Kubernetes labels\n8. **Validate before applying** - Use dry-run and validation tools\n9. **Version your manifests** - Keep in Git with version control\n10. **Document with annotations** - Add context for other developers\n\n## Troubleshooting\n\n**Pods not starting:**\n- Check image pull errors: `kubectl describe pod <pod-name>`\n- Verify resource availability: `kubectl get nodes`\n- Check events: `kubectl get events --sort-by='.lastTimestamp'`\n\n**Service not accessible:**\n- Verify selector matches pod labels: `kubectl get endpoints <service-name>`\n- Check service type and port configuration\n- Test from within cluster: `kubectl run debug --rm -it --image=busybox -- sh`\n\n**ConfigMap/Secret not loading:**\n- Verify names match in Deployment\n- Check namespace\n- Ensure resources exist: `kubectl get configmap,secret`\n\n## Next Steps\n\nAfter creating manifests:\n1. Store in Git repository\n2. Set up CI/CD pipeline for deployment\n3. Consider using Helm or Kustomize for templating\n4. Implement GitOps with ArgoCD or Flux\n5. Add monitoring and observability\n\n## Related Skills\n\n- `helm-chart-scaffolding` - For templating and packaging\n- `gitops-workflow` - For automated deployments\n- `k8s-security-policies` - For advanced security configurations\n",
      "references": {
        "deployment-spec.md": "# Kubernetes Deployment Specification Reference\n\nComprehensive reference for Kubernetes Deployment resources, covering all key fields, best practices, and common patterns.\n\n## Overview\n\nA Deployment provides declarative updates for Pods and ReplicaSets. It manages the desired state of your application, handling rollouts, rollbacks, and scaling operations.\n\n## Complete Deployment Specification\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\n  namespace: production\n  labels:\n    app.kubernetes.io/name: my-app\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/component: backend\n    app.kubernetes.io/part-of: my-system\n  annotations:\n    description: \"Main application deployment\"\n    contact: \"backend-team@example.com\"\nspec:\n  # Replica management\n  replicas: 3\n  revisionHistoryLimit: 10\n\n  # Pod selection\n  selector:\n    matchLabels:\n      app: my-app\n      version: v1\n\n  # Update strategy\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n\n  # Minimum time for pod to be ready\n  minReadySeconds: 10\n\n  # Deployment will fail if it doesn't progress in this time\n  progressDeadlineSeconds: 600\n\n  # Pod template\n  template:\n    metadata:\n      labels:\n        app: my-app\n        version: v1\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"9090\"\n    spec:\n      # Service account for RBAC\n      serviceAccountName: my-app\n\n      # Security context for the pod\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        fsGroup: 1000\n        seccompProfile:\n          type: RuntimeDefault\n\n      # Init containers run before main containers\n      initContainers:\n      - name: init-db\n        image: busybox:1.36\n        command: ['sh', '-c', 'until nc -z db-service 5432; do sleep 1; done']\n        securityContext:\n          allowPrivilegeEscalation: false\n          runAsNonRoot: true\n          runAsUser: 1000\n\n      # Main containers\n      containers:\n      - name: app\n        image: myapp:1.0.0\n        imagePullPolicy: IfNotPresent\n\n        # Container ports\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        - name: metrics\n          containerPort: 9090\n          protocol: TCP\n\n        # Environment variables\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: url\n\n        # ConfigMap and Secret references\n        envFrom:\n        - configMapRef:\n            name: app-config\n        - secretRef:\n            name: app-secrets\n\n        # Resource requests and limits\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n\n        # Liveness probe\n        livenessProbe:\n          httpGet:\n            path: /health/live\n            port: http\n            httpHeaders:\n            - name: Custom-Header\n              value: Awesome\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n\n        # Readiness probe\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n\n        # Startup probe (for slow-starting containers)\n        startupProbe:\n          httpGet:\n            path: /health/startup\n            port: http\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 30\n\n        # Volume mounts\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/app\n        - name: config\n          mountPath: /etc/app\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n\n        # Security context for container\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          capabilities:\n            drop:\n            - ALL\n\n        # Lifecycle hooks\n        lifecycle:\n          postStart:\n            exec:\n              command: [\"/bin/sh\", \"-c\", \"echo Container started > /tmp/started\"]\n          preStop:\n            exec:\n              command: [\"/bin/sh\", \"-c\", \"sleep 15\"]\n\n      # Volumes\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: app-data\n      - name: config\n        configMap:\n          name: app-config\n      - name: tmp\n        emptyDir: {}\n\n      # DNS configuration\n      dnsPolicy: ClusterFirst\n      dnsConfig:\n        options:\n        - name: ndots\n          value: \"2\"\n\n      # Scheduling\n      nodeSelector:\n        disktype: ssd\n\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - my-app\n              topologyKey: kubernetes.io/hostname\n\n      tolerations:\n      - key: \"app\"\n        operator: \"Equal\"\n        value: \"my-app\"\n        effect: \"NoSchedule\"\n\n      # Termination\n      terminationGracePeriodSeconds: 30\n\n      # Image pull secrets\n      imagePullSecrets:\n      - name: regcred\n```\n\n## Field Reference\n\n### Metadata Fields\n\n#### Required Fields\n- `apiVersion`: `apps/v1` (current stable version)\n- `kind`: `Deployment`\n- `metadata.name`: Unique name within namespace\n\n#### Recommended Metadata\n- `metadata.namespace`: Target namespace (defaults to `default`)\n- `metadata.labels`: Key-value pairs for organization\n- `metadata.annotations`: Non-identifying metadata\n\n### Spec Fields\n\n#### Replica Management\n\n**`replicas`** (integer, default: 1)\n- Number of desired pod instances\n- Best practice: Use 3+ for production high availability\n- Can be scaled manually or via HorizontalPodAutoscaler\n\n**`revisionHistoryLimit`** (integer, default: 10)\n- Number of old ReplicaSets to retain for rollback\n- Set to 0 to disable rollback capability\n- Reduces storage overhead for long-running deployments\n\n#### Update Strategy\n\n**`strategy.type`** (string)\n- `RollingUpdate` (default): Gradual pod replacement\n- `Recreate`: Delete all pods before creating new ones\n\n**`strategy.rollingUpdate.maxSurge`** (int or percent, default: 25%)\n- Maximum pods above desired replicas during update\n- Example: With 3 replicas and maxSurge=1, up to 4 pods during update\n\n**`strategy.rollingUpdate.maxUnavailable`** (int or percent, default: 25%)\n- Maximum pods below desired replicas during update\n- Set to 0 for zero-downtime deployments\n- Cannot be 0 if maxSurge is 0\n\n**Best practices:**\n```yaml\n# Zero-downtime deployment\nstrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxSurge: 1\n    maxUnavailable: 0\n\n# Fast deployment (can have brief downtime)\nstrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxSurge: 2\n    maxUnavailable: 1\n\n# Complete replacement\nstrategy:\n  type: Recreate\n```\n\n#### Pod Template\n\n**`template.metadata.labels`**\n- Must include labels matching `spec.selector.matchLabels`\n- Add version labels for blue/green deployments\n- Include standard Kubernetes labels\n\n**`template.spec.containers`** (required)\n- Array of container specifications\n- At least one container required\n- Each container needs unique name\n\n#### Container Configuration\n\n**Image Management:**\n```yaml\ncontainers:\n- name: app\n  image: registry.example.com/myapp:1.0.0\n  imagePullPolicy: IfNotPresent  # or Always, Never\n```\n\nImage pull policies:\n- `IfNotPresent`: Pull if not cached (default for tagged images)\n- `Always`: Always pull (default for :latest)\n- `Never`: Never pull, fail if not cached\n\n**Port Declarations:**\n```yaml\nports:\n- name: http      # Named for referencing in Service\n  containerPort: 8080\n  protocol: TCP   # TCP (default), UDP, or SCTP\n  hostPort: 8080  # Optional: Bind to host port (rarely used)\n```\n\n#### Resource Management\n\n**Requests vs Limits:**\n\n```yaml\nresources:\n  requests:\n    memory: \"256Mi\"  # Guaranteed resources\n    cpu: \"250m\"      # 0.25 CPU cores\n  limits:\n    memory: \"512Mi\"  # Maximum allowed\n    cpu: \"500m\"      # 0.5 CPU cores\n```\n\n**QoS Classes (determined automatically):**\n\n1. **Guaranteed**: requests = limits for all containers\n   - Highest priority\n   - Last to be evicted\n\n2. **Burstable**: requests < limits or only requests set\n   - Medium priority\n   - Evicted before Guaranteed\n\n3. **BestEffort**: No requests or limits set\n   - Lowest priority\n   - First to be evicted\n\n**Best practices:**\n- Always set requests in production\n- Set limits to prevent resource monopolization\n- Memory limits should be 1.5-2x requests\n- CPU limits can be higher for bursty workloads\n\n#### Health Checks\n\n**Probe Types:**\n\n1. **startupProbe** - For slow-starting applications\n   ```yaml\n   startupProbe:\n     httpGet:\n       path: /health/startup\n       port: 8080\n     initialDelaySeconds: 0\n     periodSeconds: 10\n     failureThreshold: 30  # 5 minutes to start (10s * 30)\n   ```\n\n2. **livenessProbe** - Restarts unhealthy containers\n   ```yaml\n   livenessProbe:\n     httpGet:\n       path: /health/live\n       port: 8080\n     initialDelaySeconds: 30\n     periodSeconds: 10\n     timeoutSeconds: 5\n     failureThreshold: 3  # Restart after 3 failures\n   ```\n\n3. **readinessProbe** - Controls traffic routing\n   ```yaml\n   readinessProbe:\n     httpGet:\n       path: /health/ready\n       port: 8080\n     initialDelaySeconds: 5\n     periodSeconds: 5\n     failureThreshold: 3  # Remove from service after 3 failures\n   ```\n\n**Probe Mechanisms:**\n\n```yaml\n# HTTP GET\nhttpGet:\n  path: /health\n  port: 8080\n  httpHeaders:\n  - name: Authorization\n    value: Bearer token\n\n# TCP Socket\ntcpSocket:\n  port: 3306\n\n# Command execution\nexec:\n  command:\n  - cat\n  - /tmp/healthy\n\n# gRPC (Kubernetes 1.24+)\ngrpc:\n  port: 9090\n  service: my.service.health.v1.Health\n```\n\n**Probe Timing Parameters:**\n\n- `initialDelaySeconds`: Wait before first probe\n- `periodSeconds`: How often to probe\n- `timeoutSeconds`: Probe timeout\n- `successThreshold`: Successes needed to mark healthy (1 for liveness/startup)\n- `failureThreshold`: Failures before taking action\n\n#### Security Context\n\n**Pod-level security context:**\n```yaml\nspec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n    runAsGroup: 1000\n    fsGroup: 1000\n    fsGroupChangePolicy: OnRootMismatch\n    seccompProfile:\n      type: RuntimeDefault\n```\n\n**Container-level security context:**\n```yaml\ncontainers:\n- name: app\n  securityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    runAsNonRoot: true\n    runAsUser: 1000\n    capabilities:\n      drop:\n      - ALL\n      add:\n      - NET_BIND_SERVICE  # Only if needed\n```\n\n**Security best practices:**\n- Always run as non-root (`runAsNonRoot: true`)\n- Drop all capabilities and add only needed ones\n- Use read-only root filesystem when possible\n- Enable seccomp profile\n- Disable privilege escalation\n\n#### Volumes\n\n**Volume Types:**\n\n```yaml\nvolumes:\n# PersistentVolumeClaim\n- name: data\n  persistentVolumeClaim:\n    claimName: app-data\n\n# ConfigMap\n- name: config\n  configMap:\n    name: app-config\n    items:\n    - key: app.properties\n      path: application.properties\n\n# Secret\n- name: secrets\n  secret:\n    secretName: app-secrets\n    defaultMode: 0400\n\n# EmptyDir (ephemeral)\n- name: cache\n  emptyDir:\n    sizeLimit: 1Gi\n\n# HostPath (avoid in production)\n- name: host-data\n  hostPath:\n    path: /data\n    type: DirectoryOrCreate\n```\n\n#### Scheduling\n\n**Node Selection:**\n\n```yaml\n# Simple node selector\nnodeSelector:\n  disktype: ssd\n  zone: us-west-1a\n\n# Node affinity (more expressive)\naffinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/arch\n          operator: In\n          values:\n          - amd64\n          - arm64\n```\n\n**Pod Affinity/Anti-Affinity:**\n\n```yaml\n# Spread pods across nodes\naffinity:\n  podAntiAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n    - labelSelector:\n        matchLabels:\n          app: my-app\n      topologyKey: kubernetes.io/hostname\n\n# Co-locate with database\naffinity:\n  podAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n    - weight: 100\n      podAffinityTerm:\n        labelSelector:\n          matchLabels:\n            app: database\n        topologyKey: kubernetes.io/hostname\n```\n\n**Tolerations:**\n\n```yaml\ntolerations:\n- key: \"node.kubernetes.io/unreachable\"\n  operator: \"Exists\"\n  effect: \"NoExecute\"\n  tolerationSeconds: 30\n- key: \"dedicated\"\n  operator: \"Equal\"\n  value: \"database\"\n  effect: \"NoSchedule\"\n```\n\n## Common Patterns\n\n### High Availability Deployment\n\n```yaml\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: my-app\n            topologyKey: kubernetes.io/hostname\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: topology.kubernetes.io/zone\n        whenUnsatisfiable: DoNotSchedule\n        labelSelector:\n          matchLabels:\n            app: my-app\n```\n\n### Sidecar Container Pattern\n\n```yaml\nspec:\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myapp:1.0.0\n        volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log\n      - name: log-forwarder\n        image: fluent-bit:2.0\n        volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log\n          readOnly: true\n      volumes:\n      - name: shared-logs\n        emptyDir: {}\n```\n\n### Init Container for Dependencies\n\n```yaml\nspec:\n  template:\n    spec:\n      initContainers:\n      - name: wait-for-db\n        image: busybox:1.36\n        command:\n        - sh\n        - -c\n        - |\n          until nc -z database-service 5432; do\n            echo \"Waiting for database...\"\n            sleep 2\n          done\n      - name: run-migrations\n        image: myapp:1.0.0\n        command: [\"./migrate\", \"up\"]\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: url\n      containers:\n      - name: app\n        image: myapp:1.0.0\n```\n\n## Best Practices\n\n### Production Checklist\n\n- [ ] Set resource requests and limits\n- [ ] Implement all three probe types (startup, liveness, readiness)\n- [ ] Use specific image tags (not :latest)\n- [ ] Configure security context (non-root, read-only filesystem)\n- [ ] Set replica count >= 3 for HA\n- [ ] Configure pod anti-affinity for spread\n- [ ] Set appropriate update strategy (maxUnavailable: 0 for zero-downtime)\n- [ ] Use ConfigMaps and Secrets for configuration\n- [ ] Add standard labels and annotations\n- [ ] Configure graceful shutdown (preStop hook, terminationGracePeriodSeconds)\n- [ ] Set revisionHistoryLimit for rollback capability\n- [ ] Use ServiceAccount with minimal RBAC permissions\n\n### Performance Tuning\n\n**Fast startup:**\n```yaml\nspec:\n  minReadySeconds: 5\n  strategy:\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n```\n\n**Zero-downtime updates:**\n```yaml\nspec:\n  minReadySeconds: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n```\n\n**Graceful shutdown:**\n```yaml\nspec:\n  template:\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: app\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\", \"-c\", \"sleep 15 && kill -SIGTERM 1\"]\n```\n\n## Troubleshooting\n\n### Common Issues\n\n**Pods not starting:**\n```bash\nkubectl describe deployment <name>\nkubectl get pods -l app=<app-name>\nkubectl describe pod <pod-name>\nkubectl logs <pod-name>\n```\n\n**ImagePullBackOff:**\n- Check image name and tag\n- Verify imagePullSecrets\n- Check registry credentials\n\n**CrashLoopBackOff:**\n- Check container logs\n- Verify liveness probe is not too aggressive\n- Check resource limits\n- Verify application dependencies\n\n**Deployment stuck in progress:**\n- Check progressDeadlineSeconds\n- Verify readiness probes\n- Check resource availability\n\n## Related Resources\n\n- [Kubernetes Deployment API Reference](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#deployment-v1-apps)\n- [Pod Security Standards](https://kubernetes.io/docs/concepts/security/pod-security-standards/)\n- [Resource Management](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)\n",
        "service-spec.md": "# Kubernetes Service Specification Reference\n\nComprehensive reference for Kubernetes Service resources, covering service types, networking, load balancing, and service discovery patterns.\n\n## Overview\n\nA Service provides stable network endpoints for accessing Pods. Services enable loose coupling between microservices by providing service discovery and load balancing.\n\n## Service Types\n\n### 1. ClusterIP (Default)\n\nExposes the service on an internal cluster IP. Only reachable from within the cluster.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend-service\n  namespace: production\nspec:\n  type: ClusterIP\n  selector:\n    app: backend\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    protocol: TCP\n  sessionAffinity: None\n```\n\n**Use cases:**\n- Internal microservice communication\n- Database services\n- Internal APIs\n- Message queues\n\n### 2. NodePort\n\nExposes the service on each Node's IP at a static port (30000-32767 range).\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend-service\nspec:\n  type: NodePort\n  selector:\n    app: frontend\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    nodePort: 30080  # Optional, auto-assigned if omitted\n    protocol: TCP\n```\n\n**Use cases:**\n- Development/testing external access\n- Small deployments without load balancer\n- Direct node access requirements\n\n**Limitations:**\n- Limited port range (30000-32767)\n- Must handle node failures\n- No built-in load balancing across nodes\n\n### 3. LoadBalancer\n\nExposes the service using a cloud provider's load balancer.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: public-api\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n    service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: api\n  ports:\n  - name: https\n    port: 443\n    targetPort: 8443\n    protocol: TCP\n  loadBalancerSourceRanges:\n  - 203.0.113.0/24\n```\n\n**Cloud-specific annotations:**\n\n**AWS:**\n```yaml\nannotations:\n  service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"  # or \"external\"\n  service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\n  service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \"true\"\n  service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:...\"\n  service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n```\n\n**Azure:**\n```yaml\nannotations:\n  service.beta.kubernetes.io/azure-load-balancer-internal: \"true\"\n  service.beta.kubernetes.io/azure-pip-name: \"my-public-ip\"\n```\n\n**GCP:**\n```yaml\nannotations:\n  cloud.google.com/load-balancer-type: \"Internal\"\n  cloud.google.com/backend-config: '{\"default\": \"my-backend-config\"}'\n```\n\n### 4. ExternalName\n\nMaps service to external DNS name (CNAME record).\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: external-db\nspec:\n  type: ExternalName\n  externalName: db.external.example.com\n  ports:\n  - port: 5432\n```\n\n**Use cases:**\n- Accessing external services\n- Service migration scenarios\n- Multi-cluster service references\n\n## Complete Service Specification\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  namespace: production\n  labels:\n    app: my-app\n    tier: backend\n  annotations:\n    description: \"Main application service\"\n    prometheus.io/scrape: \"true\"\nspec:\n  # Service type\n  type: ClusterIP\n\n  # Pod selector\n  selector:\n    app: my-app\n    version: v1\n\n  # Ports configuration\n  ports:\n  - name: http\n    port: 80           # Service port\n    targetPort: 8080   # Container port (or named port)\n    protocol: TCP      # TCP, UDP, or SCTP\n\n  # Session affinity\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 10800\n\n  # IP configuration\n  clusterIP: 10.0.0.10  # Optional: specific IP\n  clusterIPs:\n  - 10.0.0.10\n  ipFamilies:\n  - IPv4\n  ipFamilyPolicy: SingleStack\n\n  # External traffic policy\n  externalTrafficPolicy: Local\n\n  # Internal traffic policy\n  internalTrafficPolicy: Local\n\n  # Health check\n  healthCheckNodePort: 30000\n\n  # Load balancer config (for type: LoadBalancer)\n  loadBalancerIP: 203.0.113.100\n  loadBalancerSourceRanges:\n  - 203.0.113.0/24\n\n  # External IPs\n  externalIPs:\n  - 80.11.12.10\n\n  # Publishing strategy\n  publishNotReadyAddresses: false\n```\n\n## Port Configuration\n\n### Named Ports\n\nUse named ports in Pods for flexibility:\n\n**Deployment:**\n```yaml\nspec:\n  template:\n    spec:\n      containers:\n      - name: app\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 9090\n```\n\n**Service:**\n```yaml\nspec:\n  ports:\n  - name: http\n    port: 80\n    targetPort: http  # References named port\n  - name: metrics\n    port: 9090\n    targetPort: metrics\n```\n\n### Multiple Ports\n\n```yaml\nspec:\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    protocol: TCP\n  - name: https\n    port: 443\n    targetPort: 8443\n    protocol: TCP\n  - name: grpc\n    port: 9090\n    targetPort: 9090\n    protocol: TCP\n```\n\n## Session Affinity\n\n### None (Default)\n\nDistributes requests randomly across pods.\n\n```yaml\nspec:\n  sessionAffinity: None\n```\n\n### ClientIP\n\nRoutes requests from same client IP to same pod.\n\n```yaml\nspec:\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 10800  # 3 hours\n```\n\n**Use cases:**\n- Stateful applications\n- Session-based applications\n- WebSocket connections\n\n## Traffic Policies\n\n### External Traffic Policy\n\n**Cluster (Default):**\n```yaml\nspec:\n  externalTrafficPolicy: Cluster\n```\n- Load balances across all nodes\n- May add extra network hop\n- Source IP is masked\n\n**Local:**\n```yaml\nspec:\n  externalTrafficPolicy: Local\n```\n- Traffic goes only to pods on receiving node\n- Preserves client source IP\n- Better performance (no extra hop)\n- May cause imbalanced load\n\n### Internal Traffic Policy\n\n```yaml\nspec:\n  internalTrafficPolicy: Local  # or Cluster\n```\n\nControls traffic routing for cluster-internal clients.\n\n## Headless Services\n\nService without cluster IP for direct pod access.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: database\nspec:\n  clusterIP: None  # Headless\n  selector:\n    app: database\n  ports:\n  - port: 5432\n    targetPort: 5432\n```\n\n**Use cases:**\n- StatefulSet pod discovery\n- Direct pod-to-pod communication\n- Custom load balancing\n- Database clusters\n\n**DNS returns:**\n- Individual pod IPs instead of service IP\n- Format: `<pod-name>.<service-name>.<namespace>.svc.cluster.local`\n\n## Service Discovery\n\n### DNS\n\n**ClusterIP Service:**\n```\n<service-name>.<namespace>.svc.cluster.local\n```\n\nExample:\n```bash\ncurl http://backend-service.production.svc.cluster.local\n```\n\n**Within same namespace:**\n```bash\ncurl http://backend-service\n```\n\n**Headless Service (returns pod IPs):**\n```\n<pod-name>.<service-name>.<namespace>.svc.cluster.local\n```\n\n### Environment Variables\n\nKubernetes injects service info into pods:\n\n```bash\n# Service host and port\nBACKEND_SERVICE_SERVICE_HOST=10.0.0.100\nBACKEND_SERVICE_SERVICE_PORT=80\n\n# For named ports\nBACKEND_SERVICE_SERVICE_PORT_HTTP=80\n```\n\n**Note:** Pods must be created after the service for env vars to be injected.\n\n## Load Balancing\n\n### Algorithms\n\nKubernetes uses random selection by default. For advanced load balancing:\n\n**Service Mesh (Istio example):**\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: my-destination-rule\nspec:\n  host: my-service\n  trafficPolicy:\n    loadBalancer:\n      simple: LEAST_REQUEST  # or ROUND_ROBIN, RANDOM, PASSTHROUGH\n    connectionPool:\n      tcp:\n        maxConnections: 100\n```\n\n### Connection Limits\n\nUse pod disruption budgets and resource limits:\n\n```yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: my-app-pdb\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: my-app\n```\n\n## Service Mesh Integration\n\n### Istio Virtual Service\n\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: my-service\nspec:\n  hosts:\n  - my-service\n  http:\n  - match:\n    - headers:\n        version:\n          exact: v2\n    route:\n    - destination:\n        host: my-service\n        subset: v2\n  - route:\n    - destination:\n        host: my-service\n        subset: v1\n      weight: 90\n    - destination:\n        host: my-service\n        subset: v2\n      weight: 10\n```\n\n## Common Patterns\n\n### Pattern 1: Internal Microservice\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-service\n  namespace: backend\n  labels:\n    app: user-service\n    tier: backend\nspec:\n  type: ClusterIP\n  selector:\n    app: user-service\n  ports:\n  - name: http\n    port: 8080\n    targetPort: http\n    protocol: TCP\n  - name: grpc\n    port: 9090\n    targetPort: grpc\n    protocol: TCP\n```\n\n### Pattern 2: Public API with Load Balancer\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-gateway\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:...\"\nspec:\n  type: LoadBalancer\n  externalTrafficPolicy: Local\n  selector:\n    app: api-gateway\n  ports:\n  - name: https\n    port: 443\n    targetPort: 8443\n    protocol: TCP\n  loadBalancerSourceRanges:\n  - 0.0.0.0/0\n```\n\n### Pattern 3: StatefulSet with Headless Service\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: cassandra\nspec:\n  clusterIP: None\n  selector:\n    app: cassandra\n  ports:\n  - port: 9042\n    targetPort: 9042\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: cassandra\nspec:\n  serviceName: cassandra\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cassandra\n  template:\n    metadata:\n      labels:\n        app: cassandra\n    spec:\n      containers:\n      - name: cassandra\n        image: cassandra:4.0\n```\n\n### Pattern 4: External Service Mapping\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: external-database\nspec:\n  type: ExternalName\n  externalName: prod-db.cxyz.us-west-2.rds.amazonaws.com\n---\n# Or with Endpoints for IP-based external service\napiVersion: v1\nkind: Service\nmetadata:\n  name: external-api\nspec:\n  ports:\n  - port: 443\n    targetPort: 443\n    protocol: TCP\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: external-api\nsubsets:\n- addresses:\n  - ip: 203.0.113.100\n  ports:\n  - port: 443\n```\n\n### Pattern 5: Multi-Port Service with Metrics\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-app\n  annotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9090\"\n    prometheus.io/path: \"/metrics\"\nspec:\n  type: ClusterIP\n  selector:\n    app: web-app\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n  - name: metrics\n    port: 9090\n    targetPort: 9090\n```\n\n## Network Policies\n\nControl traffic to services:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n```\n\n## Best Practices\n\n### Service Configuration\n\n1. **Use named ports** for flexibility\n2. **Set appropriate service type** based on exposure needs\n3. **Use labels and selectors consistently** across Deployments and Services\n4. **Configure session affinity** for stateful apps\n5. **Set external traffic policy to Local** for IP preservation\n6. **Use headless services** for StatefulSets\n7. **Implement network policies** for security\n8. **Add monitoring annotations** for observability\n\n### Production Checklist\n\n- [ ] Service type appropriate for use case\n- [ ] Selector matches pod labels\n- [ ] Named ports used for clarity\n- [ ] Session affinity configured if needed\n- [ ] Traffic policy set appropriately\n- [ ] Load balancer annotations configured (if applicable)\n- [ ] Source IP ranges restricted (for public services)\n- [ ] Health check configuration validated\n- [ ] Monitoring annotations added\n- [ ] Network policies defined\n\n### Performance Tuning\n\n**For high traffic:**\n```yaml\nspec:\n  externalTrafficPolicy: Local\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 3600\n```\n\n**For WebSocket/long connections:**\n```yaml\nspec:\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 86400  # 24 hours\n```\n\n## Troubleshooting\n\n### Service not accessible\n\n```bash\n# Check service exists\nkubectl get service <service-name>\n\n# Check endpoints (should show pod IPs)\nkubectl get endpoints <service-name>\n\n# Describe service\nkubectl describe service <service-name>\n\n# Check if pods match selector\nkubectl get pods -l app=<app-name>\n```\n\n**Common issues:**\n- Selector doesn't match pod labels\n- No pods running (endpoints empty)\n- Ports misconfigured\n- Network policy blocking traffic\n\n### DNS resolution failing\n\n```bash\n# Test DNS from pod\nkubectl run debug --rm -it --image=busybox -- nslookup <service-name>\n\n# Check CoreDNS\nkubectl get pods -n kube-system -l k8s-app=kube-dns\nkubectl logs -n kube-system -l k8s-app=kube-dns\n```\n\n### Load balancer issues\n\n```bash\n# Check load balancer status\nkubectl describe service <service-name>\n\n# Check events\nkubectl get events --sort-by='.lastTimestamp'\n\n# Verify cloud provider configuration\nkubectl describe node\n```\n\n## Related Resources\n\n- [Kubernetes Service API Reference](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#service-v1-core)\n- [Service Networking](https://kubernetes.io/docs/concepts/services-networking/service/)\n- [DNS for Services and Pods](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)\n"
      },
      "assets": {}
    },
    {
      "name": "k8s-security-policies",
      "description": "Implement Kubernetes security policies including NetworkPolicy, PodSecurityPolicy, and RBAC for production-grade security. Use when securing Kubernetes clusters, implementing network isolation, or enforcing pod security standards.",
      "plugin": "kubernetes-operations",
      "source_path": "plugins/kubernetes-operations/skills/k8s-security-policies/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "kubernetes",
        "k8s",
        "containers",
        "helm",
        "argocd",
        "gitops"
      ],
      "content": "---\nname: k8s-security-policies\ndescription: Implement Kubernetes security policies including NetworkPolicy, PodSecurityPolicy, and RBAC for production-grade security. Use when securing Kubernetes clusters, implementing network isolation, or enforcing pod security standards.\n---\n\n# Kubernetes Security Policies\n\nComprehensive guide for implementing NetworkPolicy, PodSecurityPolicy, RBAC, and Pod Security Standards in Kubernetes.\n\n## Purpose\n\nImplement defense-in-depth security for Kubernetes clusters using network policies, pod security standards, and RBAC.\n\n## When to Use This Skill\n\n- Implement network segmentation\n- Configure pod security standards\n- Set up RBAC for least-privilege access\n- Create security policies for compliance\n- Implement admission control\n- Secure multi-tenant clusters\n\n## Pod Security Standards\n\n### 1. Privileged (Unrestricted)\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: privileged-ns\n  labels:\n    pod-security.kubernetes.io/enforce: privileged\n    pod-security.kubernetes.io/audit: privileged\n    pod-security.kubernetes.io/warn: privileged\n```\n\n### 2. Baseline (Minimally restrictive)\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: baseline-ns\n  labels:\n    pod-security.kubernetes.io/enforce: baseline\n    pod-security.kubernetes.io/audit: baseline\n    pod-security.kubernetes.io/warn: baseline\n```\n\n### 3. Restricted (Most restrictive)\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: restricted-ns\n  labels:\n    pod-security.kubernetes.io/enforce: restricted\n    pod-security.kubernetes.io/audit: restricted\n    pod-security.kubernetes.io/warn: restricted\n```\n\n## Network Policies\n\n### Default Deny All\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n```\n\n### Allow Frontend to Backend\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n```\n\n### Allow DNS\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-dns\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n    ports:\n    - protocol: UDP\n      port: 53\n```\n\n**Reference:** See `assets/network-policy-template.yaml`\n\n## RBAC Configuration\n\n### Role (Namespace-scoped)\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-reader\n  namespace: production\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n```\n\n### ClusterRole (Cluster-wide)\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: secret-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n```\n\n### RoleBinding\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: production\nsubjects:\n- kind: User\n  name: jane\n  apiGroup: rbac.authorization.k8s.io\n- kind: ServiceAccount\n  name: default\n  namespace: production\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n```\n\n**Reference:** See `references/rbac-patterns.md`\n\n## Pod Security Context\n\n### Restricted Pod\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secure-pod\nspec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n    fsGroup: 1000\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n  - name: app\n    image: myapp:1.0\n    securityContext:\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      capabilities:\n        drop:\n        - ALL\n```\n\n## Policy Enforcement with OPA Gatekeeper\n\n### ConstraintTemplate\n```yaml\napiVersion: templates.gatekeeper.sh/v1\nkind: ConstraintTemplate\nmetadata:\n  name: k8srequiredlabels\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sRequiredLabels\n      validation:\n        openAPIV3Schema:\n          type: object\n          properties:\n            labels:\n              type: array\n              items:\n                type: string\n  targets:\n    - target: admission.k8s.gatekeeper.sh\n      rego: |\n        package k8srequiredlabels\n        violation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] {\n          provided := {label | input.review.object.metadata.labels[label]}\n          required := {label | label := input.parameters.labels[_]}\n          missing := required - provided\n          count(missing) > 0\n          msg := sprintf(\"missing required labels: %v\", [missing])\n        }\n```\n\n### Constraint\n```yaml\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sRequiredLabels\nmetadata:\n  name: require-app-label\nspec:\n  match:\n    kinds:\n      - apiGroups: [\"apps\"]\n        kinds: [\"Deployment\"]\n  parameters:\n    labels: [\"app\", \"environment\"]\n```\n\n## Service Mesh Security (Istio)\n\n### PeerAuthentication (mTLS)\n```yaml\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: production\nspec:\n  mtls:\n    mode: STRICT\n```\n\n### AuthorizationPolicy\n```yaml\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: allow-frontend\n  namespace: production\nspec:\n  selector:\n    matchLabels:\n      app: backend\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/production/sa/frontend\"]\n```\n\n## Best Practices\n\n1. **Implement Pod Security Standards** at namespace level\n2. **Use Network Policies** for network segmentation\n3. **Apply least-privilege RBAC** for all service accounts\n4. **Enable admission control** (OPA Gatekeeper/Kyverno)\n5. **Run containers as non-root**\n6. **Use read-only root filesystem**\n7. **Drop all capabilities** unless needed\n8. **Implement resource quotas** and limit ranges\n9. **Enable audit logging** for security events\n10. **Regular security scanning** of images\n\n## Compliance Frameworks\n\n### CIS Kubernetes Benchmark\n- Use RBAC authorization\n- Enable audit logging\n- Use Pod Security Standards\n- Configure network policies\n- Implement secrets encryption at rest\n- Enable node authentication\n\n### NIST Cybersecurity Framework\n- Implement defense in depth\n- Use network segmentation\n- Configure security monitoring\n- Implement access controls\n- Enable logging and monitoring\n\n## Troubleshooting\n\n**NetworkPolicy not working:**\n```bash\n# Check if CNI supports NetworkPolicy\nkubectl get nodes -o wide\nkubectl describe networkpolicy <name>\n```\n\n**RBAC permission denied:**\n```bash\n# Check effective permissions\nkubectl auth can-i list pods --as system:serviceaccount:default:my-sa\nkubectl auth can-i '*' '*' --as system:serviceaccount:default:my-sa\n```\n\n## Reference Files\n\n- `assets/network-policy-template.yaml` - Network policy examples\n- `assets/pod-security-template.yaml` - Pod security policies\n- `references/rbac-patterns.md` - RBAC configuration patterns\n\n## Related Skills\n\n- `k8s-manifest-generator` - For creating secure manifests\n- `gitops-workflow` - For automated policy deployment\n",
      "references": {
        "rbac-patterns.md": "# RBAC Patterns and Best Practices\n\n## Common RBAC Patterns\n\n### Pattern 1: Read-Only Access\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: read-only\nrules:\n- apiGroups: [\"\", \"apps\", \"batch\"]\n  resources: [\"*\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n```\n\n### Pattern 2: Namespace Admin\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: namespace-admin\n  namespace: production\nrules:\n- apiGroups: [\"\", \"apps\", \"batch\", \"extensions\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n```\n\n### Pattern 3: Deployment Manager\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: deployment-manager\n  namespace: production\nrules:\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n```\n\n### Pattern 4: Secret Reader (ServiceAccount)\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: secret-reader\n  namespace: production\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\"]\n  resourceNames: [\"app-secrets\"]  # Specific secret only\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: app-secret-reader\n  namespace: production\nsubjects:\n- kind: ServiceAccount\n  name: my-app\n  namespace: production\nroleRef:\n  kind: Role\n  name: secret-reader\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Pattern 5: CI/CD Pipeline Access\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cicd-deployer\nrules:\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"replicasets\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\"]\n- apiGroups: [\"\"]\n  resources: [\"services\", \"configmaps\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n```\n\n## ServiceAccount Best Practices\n\n### Create Dedicated ServiceAccounts\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-app\n  namespace: production\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  template:\n    spec:\n      serviceAccountName: my-app\n      automountServiceAccountToken: false  # Disable if not needed\n```\n\n### Least-Privilege ServiceAccount\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: my-app-role\n  namespace: production\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"get\"]\n  resourceNames: [\"my-app-config\"]\n```\n\n## Security Best Practices\n\n1. **Use Roles over ClusterRoles** when possible\n2. **Specify resourceNames** for fine-grained access\n3. **Avoid wildcard permissions** (`*`) in production\n4. **Create dedicated ServiceAccounts** for each app\n5. **Disable token auto-mounting** if not needed\n6. **Regular RBAC audits** to remove unused permissions\n7. **Use groups** for user management\n8. **Implement namespace isolation**\n9. **Monitor RBAC usage** with audit logs\n10. **Document role purposes** in metadata\n\n## Troubleshooting RBAC\n\n### Check User Permissions\n```bash\nkubectl auth can-i list pods --as john@example.com\nkubectl auth can-i '*' '*' --as system:serviceaccount:default:my-app\n```\n\n### View Effective Permissions\n```bash\nkubectl describe clusterrole cluster-admin\nkubectl describe rolebinding -n production\n```\n\n### Debug Access Issues\n```bash\nkubectl get rolebindings,clusterrolebindings --all-namespaces -o wide | grep my-user\n```\n\n## Common RBAC Verbs\n\n- `get` - Read a specific resource\n- `list` - List all resources of a type\n- `watch` - Watch for resource changes\n- `create` - Create new resources\n- `update` - Update existing resources\n- `patch` - Partially update resources\n- `delete` - Delete resources\n- `deletecollection` - Delete multiple resources\n- `*` - All verbs (avoid in production)\n\n## Resource Scope\n\n### Cluster-Scoped Resources\n- Nodes\n- PersistentVolumes\n- ClusterRoles\n- ClusterRoleBindings\n- Namespaces\n\n### Namespace-Scoped Resources\n- Pods\n- Services\n- Deployments\n- ConfigMaps\n- Secrets\n- Roles\n- RoleBindings\n"
      },
      "assets": {}
    },
    {
      "name": "cost-optimization",
      "description": "Optimize cloud costs through resource rightsizing, tagging strategies, reserved instances, and spending analysis. Use when reducing cloud expenses, analyzing infrastructure costs, or implementing cost governance policies.",
      "plugin": "cloud-infrastructure",
      "source_path": "plugins/cloud-infrastructure/skills/cost-optimization/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "cloud",
        "aws",
        "azure",
        "gcp",
        "kubernetes",
        "terraform",
        "infrastructure"
      ],
      "content": "---\nname: cost-optimization\ndescription: Optimize cloud costs through resource rightsizing, tagging strategies, reserved instances, and spending analysis. Use when reducing cloud expenses, analyzing infrastructure costs, or implementing cost governance policies.\n---\n\n# Cloud Cost Optimization\n\nStrategies and patterns for optimizing cloud costs across AWS, Azure, and GCP.\n\n## Purpose\n\nImplement systematic cost optimization strategies to reduce cloud spending while maintaining performance and reliability.\n\n## When to Use\n\n- Reduce cloud spending\n- Right-size resources\n- Implement cost governance\n- Optimize multi-cloud costs\n- Meet budget constraints\n\n## Cost Optimization Framework\n\n### 1. Visibility\n- Implement cost allocation tags\n- Use cloud cost management tools\n- Set up budget alerts\n- Create cost dashboards\n\n### 2. Right-Sizing\n- Analyze resource utilization\n- Downsize over-provisioned resources\n- Use auto-scaling\n- Remove idle resources\n\n### 3. Pricing Models\n- Use reserved capacity\n- Leverage spot/preemptible instances\n- Implement savings plans\n- Use committed use discounts\n\n### 4. Architecture Optimization\n- Use managed services\n- Implement caching\n- Optimize data transfer\n- Use lifecycle policies\n\n## AWS Cost Optimization\n\n### Reserved Instances\n```\nSavings: 30-72% vs On-Demand\nTerm: 1 or 3 years\nPayment: All/Partial/No upfront\nFlexibility: Standard or Convertible\n```\n\n### Savings Plans\n```\nCompute Savings Plans: 66% savings\nEC2 Instance Savings Plans: 72% savings\nApplies to: EC2, Fargate, Lambda\nFlexible across: Instance families, regions, OS\n```\n\n### Spot Instances\n```\nSavings: Up to 90% vs On-Demand\nBest for: Batch jobs, CI/CD, stateless workloads\nRisk: 2-minute interruption notice\nStrategy: Mix with On-Demand for resilience\n```\n\n### S3 Cost Optimization\n```hcl\nresource \"aws_s3_bucket_lifecycle_configuration\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n\n  rule {\n    id     = \"transition-to-ia\"\n    status = \"Enabled\"\n\n    transition {\n      days          = 30\n      storage_class = \"STANDARD_IA\"\n    }\n\n    transition {\n      days          = 90\n      storage_class = \"GLACIER\"\n    }\n\n    expiration {\n      days = 365\n    }\n  }\n}\n```\n\n## Azure Cost Optimization\n\n### Reserved VM Instances\n- 1 or 3 year terms\n- Up to 72% savings\n- Flexible sizing\n- Exchangeable\n\n### Azure Hybrid Benefit\n- Use existing Windows Server licenses\n- Up to 80% savings with RI\n- Available for Windows and SQL Server\n\n### Azure Advisor Recommendations\n- Right-size VMs\n- Delete unused resources\n- Use reserved capacity\n- Optimize storage\n\n## GCP Cost Optimization\n\n### Committed Use Discounts\n- 1 or 3 year commitment\n- Up to 57% savings\n- Applies to vCPUs and memory\n- Resource-based or spend-based\n\n### Sustained Use Discounts\n- Automatic discounts\n- Up to 30% for running instances\n- No commitment required\n- Applies to Compute Engine, GKE\n\n### Preemptible VMs\n- Up to 80% savings\n- 24-hour maximum runtime\n- Best for batch workloads\n\n## Tagging Strategy\n\n### AWS Tagging\n```hcl\nlocals {\n  common_tags = {\n    Environment = \"production\"\n    Project     = \"my-project\"\n    CostCenter  = \"engineering\"\n    Owner       = \"team@example.com\"\n    ManagedBy   = \"terraform\"\n  }\n}\n\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-12345678\"\n  instance_type = \"t3.medium\"\n\n  tags = merge(\n    local.common_tags,\n    {\n      Name = \"web-server\"\n    }\n  )\n}\n```\n\n**Reference:** See `references/tagging-standards.md`\n\n## Cost Monitoring\n\n### Budget Alerts\n```hcl\n# AWS Budget\nresource \"aws_budgets_budget\" \"monthly\" {\n  name              = \"monthly-budget\"\n  budget_type       = \"COST\"\n  limit_amount      = \"1000\"\n  limit_unit        = \"USD\"\n  time_period_start = \"2024-01-01_00:00\"\n  time_unit         = \"MONTHLY\"\n\n  notification {\n    comparison_operator        = \"GREATER_THAN\"\n    threshold                  = 80\n    threshold_type            = \"PERCENTAGE\"\n    notification_type         = \"ACTUAL\"\n    subscriber_email_addresses = [\"team@example.com\"]\n  }\n}\n```\n\n### Cost Anomaly Detection\n- AWS Cost Anomaly Detection\n- Azure Cost Management alerts\n- GCP Budget alerts\n\n## Architecture Patterns\n\n### Pattern 1: Serverless First\n- Use Lambda/Functions for event-driven\n- Pay only for execution time\n- Auto-scaling included\n- No idle costs\n\n### Pattern 2: Right-Sized Databases\n```\nDevelopment: t3.small RDS\nStaging: t3.large RDS\nProduction: r6g.2xlarge RDS with read replicas\n```\n\n### Pattern 3: Multi-Tier Storage\n```\nHot data: S3 Standard\nWarm data: S3 Standard-IA (30 days)\nCold data: S3 Glacier (90 days)\nArchive: S3 Deep Archive (365 days)\n```\n\n### Pattern 4: Auto-Scaling\n```hcl\nresource \"aws_autoscaling_policy\" \"scale_up\" {\n  name                   = \"scale-up\"\n  scaling_adjustment     = 2\n  adjustment_type        = \"ChangeInCapacity\"\n  cooldown              = 300\n  autoscaling_group_name = aws_autoscaling_group.main.name\n}\n\nresource \"aws_cloudwatch_metric_alarm\" \"cpu_high\" {\n  alarm_name          = \"cpu-high\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = \"2\"\n  metric_name         = \"CPUUtilization\"\n  namespace           = \"AWS/EC2\"\n  period              = \"60\"\n  statistic           = \"Average\"\n  threshold           = \"80\"\n  alarm_actions       = [aws_autoscaling_policy.scale_up.arn]\n}\n```\n\n## Cost Optimization Checklist\n\n- [ ] Implement cost allocation tags\n- [ ] Delete unused resources (EBS, EIPs, snapshots)\n- [ ] Right-size instances based on utilization\n- [ ] Use reserved capacity for steady workloads\n- [ ] Implement auto-scaling\n- [ ] Optimize storage classes\n- [ ] Use lifecycle policies\n- [ ] Enable cost anomaly detection\n- [ ] Set budget alerts\n- [ ] Review costs weekly\n- [ ] Use spot/preemptible instances\n- [ ] Optimize data transfer costs\n- [ ] Implement caching layers\n- [ ] Use managed services\n- [ ] Monitor and optimize continuously\n\n## Tools\n\n- **AWS:** Cost Explorer, Cost Anomaly Detection, Compute Optimizer\n- **Azure:** Cost Management, Advisor\n- **GCP:** Cost Management, Recommender\n- **Multi-cloud:** CloudHealth, Cloudability, Kubecost\n\n## Reference Files\n\n- `references/tagging-standards.md` - Tagging conventions\n- `assets/cost-analysis-template.xlsx` - Cost analysis spreadsheet\n\n## Related Skills\n\n- `terraform-module-library` - For resource provisioning\n- `multi-cloud-architecture` - For cloud selection\n",
      "references": {},
      "assets": {}
    },
    {
      "name": "hybrid-cloud-networking",
      "description": "Configure secure, high-performance connectivity between on-premises infrastructure and cloud platforms using VPN and dedicated connections. Use when building hybrid cloud architectures, connecting data centers to cloud, or implementing secure cross-premises networking.",
      "plugin": "cloud-infrastructure",
      "source_path": "plugins/cloud-infrastructure/skills/hybrid-cloud-networking/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "cloud",
        "aws",
        "azure",
        "gcp",
        "kubernetes",
        "terraform",
        "infrastructure"
      ],
      "content": "---\nname: hybrid-cloud-networking\ndescription: Configure secure, high-performance connectivity between on-premises infrastructure and cloud platforms using VPN and dedicated connections. Use when building hybrid cloud architectures, connecting data centers to cloud, or implementing secure cross-premises networking.\n---\n\n# Hybrid Cloud Networking\n\nConfigure secure, high-performance connectivity between on-premises and cloud environments using VPN, Direct Connect, and ExpressRoute.\n\n## Purpose\n\nEstablish secure, reliable network connectivity between on-premises data centers and cloud providers (AWS, Azure, GCP).\n\n## When to Use\n\n- Connect on-premises to cloud\n- Extend datacenter to cloud\n- Implement hybrid active-active setups\n- Meet compliance requirements\n- Migrate to cloud gradually\n\n## Connection Options\n\n### AWS Connectivity\n\n#### 1. Site-to-Site VPN\n- IPSec VPN over internet\n- Up to 1.25 Gbps per tunnel\n- Cost-effective for moderate bandwidth\n- Higher latency, internet-dependent\n\n```hcl\nresource \"aws_vpn_gateway\" \"main\" {\n  vpc_id = aws_vpc.main.id\n  tags = {\n    Name = \"main-vpn-gateway\"\n  }\n}\n\nresource \"aws_customer_gateway\" \"main\" {\n  bgp_asn    = 65000\n  ip_address = \"203.0.113.1\"\n  type       = \"ipsec.1\"\n}\n\nresource \"aws_vpn_connection\" \"main\" {\n  vpn_gateway_id      = aws_vpn_gateway.main.id\n  customer_gateway_id = aws_customer_gateway.main.id\n  type                = \"ipsec.1\"\n  static_routes_only  = false\n}\n```\n\n#### 2. AWS Direct Connect\n- Dedicated network connection\n- 1 Gbps to 100 Gbps\n- Lower latency, consistent bandwidth\n- More expensive, setup time required\n\n**Reference:** See `references/direct-connect.md`\n\n### Azure Connectivity\n\n#### 1. Site-to-Site VPN\n```hcl\nresource \"azurerm_virtual_network_gateway\" \"vpn\" {\n  name                = \"vpn-gateway\"\n  location            = azurerm_resource_group.main.location\n  resource_group_name = azurerm_resource_group.main.name\n\n  type     = \"Vpn\"\n  vpn_type = \"RouteBased\"\n  sku      = \"VpnGw1\"\n\n  ip_configuration {\n    name                          = \"vnetGatewayConfig\"\n    public_ip_address_id          = azurerm_public_ip.vpn.id\n    private_ip_address_allocation = \"Dynamic\"\n    subnet_id                     = azurerm_subnet.gateway.id\n  }\n}\n```\n\n#### 2. Azure ExpressRoute\n- Private connection via connectivity provider\n- Up to 100 Gbps\n- Low latency, high reliability\n- Premium for global connectivity\n\n### GCP Connectivity\n\n#### 1. Cloud VPN\n- IPSec VPN (Classic or HA VPN)\n- HA VPN: 99.99% SLA\n- Up to 3 Gbps per tunnel\n\n#### 2. Cloud Interconnect\n- Dedicated (10 Gbps, 100 Gbps)\n- Partner (50 Mbps to 50 Gbps)\n- Lower latency than VPN\n\n## Hybrid Network Patterns\n\n### Pattern 1: Hub-and-Spoke\n```\nOn-Premises Datacenter\n         \u2193\n    VPN/Direct Connect\n         \u2193\n    Transit Gateway (AWS) / vWAN (Azure)\n         \u2193\n    \u251c\u2500 Production VPC/VNet\n    \u251c\u2500 Staging VPC/VNet\n    \u2514\u2500 Development VPC/VNet\n```\n\n### Pattern 2: Multi-Region Hybrid\n```\nOn-Premises\n    \u251c\u2500 Direct Connect \u2192 us-east-1\n    \u2514\u2500 Direct Connect \u2192 us-west-2\n            \u2193\n        Cross-Region Peering\n```\n\n### Pattern 3: Multi-Cloud Hybrid\n```\nOn-Premises Datacenter\n    \u251c\u2500 Direct Connect \u2192 AWS\n    \u251c\u2500 ExpressRoute \u2192 Azure\n    \u2514\u2500 Interconnect \u2192 GCP\n```\n\n## Routing Configuration\n\n### BGP Configuration\n```\nOn-Premises Router:\n- AS Number: 65000\n- Advertise: 10.0.0.0/8\n\nCloud Router:\n- AS Number: 64512 (AWS), 65515 (Azure)\n- Advertise: Cloud VPC/VNet CIDRs\n```\n\n### Route Propagation\n- Enable route propagation on route tables\n- Use BGP for dynamic routing\n- Implement route filtering\n- Monitor route advertisements\n\n## Security Best Practices\n\n1. **Use private connectivity** (Direct Connect/ExpressRoute)\n2. **Implement encryption** for VPN tunnels\n3. **Use VPC endpoints** to avoid internet routing\n4. **Configure network ACLs** and security groups\n5. **Enable VPC Flow Logs** for monitoring\n6. **Implement DDoS protection**\n7. **Use PrivateLink/Private Endpoints**\n8. **Monitor connections** with CloudWatch/Monitor\n9. **Implement redundancy** (dual tunnels)\n10. **Regular security audits**\n\n## High Availability\n\n### Dual VPN Tunnels\n```hcl\nresource \"aws_vpn_connection\" \"primary\" {\n  vpn_gateway_id      = aws_vpn_gateway.main.id\n  customer_gateway_id = aws_customer_gateway.primary.id\n  type                = \"ipsec.1\"\n}\n\nresource \"aws_vpn_connection\" \"secondary\" {\n  vpn_gateway_id      = aws_vpn_gateway.main.id\n  customer_gateway_id = aws_customer_gateway.secondary.id\n  type                = \"ipsec.1\"\n}\n```\n\n### Active-Active Configuration\n- Multiple connections from different locations\n- BGP for automatic failover\n- Equal-cost multi-path (ECMP) routing\n- Monitor health of all connections\n\n## Monitoring and Troubleshooting\n\n### Key Metrics\n- Tunnel status (up/down)\n- Bytes in/out\n- Packet loss\n- Latency\n- BGP session status\n\n### Troubleshooting\n```bash\n# AWS VPN\naws ec2 describe-vpn-connections\naws ec2 get-vpn-connection-telemetry\n\n# Azure VPN\naz network vpn-connection show\naz network vpn-connection show-device-config-script\n```\n\n## Cost Optimization\n\n1. **Right-size connections** based on traffic\n2. **Use VPN for low-bandwidth** workloads\n3. **Consolidate traffic** through fewer connections\n4. **Minimize data transfer** costs\n5. **Use Direct Connect** for high bandwidth\n6. **Implement caching** to reduce traffic\n\n## Reference Files\n\n- `references/vpn-setup.md` - VPN configuration guide\n- `references/direct-connect.md` - Direct Connect setup\n\n## Related Skills\n\n- `multi-cloud-architecture` - For architecture decisions\n- `terraform-module-library` - For IaC implementation\n",
      "references": {},
      "assets": {}
    },
    {
      "name": "multi-cloud-architecture",
      "description": "Design multi-cloud architectures using a decision framework to select and integrate services across AWS, Azure, and GCP. Use when building multi-cloud systems, avoiding vendor lock-in, or leveraging best-of-breed services from multiple providers.",
      "plugin": "cloud-infrastructure",
      "source_path": "plugins/cloud-infrastructure/skills/multi-cloud-architecture/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "cloud",
        "aws",
        "azure",
        "gcp",
        "kubernetes",
        "terraform",
        "infrastructure"
      ],
      "content": "---\nname: multi-cloud-architecture\ndescription: Design multi-cloud architectures using a decision framework to select and integrate services across AWS, Azure, and GCP. Use when building multi-cloud systems, avoiding vendor lock-in, or leveraging best-of-breed services from multiple providers.\n---\n\n# Multi-Cloud Architecture\n\nDecision framework and patterns for architecting applications across AWS, Azure, and GCP.\n\n## Purpose\n\nDesign cloud-agnostic architectures and make informed decisions about service selection across cloud providers.\n\n## When to Use\n\n- Design multi-cloud strategies\n- Migrate between cloud providers\n- Select cloud services for specific workloads\n- Implement cloud-agnostic architectures\n- Optimize costs across providers\n\n## Cloud Service Comparison\n\n### Compute Services\n\n| AWS | Azure | GCP | Use Case |\n|-----|-------|-----|----------|\n| EC2 | Virtual Machines | Compute Engine | IaaS VMs |\n| ECS | Container Instances | Cloud Run | Containers |\n| EKS | AKS | GKE | Kubernetes |\n| Lambda | Functions | Cloud Functions | Serverless |\n| Fargate | Container Apps | Cloud Run | Managed containers |\n\n### Storage Services\n\n| AWS | Azure | GCP | Use Case |\n|-----|-------|-----|----------|\n| S3 | Blob Storage | Cloud Storage | Object storage |\n| EBS | Managed Disks | Persistent Disk | Block storage |\n| EFS | Azure Files | Filestore | File storage |\n| Glacier | Archive Storage | Archive Storage | Cold storage |\n\n### Database Services\n\n| AWS | Azure | GCP | Use Case |\n|-----|-------|-----|----------|\n| RDS | SQL Database | Cloud SQL | Managed SQL |\n| DynamoDB | Cosmos DB | Firestore | NoSQL |\n| Aurora | PostgreSQL/MySQL | Cloud Spanner | Distributed SQL |\n| ElastiCache | Cache for Redis | Memorystore | Caching |\n\n**Reference:** See `references/service-comparison.md` for complete comparison\n\n## Multi-Cloud Patterns\n\n### Pattern 1: Single Provider with DR\n\n- Primary workload in one cloud\n- Disaster recovery in another\n- Database replication across clouds\n- Automated failover\n\n### Pattern 2: Best-of-Breed\n\n- Use best service from each provider\n- AI/ML on GCP\n- Enterprise apps on Azure\n- General compute on AWS\n\n### Pattern 3: Geographic Distribution\n\n- Serve users from nearest cloud region\n- Data sovereignty compliance\n- Global load balancing\n- Regional failover\n\n### Pattern 4: Cloud-Agnostic Abstraction\n\n- Kubernetes for compute\n- PostgreSQL for database\n- S3-compatible storage (MinIO)\n- Open source tools\n\n## Cloud-Agnostic Architecture\n\n### Use Cloud-Native Alternatives\n\n- **Compute:** Kubernetes (EKS/AKS/GKE)\n- **Database:** PostgreSQL/MySQL (RDS/SQL Database/Cloud SQL)\n- **Message Queue:** Apache Kafka (MSK/Event Hubs/Confluent)\n- **Cache:** Redis (ElastiCache/Azure Cache/Memorystore)\n- **Object Storage:** S3-compatible API\n- **Monitoring:** Prometheus/Grafana\n- **Service Mesh:** Istio/Linkerd\n\n### Abstraction Layers\n\n```\nApplication Layer\n    \u2193\nInfrastructure Abstraction (Terraform)\n    \u2193\nCloud Provider APIs\n    \u2193\nAWS / Azure / GCP\n```\n\n## Cost Comparison\n\n### Compute Pricing Factors\n\n- **AWS:** On-demand, Reserved, Spot, Savings Plans\n- **Azure:** Pay-as-you-go, Reserved, Spot\n- **GCP:** On-demand, Committed use, Preemptible\n\n### Cost Optimization Strategies\n\n1. Use reserved/committed capacity (30-70% savings)\n2. Leverage spot/preemptible instances\n3. Right-size resources\n4. Use serverless for variable workloads\n5. Optimize data transfer costs\n6. Implement lifecycle policies\n7. Use cost allocation tags\n8. Monitor with cloud cost tools\n\n**Reference:** See `references/multi-cloud-patterns.md`\n\n## Migration Strategy\n\n### Phase 1: Assessment\n- Inventory current infrastructure\n- Identify dependencies\n- Assess cloud compatibility\n- Estimate costs\n\n### Phase 2: Pilot\n- Select pilot workload\n- Implement in target cloud\n- Test thoroughly\n- Document learnings\n\n### Phase 3: Migration\n- Migrate workloads incrementally\n- Maintain dual-run period\n- Monitor performance\n- Validate functionality\n\n### Phase 4: Optimization\n- Right-size resources\n- Implement cloud-native services\n- Optimize costs\n- Enhance security\n\n## Best Practices\n\n1. **Use infrastructure as code** (Terraform/OpenTofu)\n2. **Implement CI/CD pipelines** for deployments\n3. **Design for failure** across clouds\n4. **Use managed services** when possible\n5. **Implement comprehensive monitoring**\n6. **Automate cost optimization**\n7. **Follow security best practices**\n8. **Document cloud-specific configurations**\n9. **Test disaster recovery** procedures\n10. **Train teams** on multiple clouds\n\n## Reference Files\n\n- `references/service-comparison.md` - Complete service comparison\n- `references/multi-cloud-patterns.md` - Architecture patterns\n\n## Related Skills\n\n- `terraform-module-library` - For IaC implementation\n- `cost-optimization` - For cost management\n- `hybrid-cloud-networking` - For connectivity\n",
      "references": {},
      "assets": {}
    },
    {
      "name": "terraform-module-library",
      "description": "Build reusable Terraform modules for AWS, Azure, and GCP infrastructure following infrastructure-as-code best practices. Use when creating infrastructure modules, standardizing cloud provisioning, or implementing reusable IaC components.",
      "plugin": "cloud-infrastructure",
      "source_path": "plugins/cloud-infrastructure/skills/terraform-module-library/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "cloud",
        "aws",
        "azure",
        "gcp",
        "kubernetes",
        "terraform",
        "infrastructure"
      ],
      "content": "---\nname: terraform-module-library\ndescription: Build reusable Terraform modules for AWS, Azure, and GCP infrastructure following infrastructure-as-code best practices. Use when creating infrastructure modules, standardizing cloud provisioning, or implementing reusable IaC components.\n---\n\n# Terraform Module Library\n\nProduction-ready Terraform module patterns for AWS, Azure, and GCP infrastructure.\n\n## Purpose\n\nCreate reusable, well-tested Terraform modules for common cloud infrastructure patterns across multiple cloud providers.\n\n## When to Use\n\n- Build reusable infrastructure components\n- Standardize cloud resource provisioning\n- Implement infrastructure as code best practices\n- Create multi-cloud compatible modules\n- Establish organizational Terraform standards\n\n## Module Structure\n\n```\nterraform-modules/\n\u251c\u2500\u2500 aws/\n\u2502   \u251c\u2500\u2500 vpc/\n\u2502   \u251c\u2500\u2500 eks/\n\u2502   \u251c\u2500\u2500 rds/\n\u2502   \u2514\u2500\u2500 s3/\n\u251c\u2500\u2500 azure/\n\u2502   \u251c\u2500\u2500 vnet/\n\u2502   \u251c\u2500\u2500 aks/\n\u2502   \u2514\u2500\u2500 storage/\n\u2514\u2500\u2500 gcp/\n    \u251c\u2500\u2500 vpc/\n    \u251c\u2500\u2500 gke/\n    \u2514\u2500\u2500 cloud-sql/\n```\n\n## Standard Module Pattern\n\n```\nmodule-name/\n\u251c\u2500\u2500 main.tf          # Main resources\n\u251c\u2500\u2500 variables.tf     # Input variables\n\u251c\u2500\u2500 outputs.tf       # Output values\n\u251c\u2500\u2500 versions.tf      # Provider versions\n\u251c\u2500\u2500 README.md        # Documentation\n\u251c\u2500\u2500 examples/        # Usage examples\n\u2502   \u2514\u2500\u2500 complete/\n\u2502       \u251c\u2500\u2500 main.tf\n\u2502       \u2514\u2500\u2500 variables.tf\n\u2514\u2500\u2500 tests/           # Terratest files\n    \u2514\u2500\u2500 module_test.go\n```\n\n## AWS VPC Module Example\n\n**main.tf:**\n```hcl\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = var.cidr_block\n  enable_dns_hostnames = var.enable_dns_hostnames\n  enable_dns_support   = var.enable_dns_support\n\n  tags = merge(\n    {\n      Name = var.name\n    },\n    var.tags\n  )\n}\n\nresource \"aws_subnet\" \"private\" {\n  count             = length(var.private_subnet_cidrs)\n  vpc_id            = aws_vpc.main.id\n  cidr_block        = var.private_subnet_cidrs[count.index]\n  availability_zone = var.availability_zones[count.index]\n\n  tags = merge(\n    {\n      Name = \"${var.name}-private-${count.index + 1}\"\n      Tier = \"private\"\n    },\n    var.tags\n  )\n}\n\nresource \"aws_internet_gateway\" \"main\" {\n  count  = var.create_internet_gateway ? 1 : 0\n  vpc_id = aws_vpc.main.id\n\n  tags = merge(\n    {\n      Name = \"${var.name}-igw\"\n    },\n    var.tags\n  )\n}\n```\n\n**variables.tf:**\n```hcl\nvariable \"name\" {\n  description = \"Name of the VPC\"\n  type        = string\n}\n\nvariable \"cidr_block\" {\n  description = \"CIDR block for VPC\"\n  type        = string\n  validation {\n    condition     = can(regex(\"^([0-9]{1,3}\\\\.){3}[0-9]{1,3}/[0-9]{1,2}$\", var.cidr_block))\n    error_message = \"CIDR block must be valid IPv4 CIDR notation.\"\n  }\n}\n\nvariable \"availability_zones\" {\n  description = \"List of availability zones\"\n  type        = list(string)\n}\n\nvariable \"private_subnet_cidrs\" {\n  description = \"CIDR blocks for private subnets\"\n  type        = list(string)\n  default     = []\n}\n\nvariable \"enable_dns_hostnames\" {\n  description = \"Enable DNS hostnames in VPC\"\n  type        = bool\n  default     = true\n}\n\nvariable \"tags\" {\n  description = \"Additional tags\"\n  type        = map(string)\n  default     = {}\n}\n```\n\n**outputs.tf:**\n```hcl\noutput \"vpc_id\" {\n  description = \"ID of the VPC\"\n  value       = aws_vpc.main.id\n}\n\noutput \"private_subnet_ids\" {\n  description = \"IDs of private subnets\"\n  value       = aws_subnet.private[*].id\n}\n\noutput \"vpc_cidr_block\" {\n  description = \"CIDR block of VPC\"\n  value       = aws_vpc.main.cidr_block\n}\n```\n\n## Best Practices\n\n1. **Use semantic versioning** for modules\n2. **Document all variables** with descriptions\n3. **Provide examples** in examples/ directory\n4. **Use validation blocks** for input validation\n5. **Output important attributes** for module composition\n6. **Pin provider versions** in versions.tf\n7. **Use locals** for computed values\n8. **Implement conditional resources** with count/for_each\n9. **Test modules** with Terratest\n10. **Tag all resources** consistently\n\n## Module Composition\n\n```hcl\nmodule \"vpc\" {\n  source = \"../../modules/aws/vpc\"\n\n  name               = \"production\"\n  cidr_block         = \"10.0.0.0/16\"\n  availability_zones = [\"us-west-2a\", \"us-west-2b\", \"us-west-2c\"]\n\n  private_subnet_cidrs = [\n    \"10.0.1.0/24\",\n    \"10.0.2.0/24\",\n    \"10.0.3.0/24\"\n  ]\n\n  tags = {\n    Environment = \"production\"\n    ManagedBy   = \"terraform\"\n  }\n}\n\nmodule \"rds\" {\n  source = \"../../modules/aws/rds\"\n\n  identifier     = \"production-db\"\n  engine         = \"postgres\"\n  engine_version = \"15.3\"\n  instance_class = \"db.t3.large\"\n\n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnet_ids\n\n  tags = {\n    Environment = \"production\"\n  }\n}\n```\n\n## Reference Files\n\n- `assets/vpc-module/` - Complete VPC module example\n- `assets/rds-module/` - RDS module example\n- `references/aws-modules.md` - AWS module patterns\n- `references/azure-modules.md` - Azure module patterns\n- `references/gcp-modules.md` - GCP module patterns\n\n## Testing\n\n```go\n// tests/vpc_test.go\npackage test\n\nimport (\n    \"testing\"\n    \"github.com/gruntwork-io/terratest/modules/terraform\"\n    \"github.com/stretchr/testify/assert\"\n)\n\nfunc TestVPCModule(t *testing.T) {\n    terraformOptions := &terraform.Options{\n        TerraformDir: \"../examples/complete\",\n    }\n\n    defer terraform.Destroy(t, terraformOptions)\n    terraform.InitAndApply(t, terraformOptions)\n\n    vpcID := terraform.Output(t, terraformOptions, \"vpc_id\")\n    assert.NotEmpty(t, vpcID)\n}\n```\n\n## Related Skills\n\n- `multi-cloud-architecture` - For architectural decisions\n- `cost-optimization` - For cost-effective designs\n",
      "references": {
        "aws-modules.md": "# AWS Terraform Module Patterns\n\n## VPC Module\n- VPC with public/private subnets\n- Internet Gateway and NAT Gateways\n- Route tables and associations\n- Network ACLs\n- VPC Flow Logs\n\n## EKS Module\n- EKS cluster with managed node groups\n- IRSA (IAM Roles for Service Accounts)\n- Cluster autoscaler\n- VPC CNI configuration\n- Cluster logging\n\n## RDS Module\n- RDS instance or cluster\n- Automated backups\n- Read replicas\n- Parameter groups\n- Subnet groups\n- Security groups\n\n## S3 Module\n- S3 bucket with versioning\n- Encryption at rest\n- Bucket policies\n- Lifecycle rules\n- Replication configuration\n\n## ALB Module\n- Application Load Balancer\n- Target groups\n- Listener rules\n- SSL/TLS certificates\n- Access logs\n\n## Lambda Module\n- Lambda function\n- IAM execution role\n- CloudWatch Logs\n- Environment variables\n- VPC configuration (optional)\n\n## Security Group Module\n- Reusable security group rules\n- Ingress/egress rules\n- Dynamic rule creation\n- Rule descriptions\n\n## Best Practices\n\n1. Use AWS provider version ~> 5.0\n2. Enable encryption by default\n3. Use least-privilege IAM\n4. Tag all resources consistently\n5. Enable logging and monitoring\n6. Use KMS for encryption\n7. Implement backup strategies\n8. Use PrivateLink when possible\n9. Enable GuardDuty/SecurityHub\n10. Follow AWS Well-Architected Framework\n"
      },
      "assets": {}
    },
    {
      "name": "deployment-pipeline-design",
      "description": "Design multi-stage CI/CD pipelines with approval gates, security checks, and deployment orchestration. Use when architecting deployment workflows, setting up continuous delivery, or implementing GitOps practices.",
      "plugin": "cicd-automation",
      "source_path": "plugins/cicd-automation/skills/deployment-pipeline-design/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "ci-cd",
        "automation",
        "pipeline",
        "github-actions",
        "gitlab-ci"
      ],
      "content": "---\nname: deployment-pipeline-design\ndescription: Design multi-stage CI/CD pipelines with approval gates, security checks, and deployment orchestration. Use when architecting deployment workflows, setting up continuous delivery, or implementing GitOps practices.\n---\n\n# Deployment Pipeline Design\n\nArchitecture patterns for multi-stage CI/CD pipelines with approval gates and deployment strategies.\n\n## Purpose\n\nDesign robust, secure deployment pipelines that balance speed with safety through proper stage organization and approval workflows.\n\n## When to Use\n\n- Design CI/CD architecture\n- Implement deployment gates\n- Configure multi-environment pipelines\n- Establish deployment best practices\n- Implement progressive delivery\n\n## Pipeline Stages\n\n### Standard Pipeline Flow\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Build  \u2502 \u2192 \u2502 Test \u2502 \u2192 \u2502 Staging \u2502 \u2192 \u2502 Approve\u2502 \u2192 \u2502Production\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Detailed Stage Breakdown\n\n1. **Source** - Code checkout\n2. **Build** - Compile, package, containerize\n3. **Test** - Unit, integration, security scans\n4. **Staging Deploy** - Deploy to staging environment\n5. **Integration Tests** - E2E, smoke tests\n6. **Approval Gate** - Manual approval required\n7. **Production Deploy** - Canary, blue-green, rolling\n8. **Verification** - Health checks, monitoring\n9. **Rollback** - Automated rollback on failure\n\n## Approval Gate Patterns\n\n### Pattern 1: Manual Approval\n\n```yaml\n# GitHub Actions\nproduction-deploy:\n  needs: staging-deploy\n  environment:\n    name: production\n    url: https://app.example.com\n  runs-on: ubuntu-latest\n  steps:\n    - name: Deploy to production\n      run: |\n        # Deployment commands\n```\n\n### Pattern 2: Time-Based Approval\n\n```yaml\n# GitLab CI\ndeploy:production:\n  stage: deploy\n  script:\n    - deploy.sh production\n  environment:\n    name: production\n  when: delayed\n  start_in: 30 minutes\n  only:\n    - main\n```\n\n### Pattern 3: Multi-Approver\n\n```yaml\n# Azure Pipelines\nstages:\n- stage: Production\n  dependsOn: Staging\n  jobs:\n  - deployment: Deploy\n    environment:\n      name: production\n      resourceType: Kubernetes\n    strategy:\n      runOnce:\n        preDeploy:\n          steps:\n          - task: ManualValidation@0\n            inputs:\n              notifyUsers: 'team-leads@example.com'\n              instructions: 'Review staging metrics before approving'\n```\n\n**Reference:** See `assets/approval-gate-template.yml`\n\n## Deployment Strategies\n\n### 1. Rolling Deployment\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 10\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n```\n\n**Characteristics:**\n- Gradual rollout\n- Zero downtime\n- Easy rollback\n- Best for most applications\n\n### 2. Blue-Green Deployment\n\n```yaml\n# Blue (current)\nkubectl apply -f blue-deployment.yaml\nkubectl label service my-app version=blue\n\n# Green (new)\nkubectl apply -f green-deployment.yaml\n# Test green environment\nkubectl label service my-app version=green\n\n# Rollback if needed\nkubectl label service my-app version=blue\n```\n\n**Characteristics:**\n- Instant switchover\n- Easy rollback\n- Doubles infrastructure cost temporarily\n- Good for high-risk deployments\n\n### 3. Canary Deployment\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: my-app\nspec:\n  replicas: 10\n  strategy:\n    canary:\n      steps:\n      - setWeight: 10\n      - pause: {duration: 5m}\n      - setWeight: 25\n      - pause: {duration: 5m}\n      - setWeight: 50\n      - pause: {duration: 5m}\n      - setWeight: 100\n```\n\n**Characteristics:**\n- Gradual traffic shift\n- Risk mitigation\n- Real user testing\n- Requires service mesh or similar\n\n### 4. Feature Flags\n\n```python\nfrom flagsmith import Flagsmith\n\nflagsmith = Flagsmith(environment_key=\"API_KEY\")\n\nif flagsmith.has_feature(\"new_checkout_flow\"):\n    # New code path\n    process_checkout_v2()\nelse:\n    # Existing code path\n    process_checkout_v1()\n```\n\n**Characteristics:**\n- Deploy without releasing\n- A/B testing\n- Instant rollback\n- Granular control\n\n## Pipeline Orchestration\n\n### Multi-Stage Pipeline Example\n\n```yaml\nname: Production Pipeline\n\non:\n  push:\n    branches: [ main ]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Build application\n        run: make build\n      - name: Build Docker image\n        run: docker build -t myapp:${{ github.sha }} .\n      - name: Push to registry\n        run: docker push myapp:${{ github.sha }}\n\n  test:\n    needs: build\n    runs-on: ubuntu-latest\n    steps:\n      - name: Unit tests\n        run: make test\n      - name: Security scan\n        run: trivy image myapp:${{ github.sha }}\n\n  deploy-staging:\n    needs: test\n    runs-on: ubuntu-latest\n    environment:\n      name: staging\n    steps:\n      - name: Deploy to staging\n        run: kubectl apply -f k8s/staging/\n\n  integration-test:\n    needs: deploy-staging\n    runs-on: ubuntu-latest\n    steps:\n      - name: Run E2E tests\n        run: npm run test:e2e\n\n  deploy-production:\n    needs: integration-test\n    runs-on: ubuntu-latest\n    environment:\n      name: production\n    steps:\n      - name: Canary deployment\n        run: |\n          kubectl apply -f k8s/production/\n          kubectl argo rollouts promote my-app\n\n  verify:\n    needs: deploy-production\n    runs-on: ubuntu-latest\n    steps:\n      - name: Health check\n        run: curl -f https://app.example.com/health\n      - name: Notify team\n        run: |\n          curl -X POST ${{ secrets.SLACK_WEBHOOK }} \\\n            -d '{\"text\":\"Production deployment successful!\"}'\n```\n\n## Pipeline Best Practices\n\n1. **Fail fast** - Run quick tests first\n2. **Parallel execution** - Run independent jobs concurrently\n3. **Caching** - Cache dependencies between runs\n4. **Artifact management** - Store build artifacts\n5. **Environment parity** - Keep environments consistent\n6. **Secrets management** - Use secret stores (Vault, etc.)\n7. **Deployment windows** - Schedule deployments appropriately\n8. **Monitoring integration** - Track deployment metrics\n9. **Rollback automation** - Auto-rollback on failures\n10. **Documentation** - Document pipeline stages\n\n## Rollback Strategies\n\n### Automated Rollback\n\n```yaml\ndeploy-and-verify:\n  steps:\n    - name: Deploy new version\n      run: kubectl apply -f k8s/\n\n    - name: Wait for rollout\n      run: kubectl rollout status deployment/my-app\n\n    - name: Health check\n      id: health\n      run: |\n        for i in {1..10}; do\n          if curl -sf https://app.example.com/health; then\n            exit 0\n          fi\n          sleep 10\n        done\n        exit 1\n\n    - name: Rollback on failure\n      if: failure()\n      run: kubectl rollout undo deployment/my-app\n```\n\n### Manual Rollback\n\n```bash\n# List revision history\nkubectl rollout history deployment/my-app\n\n# Rollback to previous version\nkubectl rollout undo deployment/my-app\n\n# Rollback to specific revision\nkubectl rollout undo deployment/my-app --to-revision=3\n```\n\n## Monitoring and Metrics\n\n### Key Pipeline Metrics\n\n- **Deployment Frequency** - How often deployments occur\n- **Lead Time** - Time from commit to production\n- **Change Failure Rate** - Percentage of failed deployments\n- **Mean Time to Recovery (MTTR)** - Time to recover from failure\n- **Pipeline Success Rate** - Percentage of successful runs\n- **Average Pipeline Duration** - Time to complete pipeline\n\n### Integration with Monitoring\n\n```yaml\n- name: Post-deployment verification\n  run: |\n    # Wait for metrics stabilization\n    sleep 60\n\n    # Check error rate\n    ERROR_RATE=$(curl -s \"$PROMETHEUS_URL/api/v1/query?query=rate(http_errors_total[5m])\" | jq '.data.result[0].value[1]')\n\n    if (( $(echo \"$ERROR_RATE > 0.01\" | bc -l) )); then\n      echo \"Error rate too high: $ERROR_RATE\"\n      exit 1\n    fi\n```\n\n## Reference Files\n\n- `references/pipeline-orchestration.md` - Complex pipeline patterns\n- `assets/approval-gate-template.yml` - Approval workflow templates\n\n## Related Skills\n\n- `github-actions-templates` - For GitHub Actions implementation\n- `gitlab-ci-patterns` - For GitLab CI implementation\n- `secrets-management` - For secrets handling\n",
      "references": {},
      "assets": {}
    },
    {
      "name": "github-actions-templates",
      "description": "Create production-ready GitHub Actions workflows for automated testing, building, and deploying applications. Use when setting up CI/CD with GitHub Actions, automating development workflows, or creating reusable workflow templates.",
      "plugin": "cicd-automation",
      "source_path": "plugins/cicd-automation/skills/github-actions-templates/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "ci-cd",
        "automation",
        "pipeline",
        "github-actions",
        "gitlab-ci"
      ],
      "content": "---\nname: github-actions-templates\ndescription: Create production-ready GitHub Actions workflows for automated testing, building, and deploying applications. Use when setting up CI/CD with GitHub Actions, automating development workflows, or creating reusable workflow templates.\n---\n\n# GitHub Actions Templates\n\nProduction-ready GitHub Actions workflow patterns for testing, building, and deploying applications.\n\n## Purpose\n\nCreate efficient, secure GitHub Actions workflows for continuous integration and deployment across various tech stacks.\n\n## When to Use\n\n- Automate testing and deployment\n- Build Docker images and push to registries\n- Deploy to Kubernetes clusters\n- Run security scans\n- Implement matrix builds for multiple environments\n\n## Common Workflow Patterns\n\n### Pattern 1: Test Workflow\n\n```yaml\nname: Test\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        node-version: [18.x, 20.x]\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Use Node.js ${{ matrix.node-version }}\n      uses: actions/setup-node@v4\n      with:\n        node-version: ${{ matrix.node-version }}\n        cache: 'npm'\n\n    - name: Install dependencies\n      run: npm ci\n\n    - name: Run linter\n      run: npm run lint\n\n    - name: Run tests\n      run: npm test\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n      with:\n        files: ./coverage/lcov.info\n```\n\n**Reference:** See `assets/test-workflow.yml`\n\n### Pattern 2: Build and Push Docker Image\n\n```yaml\nname: Build and Push\n\non:\n  push:\n    branches: [ main ]\n    tags: [ 'v*' ]\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Log in to Container Registry\n      uses: docker/login-action@v3\n      with:\n        registry: ${{ env.REGISTRY }}\n        username: ${{ github.actor }}\n        password: ${{ secrets.GITHUB_TOKEN }}\n\n    - name: Extract metadata\n      id: meta\n      uses: docker/metadata-action@v5\n      with:\n        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\n        tags: |\n          type=ref,event=branch\n          type=ref,event=pr\n          type=semver,pattern={{version}}\n          type=semver,pattern={{major}}.{{minor}}\n\n    - name: Build and push\n      uses: docker/build-push-action@v5\n      with:\n        context: .\n        push: true\n        tags: ${{ steps.meta.outputs.tags }}\n        labels: ${{ steps.meta.outputs.labels }}\n        cache-from: type=gha\n        cache-to: type=gha,mode=max\n```\n\n**Reference:** See `assets/deploy-workflow.yml`\n\n### Pattern 3: Deploy to Kubernetes\n\n```yaml\nname: Deploy to Kubernetes\n\non:\n  push:\n    branches: [ main ]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Configure AWS credentials\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n        aws-region: us-west-2\n\n    - name: Update kubeconfig\n      run: |\n        aws eks update-kubeconfig --name production-cluster --region us-west-2\n\n    - name: Deploy to Kubernetes\n      run: |\n        kubectl apply -f k8s/\n        kubectl rollout status deployment/my-app -n production\n        kubectl get services -n production\n\n    - name: Verify deployment\n      run: |\n        kubectl get pods -n production\n        kubectl describe deployment my-app -n production\n```\n\n### Pattern 4: Matrix Build\n\n```yaml\nname: Matrix Build\n\non: [push, pull_request]\n\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n\n    strategy:\n      matrix:\n        os: [ubuntu-latest, macos-latest, windows-latest]\n        python-version: ['3.9', '3.10', '3.11', '3.12']\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Run tests\n      run: pytest\n```\n\n**Reference:** See `assets/matrix-build.yml`\n\n## Workflow Best Practices\n\n1. **Use specific action versions** (@v4, not @latest)\n2. **Cache dependencies** to speed up builds\n3. **Use secrets** for sensitive data\n4. **Implement status checks** on PRs\n5. **Use matrix builds** for multi-version testing\n6. **Set appropriate permissions**\n7. **Use reusable workflows** for common patterns\n8. **Implement approval gates** for production\n9. **Add notification steps** for failures\n10. **Use self-hosted runners** for sensitive workloads\n\n## Reusable Workflows\n\n```yaml\n# .github/workflows/reusable-test.yml\nname: Reusable Test Workflow\n\non:\n  workflow_call:\n    inputs:\n      node-version:\n        required: true\n        type: string\n    secrets:\n      NPM_TOKEN:\n        required: true\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n    - uses: actions/setup-node@v4\n      with:\n        node-version: ${{ inputs.node-version }}\n    - run: npm ci\n    - run: npm test\n```\n\n**Use reusable workflow:**\n```yaml\njobs:\n  call-test:\n    uses: ./.github/workflows/reusable-test.yml\n    with:\n      node-version: '20.x'\n    secrets:\n      NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n```\n\n## Security Scanning\n\n```yaml\nname: Security Scan\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  security:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Run Trivy vulnerability scanner\n      uses: aquasecurity/trivy-action@master\n      with:\n        scan-type: 'fs'\n        scan-ref: '.'\n        format: 'sarif'\n        output: 'trivy-results.sarif'\n\n    - name: Upload Trivy results to GitHub Security\n      uses: github/codeql-action/upload-sarif@v2\n      with:\n        sarif_file: 'trivy-results.sarif'\n\n    - name: Run Snyk Security Scan\n      uses: snyk/actions/node@master\n      env:\n        SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n```\n\n## Deployment with Approvals\n\n```yaml\nname: Deploy to Production\n\non:\n  push:\n    tags: [ 'v*' ]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment:\n      name: production\n      url: https://app.example.com\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Deploy application\n      run: |\n        echo \"Deploying to production...\"\n        # Deployment commands here\n\n    - name: Notify Slack\n      if: success()\n      uses: slackapi/slack-github-action@v1\n      with:\n        webhook-url: ${{ secrets.SLACK_WEBHOOK }}\n        payload: |\n          {\n            \"text\": \"Deployment to production completed successfully!\"\n          }\n```\n\n## Reference Files\n\n- `assets/test-workflow.yml` - Testing workflow template\n- `assets/deploy-workflow.yml` - Deployment workflow template\n- `assets/matrix-build.yml` - Matrix build template\n- `references/common-workflows.md` - Common workflow patterns\n\n## Related Skills\n\n- `gitlab-ci-patterns` - For GitLab CI workflows\n- `deployment-pipeline-design` - For pipeline architecture\n- `secrets-management` - For secrets handling\n",
      "references": {},
      "assets": {}
    },
    {
      "name": "gitlab-ci-patterns",
      "description": "Build GitLab CI/CD pipelines with multi-stage workflows, caching, and distributed runners for scalable automation. Use when implementing GitLab CI/CD, optimizing pipeline performance, or setting up automated testing and deployment.",
      "plugin": "cicd-automation",
      "source_path": "plugins/cicd-automation/skills/gitlab-ci-patterns/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "ci-cd",
        "automation",
        "pipeline",
        "github-actions",
        "gitlab-ci"
      ],
      "content": "---\nname: gitlab-ci-patterns\ndescription: Build GitLab CI/CD pipelines with multi-stage workflows, caching, and distributed runners for scalable automation. Use when implementing GitLab CI/CD, optimizing pipeline performance, or setting up automated testing and deployment.\n---\n\n# GitLab CI Patterns\n\nComprehensive GitLab CI/CD pipeline patterns for automated testing, building, and deployment.\n\n## Purpose\n\nCreate efficient GitLab CI pipelines with proper stage organization, caching, and deployment strategies.\n\n## When to Use\n\n- Automate GitLab-based CI/CD\n- Implement multi-stage pipelines\n- Configure GitLab Runners\n- Deploy to Kubernetes from GitLab\n- Implement GitOps workflows\n\n## Basic Pipeline Structure\n\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n\nvariables:\n  DOCKER_DRIVER: overlay2\n  DOCKER_TLS_CERTDIR: \"/certs\"\n\nbuild:\n  stage: build\n  image: node:20\n  script:\n    - npm ci\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n    expire_in: 1 hour\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n\ntest:\n  stage: test\n  image: node:20\n  script:\n    - npm ci\n    - npm run lint\n    - npm test\n  coverage: '/Lines\\s*:\\s*(\\d+\\.\\d+)%/'\n  artifacts:\n    reports:\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage/cobertura-coverage.xml\n\ndeploy:\n  stage: deploy\n  image: bitnami/kubectl:latest\n  script:\n    - kubectl apply -f k8s/\n    - kubectl rollout status deployment/my-app\n  only:\n    - main\n  environment:\n    name: production\n    url: https://app.example.com\n```\n\n## Docker Build and Push\n\n```yaml\nbuild-docker:\n  stage: build\n  image: docker:24\n  services:\n    - docker:24-dind\n  before_script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n  script:\n    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .\n    - docker build -t $CI_REGISTRY_IMAGE:latest .\n    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n    - docker push $CI_REGISTRY_IMAGE:latest\n  only:\n    - main\n    - tags\n```\n\n## Multi-Environment Deployment\n\n```yaml\n.deploy_template: &deploy_template\n  image: bitnami/kubectl:latest\n  before_script:\n    - kubectl config set-cluster k8s --server=\"$KUBE_URL\" --insecure-skip-tls-verify=true\n    - kubectl config set-credentials admin --token=\"$KUBE_TOKEN\"\n    - kubectl config set-context default --cluster=k8s --user=admin\n    - kubectl config use-context default\n\ndeploy:staging:\n  <<: *deploy_template\n  stage: deploy\n  script:\n    - kubectl apply -f k8s/ -n staging\n    - kubectl rollout status deployment/my-app -n staging\n  environment:\n    name: staging\n    url: https://staging.example.com\n  only:\n    - develop\n\ndeploy:production:\n  <<: *deploy_template\n  stage: deploy\n  script:\n    - kubectl apply -f k8s/ -n production\n    - kubectl rollout status deployment/my-app -n production\n  environment:\n    name: production\n    url: https://app.example.com\n  when: manual\n  only:\n    - main\n```\n\n## Terraform Pipeline\n\n```yaml\nstages:\n  - validate\n  - plan\n  - apply\n\nvariables:\n  TF_ROOT: ${CI_PROJECT_DIR}/terraform\n  TF_VERSION: \"1.6.0\"\n\nbefore_script:\n  - cd ${TF_ROOT}\n  - terraform --version\n\nvalidate:\n  stage: validate\n  image: hashicorp/terraform:${TF_VERSION}\n  script:\n    - terraform init -backend=false\n    - terraform validate\n    - terraform fmt -check\n\nplan:\n  stage: plan\n  image: hashicorp/terraform:${TF_VERSION}\n  script:\n    - terraform init\n    - terraform plan -out=tfplan\n  artifacts:\n    paths:\n      - ${TF_ROOT}/tfplan\n    expire_in: 1 day\n\napply:\n  stage: apply\n  image: hashicorp/terraform:${TF_VERSION}\n  script:\n    - terraform init\n    - terraform apply -auto-approve tfplan\n  dependencies:\n    - plan\n  when: manual\n  only:\n    - main\n```\n\n## Security Scanning\n\n```yaml\ninclude:\n  - template: Security/SAST.gitlab-ci.yml\n  - template: Security/Dependency-Scanning.gitlab-ci.yml\n  - template: Security/Container-Scanning.gitlab-ci.yml\n\ntrivy-scan:\n  stage: test\n  image: aquasec/trivy:latest\n  script:\n    - trivy image --exit-code 1 --severity HIGH,CRITICAL $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n  allow_failure: true\n```\n\n## Caching Strategies\n\n```yaml\n# Cache node_modules\nbuild:\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n    policy: pull-push\n\n# Global cache\ncache:\n  key: ${CI_COMMIT_REF_SLUG}\n  paths:\n    - .cache/\n    - vendor/\n\n# Separate cache per job\njob1:\n  cache:\n    key: job1-cache\n    paths:\n      - build/\n\njob2:\n  cache:\n    key: job2-cache\n    paths:\n      - dist/\n```\n\n## Dynamic Child Pipelines\n\n```yaml\ngenerate-pipeline:\n  stage: build\n  script:\n    - python generate_pipeline.py > child-pipeline.yml\n  artifacts:\n    paths:\n      - child-pipeline.yml\n\ntrigger-child:\n  stage: deploy\n  trigger:\n    include:\n      - artifact: child-pipeline.yml\n        job: generate-pipeline\n    strategy: depend\n```\n\n## Reference Files\n\n- `assets/gitlab-ci.yml.template` - Complete pipeline template\n- `references/pipeline-stages.md` - Stage organization patterns\n\n## Best Practices\n\n1. **Use specific image tags** (node:20, not node:latest)\n2. **Cache dependencies** appropriately\n3. **Use artifacts** for build outputs\n4. **Implement manual gates** for production\n5. **Use environments** for deployment tracking\n6. **Enable merge request pipelines**\n7. **Use pipeline schedules** for recurring jobs\n8. **Implement security scanning**\n9. **Use CI/CD variables** for secrets\n10. **Monitor pipeline performance**\n\n## Related Skills\n\n- `github-actions-templates` - For GitHub Actions\n- `deployment-pipeline-design` - For architecture\n- `secrets-management` - For secrets handling\n",
      "references": {},
      "assets": {}
    },
    {
      "name": "secrets-management",
      "description": "Implement secure secrets management for CI/CD pipelines using Vault, AWS Secrets Manager, or native platform solutions. Use when handling sensitive credentials, rotating secrets, or securing CI/CD environments.",
      "plugin": "cicd-automation",
      "source_path": "plugins/cicd-automation/skills/secrets-management/SKILL.md",
      "category": "infrastructure",
      "keywords": [
        "ci-cd",
        "automation",
        "pipeline",
        "github-actions",
        "gitlab-ci"
      ],
      "content": "---\nname: secrets-management\ndescription: Implement secure secrets management for CI/CD pipelines using Vault, AWS Secrets Manager, or native platform solutions. Use when handling sensitive credentials, rotating secrets, or securing CI/CD environments.\n---\n\n# Secrets Management\n\nSecure secrets management practices for CI/CD pipelines using Vault, AWS Secrets Manager, and other tools.\n\n## Purpose\n\nImplement secure secrets management in CI/CD pipelines without hardcoding sensitive information.\n\n## When to Use\n\n- Store API keys and credentials\n- Manage database passwords\n- Handle TLS certificates\n- Rotate secrets automatically\n- Implement least-privilege access\n\n## Secrets Management Tools\n\n### HashiCorp Vault\n- Centralized secrets management\n- Dynamic secrets generation\n- Secret rotation\n- Audit logging\n- Fine-grained access control\n\n### AWS Secrets Manager\n- AWS-native solution\n- Automatic rotation\n- Integration with RDS\n- CloudFormation support\n\n### Azure Key Vault\n- Azure-native solution\n- HSM-backed keys\n- Certificate management\n- RBAC integration\n\n### Google Secret Manager\n- GCP-native solution\n- Versioning\n- IAM integration\n\n## HashiCorp Vault Integration\n\n### Setup Vault\n\n```bash\n# Start Vault dev server\nvault server -dev\n\n# Set environment\nexport VAULT_ADDR='http://127.0.0.1:8200'\nexport VAULT_TOKEN='root'\n\n# Enable secrets engine\nvault secrets enable -path=secret kv-v2\n\n# Store secret\nvault kv put secret/database/config username=admin password=secret\n```\n\n### GitHub Actions with Vault\n\n```yaml\nname: Deploy with Vault Secrets\n\non: [push]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Import Secrets from Vault\n      uses: hashicorp/vault-action@v2\n      with:\n        url: https://vault.example.com:8200\n        token: ${{ secrets.VAULT_TOKEN }}\n        secrets: |\n          secret/data/database username | DB_USERNAME ;\n          secret/data/database password | DB_PASSWORD ;\n          secret/data/api key | API_KEY\n\n    - name: Use secrets\n      run: |\n        echo \"Connecting to database as $DB_USERNAME\"\n        # Use $DB_PASSWORD, $API_KEY\n```\n\n### GitLab CI with Vault\n\n```yaml\ndeploy:\n  image: vault:latest\n  before_script:\n    - export VAULT_ADDR=https://vault.example.com:8200\n    - export VAULT_TOKEN=$VAULT_TOKEN\n    - apk add curl jq\n  script:\n    - |\n      DB_PASSWORD=$(vault kv get -field=password secret/database/config)\n      API_KEY=$(vault kv get -field=key secret/api/credentials)\n      echo \"Deploying with secrets...\"\n      # Use $DB_PASSWORD, $API_KEY\n```\n\n**Reference:** See `references/vault-setup.md`\n\n## AWS Secrets Manager\n\n### Store Secret\n\n```bash\naws secretsmanager create-secret \\\n  --name production/database/password \\\n  --secret-string \"super-secret-password\"\n```\n\n### Retrieve in GitHub Actions\n\n```yaml\n- name: Configure AWS credentials\n  uses: aws-actions/configure-aws-credentials@v4\n  with:\n    aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n    aws-region: us-west-2\n\n- name: Get secret from AWS\n  run: |\n    SECRET=$(aws secretsmanager get-secret-value \\\n      --secret-id production/database/password \\\n      --query SecretString \\\n      --output text)\n    echo \"::add-mask::$SECRET\"\n    echo \"DB_PASSWORD=$SECRET\" >> $GITHUB_ENV\n\n- name: Use secret\n  run: |\n    # Use $DB_PASSWORD\n    ./deploy.sh\n```\n\n### Terraform with AWS Secrets Manager\n\n```hcl\ndata \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id = \"production/database/password\"\n}\n\nresource \"aws_db_instance\" \"main\" {\n  allocated_storage    = 100\n  engine              = \"postgres\"\n  instance_class      = \"db.t3.large\"\n  username            = \"admin\"\n  password            = jsondecode(data.aws_secretsmanager_secret_version.db_password.secret_string)[\"password\"]\n}\n```\n\n## GitHub Secrets\n\n### Organization/Repository Secrets\n\n```yaml\n- name: Use GitHub secret\n  run: |\n    echo \"API Key: ${{ secrets.API_KEY }}\"\n    echo \"Database URL: ${{ secrets.DATABASE_URL }}\"\n```\n\n### Environment Secrets\n\n```yaml\ndeploy:\n  runs-on: ubuntu-latest\n  environment: production\n  steps:\n  - name: Deploy\n    run: |\n      echo \"Deploying with ${{ secrets.PROD_API_KEY }}\"\n```\n\n**Reference:** See `references/github-secrets.md`\n\n## GitLab CI/CD Variables\n\n### Project Variables\n\n```yaml\ndeploy:\n  script:\n    - echo \"Deploying with $API_KEY\"\n    - echo \"Database: $DATABASE_URL\"\n```\n\n### Protected and Masked Variables\n- Protected: Only available in protected branches\n- Masked: Hidden in job logs\n- File type: Stored as file\n\n## Best Practices\n\n1. **Never commit secrets** to Git\n2. **Use different secrets** per environment\n3. **Rotate secrets regularly**\n4. **Implement least-privilege access**\n5. **Enable audit logging**\n6. **Use secret scanning** (GitGuardian, TruffleHog)\n7. **Mask secrets in logs**\n8. **Encrypt secrets at rest**\n9. **Use short-lived tokens** when possible\n10. **Document secret requirements**\n\n## Secret Rotation\n\n### Automated Rotation with AWS\n\n```python\nimport boto3\nimport json\n\ndef lambda_handler(event, context):\n    client = boto3.client('secretsmanager')\n\n    # Get current secret\n    response = client.get_secret_value(SecretId='my-secret')\n    current_secret = json.loads(response['SecretString'])\n\n    # Generate new password\n    new_password = generate_strong_password()\n\n    # Update database password\n    update_database_password(new_password)\n\n    # Update secret\n    client.put_secret_value(\n        SecretId='my-secret',\n        SecretString=json.dumps({\n            'username': current_secret['username'],\n            'password': new_password\n        })\n    )\n\n    return {'statusCode': 200}\n```\n\n### Manual Rotation Process\n\n1. Generate new secret\n2. Update secret in secret store\n3. Update applications to use new secret\n4. Verify functionality\n5. Revoke old secret\n\n## External Secrets Operator\n\n### Kubernetes Integration\n\n```yaml\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: vault-backend\n  namespace: production\nspec:\n  provider:\n    vault:\n      server: \"https://vault.example.com:8200\"\n      path: \"secret\"\n      version: \"v2\"\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"production\"\n\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: database-credentials\n  namespace: production\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: vault-backend\n    kind: SecretStore\n  target:\n    name: database-credentials\n    creationPolicy: Owner\n  data:\n  - secretKey: username\n    remoteRef:\n      key: database/config\n      property: username\n  - secretKey: password\n    remoteRef:\n      key: database/config\n      property: password\n```\n\n## Secret Scanning\n\n### Pre-commit Hook\n\n```bash\n#!/bin/bash\n# .git/hooks/pre-commit\n\n# Check for secrets with TruffleHog\ndocker run --rm -v \"$(pwd):/repo\" \\\n  trufflesecurity/trufflehog:latest \\\n  filesystem --directory=/repo\n\nif [ $? -ne 0 ]; then\n  echo \"\u274c Secret detected! Commit blocked.\"\n  exit 1\nfi\n```\n\n### CI/CD Secret Scanning\n\n```yaml\nsecret-scan:\n  stage: security\n  image: trufflesecurity/trufflehog:latest\n  script:\n    - trufflehog filesystem .\n  allow_failure: false\n```\n\n## Reference Files\n\n- `references/vault-setup.md` - HashiCorp Vault configuration\n- `references/github-secrets.md` - GitHub Secrets best practices\n\n## Related Skills\n\n- `github-actions-templates` - For GitHub Actions integration\n- `gitlab-ci-patterns` - For GitLab CI integration\n- `deployment-pipeline-design` - For pipeline architecture\n",
      "references": {},
      "assets": {}
    }
  ]
}