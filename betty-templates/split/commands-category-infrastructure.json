{
  "total_count": 2,
  "category": "infrastructure",
  "commands": [
    {
      "name": "config-validate",
      "title": "Configuration Validation",
      "description": "You are a configuration management expert specializing in validating, testing, and ensuring the correctness of application configurations. Create comprehensive validation schemas, implement configurat",
      "plugin": "deployment-validation",
      "source_path": "plugins/deployment-validation/commands/config-validate.md",
      "category": "infrastructure",
      "keywords": [
        "validation",
        "pre-flight",
        "configuration",
        "deployment-safety"
      ],
      "content": "# Configuration Validation\n\nYou are a configuration management expert specializing in validating, testing, and ensuring the correctness of application configurations. Create comprehensive validation schemas, implement configuration testing strategies, and ensure configurations are secure, consistent, and error-free across all environments.\n\n## Context\nThe user needs to validate configuration files, implement configuration schemas, ensure consistency across environments, and prevent configuration-related errors. Focus on creating robust validation rules, type safety, security checks, and automated validation processes.\n\n## Requirements\n$ARGUMENTS\n\n## Instructions\n\n### 1. Configuration Analysis\n\nAnalyze existing configuration structure and identify validation needs:\n\n```python\nimport os\nimport yaml\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nclass ConfigurationAnalyzer:\n    def analyze_project(self, project_path: str) -> Dict[str, Any]:\n        analysis = {\n            'config_files': self._find_config_files(project_path),\n            'security_issues': self._check_security_issues(project_path),\n            'consistency_issues': self._check_consistency(project_path),\n            'recommendations': []\n        }\n        return analysis\n\n    def _find_config_files(self, project_path: str) -> List[Dict]:\n        config_patterns = [\n            '**/*.json', '**/*.yaml', '**/*.yml', '**/*.toml',\n            '**/*.ini', '**/*.env*', '**/config.js'\n        ]\n\n        config_files = []\n        for pattern in config_patterns:\n            for file_path in Path(project_path).glob(pattern):\n                if not self._should_ignore(file_path):\n                    config_files.append({\n                        'path': str(file_path),\n                        'type': self._detect_config_type(file_path),\n                        'environment': self._detect_environment(file_path)\n                    })\n        return config_files\n\n    def _check_security_issues(self, project_path: str) -> List[Dict]:\n        issues = []\n        secret_patterns = [\n            r'(api[_-]?key|apikey)',\n            r'(secret|password|passwd)',\n            r'(token|auth)',\n            r'(aws[_-]?access)'\n        ]\n\n        for config_file in self._find_config_files(project_path):\n            content = Path(config_file['path']).read_text()\n            for pattern in secret_patterns:\n                if re.search(pattern, content, re.IGNORECASE):\n                    if self._looks_like_real_secret(content, pattern):\n                        issues.append({\n                            'file': config_file['path'],\n                            'type': 'potential_secret',\n                            'severity': 'high'\n                        })\n        return issues\n```\n\n### 2. Schema Validation\n\nImplement configuration schema validation with JSON Schema:\n\n```typescript\nimport Ajv from 'ajv';\nimport ajvFormats from 'ajv-formats';\nimport { JSONSchema7 } from 'json-schema';\n\ninterface ValidationResult {\n  valid: boolean;\n  errors?: Array<{\n    path: string;\n    message: string;\n    keyword: string;\n  }>;\n}\n\nexport class ConfigValidator {\n  private ajv: Ajv;\n\n  constructor() {\n    this.ajv = new Ajv({\n      allErrors: true,\n      strict: false,\n      coerceTypes: true\n    });\n    ajvFormats(this.ajv);\n    this.addCustomFormats();\n  }\n\n  private addCustomFormats() {\n    this.ajv.addFormat('url-https', {\n      type: 'string',\n      validate: (data: string) => {\n        try {\n          return new URL(data).protocol === 'https:';\n        } catch { return false; }\n      }\n    });\n\n    this.ajv.addFormat('port', {\n      type: 'number',\n      validate: (data: number) => data >= 1 && data <= 65535\n    });\n\n    this.ajv.addFormat('duration', {\n      type: 'string',\n      validate: /^\\d+[smhd]$/\n    });\n  }\n\n  validate(configData: any, schemaName: string): ValidationResult {\n    const validate = this.ajv.getSchema(schemaName);\n    if (!validate) throw new Error(`Schema '${schemaName}' not found`);\n\n    const valid = validate(configData);\n\n    if (!valid && validate.errors) {\n      return {\n        valid: false,\n        errors: validate.errors.map(error => ({\n          path: error.instancePath || '/',\n          message: error.message || 'Validation error',\n          keyword: error.keyword\n        }))\n      };\n    }\n    return { valid: true };\n  }\n}\n\n// Example schema\nexport const schemas = {\n  database: {\n    type: 'object',\n    properties: {\n      host: { type: 'string', format: 'hostname' },\n      port: { type: 'integer', format: 'port' },\n      database: { type: 'string', minLength: 1 },\n      user: { type: 'string', minLength: 1 },\n      password: { type: 'string', minLength: 8 },\n      ssl: {\n        type: 'object',\n        properties: {\n          enabled: { type: 'boolean' }\n        },\n        required: ['enabled']\n      }\n    },\n    required: ['host', 'port', 'database', 'user', 'password']\n  }\n};\n```\n\n### 3. Environment-Specific Validation\n\n```python\nfrom typing import Dict, List, Any\n\nclass EnvironmentValidator:\n    def __init__(self):\n        self.environments = ['development', 'staging', 'production']\n        self.environment_rules = {\n            'development': {\n                'allow_debug': True,\n                'require_https': False,\n                'min_password_length': 8\n            },\n            'production': {\n                'allow_debug': False,\n                'require_https': True,\n                'min_password_length': 16,\n                'require_encryption': True\n            }\n        }\n\n    def validate_config(self, config: Dict, environment: str) -> List[Dict]:\n        if environment not in self.environment_rules:\n            raise ValueError(f\"Unknown environment: {environment}\")\n\n        rules = self.environment_rules[environment]\n        violations = []\n\n        if not rules['allow_debug'] and config.get('debug', False):\n            violations.append({\n                'rule': 'no_debug_in_production',\n                'message': 'Debug mode not allowed in production',\n                'severity': 'critical'\n            })\n\n        if rules['require_https']:\n            urls = self._extract_urls(config)\n            for url_path, url in urls:\n                if url.startswith('http://') and 'localhost' not in url:\n                    violations.append({\n                        'rule': 'require_https',\n                        'message': f'HTTPS required for {url_path}',\n                        'severity': 'high'\n                    })\n\n        return violations\n```\n\n### 4. Configuration Testing\n\n```typescript\nimport { describe, it, expect } from '@jest/globals';\nimport { ConfigValidator } from './config-validator';\n\ndescribe('Configuration Validation', () => {\n  let validator: ConfigValidator;\n\n  beforeEach(() => {\n    validator = new ConfigValidator();\n  });\n\n  it('should validate database config', () => {\n    const config = {\n      host: 'localhost',\n      port: 5432,\n      database: 'myapp',\n      user: 'dbuser',\n      password: 'securepass123'\n    };\n\n    const result = validator.validate(config, 'database');\n    expect(result.valid).toBe(true);\n  });\n\n  it('should reject invalid port', () => {\n    const config = {\n      host: 'localhost',\n      port: 70000,\n      database: 'myapp',\n      user: 'dbuser',\n      password: 'securepass123'\n    };\n\n    const result = validator.validate(config, 'database');\n    expect(result.valid).toBe(false);\n  });\n});\n```\n\n### 5. Runtime Validation\n\n```typescript\nimport { EventEmitter } from 'events';\nimport * as chokidar from 'chokidar';\n\nexport class RuntimeConfigValidator extends EventEmitter {\n  private validator: ConfigValidator;\n  private currentConfig: any;\n\n  async initialize(configPath: string): Promise<void> {\n    this.currentConfig = await this.loadAndValidate(configPath);\n    this.watchConfig(configPath);\n  }\n\n  private async loadAndValidate(configPath: string): Promise<any> {\n    const config = await this.loadConfig(configPath);\n\n    const validationResult = this.validator.validate(\n      config,\n      this.detectEnvironment()\n    );\n\n    if (!validationResult.valid) {\n      this.emit('validation:error', {\n        path: configPath,\n        errors: validationResult.errors\n      });\n\n      if (!this.isDevelopment()) {\n        throw new Error('Configuration validation failed');\n      }\n    }\n\n    return config;\n  }\n\n  private watchConfig(configPath: string): void {\n    const watcher = chokidar.watch(configPath, {\n      persistent: true,\n      ignoreInitial: true\n    });\n\n    watcher.on('change', async () => {\n      try {\n        const newConfig = await this.loadAndValidate(configPath);\n\n        if (JSON.stringify(newConfig) !== JSON.stringify(this.currentConfig)) {\n          this.emit('config:changed', {\n            oldConfig: this.currentConfig,\n            newConfig\n          });\n          this.currentConfig = newConfig;\n        }\n      } catch (error) {\n        this.emit('config:error', { error });\n      }\n    });\n  }\n}\n```\n\n### 6. Configuration Migration\n\n```python\nfrom typing import Dict\nfrom abc import ABC, abstractmethod\nimport semver\n\nclass ConfigMigration(ABC):\n    @property\n    @abstractmethod\n    def version(self) -> str:\n        pass\n\n    @abstractmethod\n    def up(self, config: Dict) -> Dict:\n        pass\n\n    @abstractmethod\n    def down(self, config: Dict) -> Dict:\n        pass\n\nclass ConfigMigrator:\n    def __init__(self):\n        self.migrations: List[ConfigMigration] = []\n\n    def migrate(self, config: Dict, target_version: str) -> Dict:\n        current_version = config.get('_version', '0.0.0')\n\n        if semver.compare(current_version, target_version) == 0:\n            return config\n\n        result = config.copy()\n        for migration in self.migrations:\n            if (semver.compare(migration.version, current_version) > 0 and\n                semver.compare(migration.version, target_version) <= 0):\n                result = migration.up(result)\n                result['_version'] = migration.version\n\n        return result\n```\n\n### 7. Secure Configuration\n\n```typescript\nimport * as crypto from 'crypto';\n\ninterface EncryptedValue {\n  encrypted: true;\n  value: string;\n  algorithm: string;\n  iv: string;\n  authTag?: string;\n}\n\nexport class SecureConfigManager {\n  private encryptionKey: Buffer;\n\n  constructor(masterKey: string) {\n    this.encryptionKey = crypto.pbkdf2Sync(masterKey, 'config-salt', 100000, 32, 'sha256');\n  }\n\n  encrypt(value: any): EncryptedValue {\n    const algorithm = 'aes-256-gcm';\n    const iv = crypto.randomBytes(16);\n    const cipher = crypto.createCipheriv(algorithm, this.encryptionKey, iv);\n\n    let encrypted = cipher.update(JSON.stringify(value), 'utf8', 'hex');\n    encrypted += cipher.final('hex');\n\n    return {\n      encrypted: true,\n      value: encrypted,\n      algorithm,\n      iv: iv.toString('hex'),\n      authTag: cipher.getAuthTag().toString('hex')\n    };\n  }\n\n  decrypt(encryptedValue: EncryptedValue): any {\n    const decipher = crypto.createDecipheriv(\n      encryptedValue.algorithm,\n      this.encryptionKey,\n      Buffer.from(encryptedValue.iv, 'hex')\n    );\n\n    if (encryptedValue.authTag) {\n      decipher.setAuthTag(Buffer.from(encryptedValue.authTag, 'hex'));\n    }\n\n    let decrypted = decipher.update(encryptedValue.value, 'hex', 'utf8');\n    decrypted += decipher.final('utf8');\n\n    return JSON.parse(decrypted);\n  }\n\n  async processConfig(config: any): Promise<any> {\n    const processed = {};\n\n    for (const [key, value] of Object.entries(config)) {\n      if (this.isEncryptedValue(value)) {\n        processed[key] = this.decrypt(value as EncryptedValue);\n      } else if (typeof value === 'object' && value !== null) {\n        processed[key] = await this.processConfig(value);\n      } else {\n        processed[key] = value;\n      }\n    }\n\n    return processed;\n  }\n}\n```\n\n### 8. Documentation Generation\n\n```python\nfrom typing import Dict, List\nimport yaml\n\nclass ConfigDocGenerator:\n    def generate_docs(self, schema: Dict, examples: Dict) -> str:\n        docs = [\"# Configuration Reference\\n\"]\n\n        docs.append(\"## Configuration Options\\n\")\n        sections = self._generate_sections(schema.get('properties', {}), examples)\n        docs.extend(sections)\n\n        return '\\n'.join(docs)\n\n    def _generate_sections(self, properties: Dict, examples: Dict, level: int = 3) -> List[str]:\n        sections = []\n\n        for prop_name, prop_schema in properties.items():\n            sections.append(f\"{'#' * level} {prop_name}\\n\")\n\n            if 'description' in prop_schema:\n                sections.append(f\"{prop_schema['description']}\\n\")\n\n            sections.append(f\"**Type:** `{prop_schema.get('type', 'any')}`\\n\")\n\n            if 'default' in prop_schema:\n                sections.append(f\"**Default:** `{prop_schema['default']}`\\n\")\n\n            if prop_name in examples:\n                sections.append(\"**Example:**\\n```yaml\")\n                sections.append(yaml.dump({prop_name: examples[prop_name]}))\n                sections.append(\"```\\n\")\n\n        return sections\n```\n\n## Output Format\n\n1. **Configuration Analysis**: Current configuration assessment\n2. **Validation Schemas**: JSON Schema definitions\n3. **Environment Rules**: Environment-specific validation\n4. **Test Suite**: Configuration tests\n5. **Migration Scripts**: Version migrations\n6. **Security Report**: Issues and recommendations\n7. **Documentation**: Auto-generated reference\n\nFocus on preventing configuration errors, ensuring consistency, and maintaining security best practices.\n"
    },
    {
      "name": "workflow-automate",
      "title": "Workflow Automation",
      "description": "You are a workflow automation expert specializing in creating efficient CI/CD pipelines, GitHub Actions workflows, and automated development processes. Design and implement automation that reduces man",
      "plugin": "cicd-automation",
      "source_path": "plugins/cicd-automation/commands/workflow-automate.md",
      "category": "infrastructure",
      "keywords": [
        "ci-cd",
        "automation",
        "pipeline",
        "github-actions",
        "gitlab-ci"
      ],
      "content": "# Workflow Automation\n\nYou are a workflow automation expert specializing in creating efficient CI/CD pipelines, GitHub Actions workflows, and automated development processes. Design and implement automation that reduces manual work, improves consistency, and accelerates delivery while maintaining quality and security.\n\n## Context\nThe user needs to automate development workflows, deployment processes, or operational tasks. Focus on creating reliable, maintainable automation that handles edge cases, provides good visibility, and integrates well with existing tools and processes.\n\n## Requirements\n$ARGUMENTS\n\n## Instructions\n\n### 1. Workflow Analysis\n\nAnalyze existing processes and identify automation opportunities:\n\n**Workflow Discovery Script**\n```python\nimport os\nimport yaml\nimport json\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nclass WorkflowAnalyzer:\n    def analyze_project(self, project_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Analyze project to identify automation opportunities\n        \"\"\"\n        analysis = {\n            'current_workflows': self._find_existing_workflows(project_path),\n            'manual_processes': self._identify_manual_processes(project_path),\n            'automation_opportunities': [],\n            'tool_recommendations': [],\n            'complexity_score': 0\n        }\n        \n        # Analyze different aspects\n        analysis['build_process'] = self._analyze_build_process(project_path)\n        analysis['test_process'] = self._analyze_test_process(project_path)\n        analysis['deployment_process'] = self._analyze_deployment_process(project_path)\n        analysis['code_quality'] = self._analyze_code_quality_checks(project_path)\n        \n        # Generate recommendations\n        self._generate_recommendations(analysis)\n        \n        return analysis\n    \n    def _find_existing_workflows(self, project_path: str) -> List[Dict]:\n        \"\"\"Find existing CI/CD workflows\"\"\"\n        workflows = []\n        \n        # GitHub Actions\n        gh_workflow_path = Path(project_path) / '.github' / 'workflows'\n        if gh_workflow_path.exists():\n            for workflow_file in gh_workflow_path.glob('*.y*ml'):\n                with open(workflow_file) as f:\n                    workflow = yaml.safe_load(f)\n                    workflows.append({\n                        'type': 'github_actions',\n                        'name': workflow.get('name', workflow_file.stem),\n                        'file': str(workflow_file),\n                        'triggers': list(workflow.get('on', {}).keys())\n                    })\n        \n        # GitLab CI\n        gitlab_ci = Path(project_path) / '.gitlab-ci.yml'\n        if gitlab_ci.exists():\n            with open(gitlab_ci) as f:\n                config = yaml.safe_load(f)\n                workflows.append({\n                    'type': 'gitlab_ci',\n                    'name': 'GitLab CI Pipeline',\n                    'file': str(gitlab_ci),\n                    'stages': config.get('stages', [])\n                })\n        \n        # Jenkins\n        jenkinsfile = Path(project_path) / 'Jenkinsfile'\n        if jenkinsfile.exists():\n            workflows.append({\n                'type': 'jenkins',\n                'name': 'Jenkins Pipeline',\n                'file': str(jenkinsfile)\n            })\n        \n        return workflows\n    \n    def _identify_manual_processes(self, project_path: str) -> List[Dict]:\n        \"\"\"Identify processes that could be automated\"\"\"\n        manual_processes = []\n        \n        # Check for manual build scripts\n        script_patterns = ['build.sh', 'deploy.sh', 'release.sh', 'test.sh']\n        for pattern in script_patterns:\n            scripts = Path(project_path).glob(f'**/{pattern}')\n            for script in scripts:\n                manual_processes.append({\n                    'type': 'script',\n                    'file': str(script),\n                    'purpose': pattern.replace('.sh', ''),\n                    'automation_potential': 'high'\n                })\n        \n        # Check README for manual steps\n        readme_files = ['README.md', 'README.rst', 'README.txt']\n        for readme_name in readme_files:\n            readme = Path(project_path) / readme_name\n            if readme.exists():\n                content = readme.read_text()\n                if any(keyword in content.lower() for keyword in ['manually', 'by hand', 'steps to']):\n                    manual_processes.append({\n                        'type': 'documented_process',\n                        'file': str(readme),\n                        'indicators': 'Contains manual process documentation'\n                    })\n        \n        return manual_processes\n    \n    def _generate_recommendations(self, analysis: Dict) -> None:\n        \"\"\"Generate automation recommendations\"\"\"\n        recommendations = []\n        \n        # CI/CD recommendations\n        if not analysis['current_workflows']:\n            recommendations.append({\n                'priority': 'high',\n                'category': 'ci_cd',\n                'recommendation': 'Implement CI/CD pipeline',\n                'tools': ['GitHub Actions', 'GitLab CI', 'Jenkins'],\n                'effort': 'medium'\n            })\n        \n        # Build automation\n        if analysis['build_process']['manual_steps']:\n            recommendations.append({\n                'priority': 'high',\n                'category': 'build',\n                'recommendation': 'Automate build process',\n                'tools': ['Make', 'Gradle', 'npm scripts'],\n                'effort': 'low'\n            })\n        \n        # Test automation\n        if not analysis['test_process']['automated_tests']:\n            recommendations.append({\n                'priority': 'high',\n                'category': 'testing',\n                'recommendation': 'Implement automated testing',\n                'tools': ['Jest', 'Pytest', 'JUnit'],\n                'effort': 'medium'\n            })\n        \n        # Deployment automation\n        if analysis['deployment_process']['manual_deployment']:\n            recommendations.append({\n                'priority': 'critical',\n                'category': 'deployment',\n                'recommendation': 'Automate deployment process',\n                'tools': ['ArgoCD', 'Flux', 'Terraform'],\n                'effort': 'high'\n            })\n        \n        analysis['automation_opportunities'] = recommendations\n```\n\n### 2. GitHub Actions Workflows\n\nCreate comprehensive GitHub Actions workflows:\n\n**Multi-Environment CI/CD Pipeline**\n```yaml\n# .github/workflows/ci-cd.yml\nname: CI/CD Pipeline\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main]\n  release:\n    types: [created]\n\nenv:\n  NODE_VERSION: '18'\n  PYTHON_VERSION: '3.11'\n  GO_VERSION: '1.21'\n\njobs:\n  # Code quality checks\n  quality:\n    name: Code Quality\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0  # Full history for better analysis\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n\n      - name: Cache dependencies\n        uses: actions/cache@v3\n        with:\n          path: |\n            ~/.npm\n            ~/.cache\n            node_modules\n          key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n          restore-keys: |\n            ${{ runner.os }}-node-\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run linting\n        run: |\n          npm run lint\n          npm run lint:styles\n\n      - name: Type checking\n        run: npm run typecheck\n\n      - name: Security audit\n        run: |\n          npm audit --production\n          npx snyk test\n\n      - name: License check\n        run: npx license-checker --production --onlyAllow 'MIT;Apache-2.0;BSD-3-Clause;BSD-2-Clause;ISC'\n\n  # Testing\n  test:\n    name: Test Suite\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n        node: [16, 18, 20]\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node }}\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run unit tests\n        run: npm run test:unit -- --coverage\n\n      - name: Run integration tests\n        run: npm run test:integration\n        env:\n          TEST_DATABASE_URL: ${{ secrets.TEST_DATABASE_URL }}\n\n      - name: Upload coverage\n        if: matrix.os == 'ubuntu-latest' && matrix.node == 18\n        uses: codecov/codecov-action@v3\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n          flags: unittests\n          name: codecov-umbrella\n\n  # Build\n  build:\n    name: Build Application\n    needs: [quality, test]\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        environment: [development, staging, production]\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up build environment\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build application\n        run: npm run build\n        env:\n          NODE_ENV: ${{ matrix.environment }}\n          BUILD_NUMBER: ${{ github.run_number }}\n          COMMIT_SHA: ${{ github.sha }}\n\n      - name: Build Docker image\n        run: |\n          docker build \\\n            --build-arg BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ') \\\n            --build-arg VCS_REF=${GITHUB_SHA::8} \\\n            --build-arg VERSION=${GITHUB_REF#refs/tags/} \\\n            -t ${{ github.repository }}:${{ matrix.environment }}-${{ github.sha }} \\\n            -t ${{ github.repository }}:${{ matrix.environment }}-latest \\\n            .\n\n      - name: Scan Docker image\n        uses: aquasecurity/trivy-action@master\n        with:\n          image-ref: ${{ github.repository }}:${{ matrix.environment }}-${{ github.sha }}\n          format: 'sarif'\n          output: 'trivy-results.sarif'\n\n      - name: Upload scan results\n        uses: github/codeql-action/upload-sarif@v2\n        with:\n          sarif_file: 'trivy-results.sarif'\n\n      - name: Push to registry\n        if: github.event_name != 'pull_request'\n        run: |\n          echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin\n          docker push ${{ github.repository }}:${{ matrix.environment }}-${{ github.sha }}\n          docker push ${{ github.repository }}:${{ matrix.environment }}-latest\n\n      - name: Upload artifacts\n        uses: actions/upload-artifact@v3\n        with:\n          name: build-${{ matrix.environment }}\n          path: |\n            dist/\n            build/\n            .next/\n          retention-days: 7\n\n  # Deploy\n  deploy:\n    name: Deploy to ${{ matrix.environment }}\n    needs: build\n    runs-on: ubuntu-latest\n    if: github.event_name != 'pull_request'\n    strategy:\n      matrix:\n        environment: [staging, production]\n        exclude:\n          - environment: production\n            branches: [develop]\n    environment:\n      name: ${{ matrix.environment }}\n      url: ${{ steps.deploy.outputs.url }}\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v2\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n\n      - name: Deploy to ECS\n        id: deploy\n        run: |\n          # Update task definition\n          aws ecs register-task-definition \\\n            --family myapp-${{ matrix.environment }} \\\n            --container-definitions \"[{\n              \\\"name\\\": \\\"app\\\",\n              \\\"image\\\": \\\"${{ github.repository }}:${{ matrix.environment }}-${{ github.sha }}\\\",\n              \\\"environment\\\": [{\n                \\\"name\\\": \\\"ENVIRONMENT\\\",\n                \\\"value\\\": \\\"${{ matrix.environment }}\\\"\n              }]\n            }]\"\n          \n          # Update service\n          aws ecs update-service \\\n            --cluster ${{ matrix.environment }}-cluster \\\n            --service myapp-service \\\n            --task-definition myapp-${{ matrix.environment }}\n          \n          # Get service URL\n          echo \"url=https://${{ matrix.environment }}.example.com\" >> $GITHUB_OUTPUT\n\n      - name: Notify deployment\n        uses: 8398a7/action-slack@v3\n        with:\n          status: ${{ job.status }}\n          text: Deployment to ${{ matrix.environment }} ${{ job.status }}\n          webhook_url: ${{ secrets.SLACK_WEBHOOK }}\n        if: always()\n\n  # Post-deployment verification\n  verify:\n    name: Verify Deployment\n    needs: deploy\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        environment: [staging, production]\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Run smoke tests\n        run: |\n          npm run test:smoke -- --url https://${{ matrix.environment }}.example.com\n\n      - name: Run E2E tests\n        uses: cypress-io/github-action@v5\n        with:\n          config: baseUrl=https://${{ matrix.environment }}.example.com\n          record: true\n        env:\n          CYPRESS_RECORD_KEY: ${{ secrets.CYPRESS_RECORD_KEY }}\n\n      - name: Performance test\n        run: |\n          npm install -g @sitespeed.io/sitespeed.io\n          sitespeed.io https://${{ matrix.environment }}.example.com \\\n            --budget.configPath=.sitespeed.io/budget.json \\\n            --plugins.add=@sitespeed.io/plugin-lighthouse\n\n      - name: Security scan\n        run: |\n          npm install -g @zaproxy/action-baseline\n          zaproxy/action-baseline -t https://${{ matrix.environment }}.example.com\n```\n\n### 3. Release Automation\n\nAutomate release processes:\n\n**Semantic Release Workflow**\n```yaml\n# .github/workflows/release.yml\nname: Release\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  release:\n    name: Create Release\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n          persist-credentials: false\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: 18\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run semantic release\n        env:\n          GITHUB_TOKEN: ${{ secrets.SEMANTIC_RELEASE_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: npx semantic-release\n\n      - name: Update documentation\n        if: steps.semantic-release.outputs.new_release_published == 'true'\n        run: |\n          npm run docs:generate\n          npm run docs:publish\n\n      - name: Create release notes\n        if: steps.semantic-release.outputs.new_release_published == 'true'\n        uses: actions/github-script@v6\n        with:\n          script: |\n            const { data: releases } = await github.rest.repos.listReleases({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              per_page: 1\n            });\n            \n            const latestRelease = releases[0];\n            const changelog = await generateChangelog(latestRelease);\n            \n            // Update release notes\n            await github.rest.repos.updateRelease({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              release_id: latestRelease.id,\n              body: changelog\n            });\n```\n\n**Release Configuration**\n```javascript\n// .releaserc.js\nmodule.exports = {\n  branches: [\n    'main',\n    { name: 'beta', prerelease: true },\n    { name: 'alpha', prerelease: true }\n  ],\n  plugins: [\n    '@semantic-release/commit-analyzer',\n    '@semantic-release/release-notes-generator',\n    ['@semantic-release/changelog', {\n      changelogFile: 'CHANGELOG.md'\n    }],\n    '@semantic-release/npm',\n    ['@semantic-release/git', {\n      assets: ['CHANGELOG.md', 'package.json'],\n      message: 'chore(release): ${nextRelease.version} [skip ci]\\n\\n${nextRelease.notes}'\n    }],\n    '@semantic-release/github'\n  ]\n};\n```\n\n### 4. Development Workflow Automation\n\nAutomate common development tasks:\n\n**Pre-commit Hooks**\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.5.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n        args: ['--maxkb=1000']\n      - id: check-case-conflict\n      - id: check-merge-conflict\n      - id: detect-private-key\n\n  - repo: https://github.com/psf/black\n    rev: 23.10.0\n    hooks:\n      - id: black\n        language_version: python3.11\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n        args: [\"--profile\", \"black\"]\n\n  - repo: https://github.com/pycqa/flake8\n    rev: 6.1.0\n    hooks:\n      - id: flake8\n        additional_dependencies: [flake8-docstrings]\n\n  - repo: https://github.com/pre-commit/mirrors-eslint\n    rev: v8.52.0\n    hooks:\n      - id: eslint\n        files: \\.[jt]sx?$\n        types: [file]\n        additional_dependencies:\n          - eslint@8.52.0\n          - eslint-config-prettier@9.0.0\n          - eslint-plugin-react@7.33.2\n\n  - repo: https://github.com/pre-commit/mirrors-prettier\n    rev: v3.0.3\n    hooks:\n      - id: prettier\n        types_or: [css, javascript, jsx, typescript, tsx, json, yaml]\n\n  - repo: local\n    hooks:\n      - id: unit-tests\n        name: Run unit tests\n        entry: npm run test:unit -- --passWithNoTests\n        language: system\n        pass_filenames: false\n        stages: [commit]\n```\n\n**Development Environment Setup**\n```bash\n#!/bin/bash\n# scripts/setup-dev-environment.sh\n\nset -euo pipefail\n\necho \"\ud83d\ude80 Setting up development environment...\"\n\n# Check prerequisites\ncheck_prerequisites() {\n    echo \"Checking prerequisites...\"\n    \n    commands=(\"git\" \"node\" \"npm\" \"docker\" \"docker-compose\")\n    for cmd in \"${commands[@]}\"; do\n        if ! command -v \"$cmd\" &> /dev/null; then\n            echo \"\u274c $cmd is not installed\"\n            exit 1\n        fi\n    done\n    \n    echo \"\u2705 All prerequisites installed\"\n}\n\n# Install dependencies\ninstall_dependencies() {\n    echo \"Installing dependencies...\"\n    npm ci\n    \n    # Install global tools\n    npm install -g @commitlint/cli @commitlint/config-conventional\n    npm install -g semantic-release\n    \n    # Install pre-commit\n    pip install pre-commit\n    pre-commit install\n    pre-commit install --hook-type commit-msg\n}\n\n# Setup local services\nsetup_services() {\n    echo \"Setting up local services...\"\n    \n    # Create docker network\n    docker network create dev-network 2>/dev/null || true\n    \n    # Start services\n    docker-compose -f docker-compose.dev.yml up -d\n    \n    # Wait for services\n    echo \"Waiting for services to be ready...\"\n    ./scripts/wait-for-services.sh\n}\n\n# Initialize database\ninitialize_database() {\n    echo \"Initializing database...\"\n    npm run db:migrate\n    npm run db:seed\n}\n\n# Setup environment variables\nsetup_environment() {\n    echo \"Setting up environment variables...\"\n    \n    if [ ! -f .env.local ]; then\n        cp .env.example .env.local\n        echo \"\u2705 Created .env.local from .env.example\"\n        echo \"\u26a0\ufe0f  Please update .env.local with your values\"\n    fi\n}\n\n# Main execution\nmain() {\n    check_prerequisites\n    install_dependencies\n    setup_services\n    setup_environment\n    initialize_database\n    \n    echo \"\u2705 Development environment setup complete!\"\n    echo \"\"\n    echo \"Next steps:\"\n    echo \"1. Update .env.local with your configuration\"\n    echo \"2. Run 'npm run dev' to start the development server\"\n    echo \"3. Visit http://localhost:3000\"\n}\n\nmain\n```\n\n### 5. Infrastructure Automation\n\nAutomate infrastructure provisioning:\n\n**Terraform Workflow**\n```yaml\n# .github/workflows/terraform.yml\nname: Terraform\n\non:\n  pull_request:\n    paths:\n      - 'terraform/**'\n      - '.github/workflows/terraform.yml'\n  push:\n    branches:\n      - main\n    paths:\n      - 'terraform/**'\n\nenv:\n  TF_VERSION: '1.6.0'\n  TF_VAR_project_name: ${{ github.event.repository.name }}\n\njobs:\n  terraform:\n    name: Terraform Plan & Apply\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: terraform\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v2\n        with:\n          terraform_version: ${{ env.TF_VERSION }}\n          terraform_wrapper: false\n      \n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v2\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      \n      - name: Terraform Format Check\n        run: terraform fmt -check -recursive\n      \n      - name: Terraform Init\n        run: |\n          terraform init \\\n            -backend-config=\"bucket=${{ secrets.TF_STATE_BUCKET }}\" \\\n            -backend-config=\"key=${{ github.repository }}/terraform.tfstate\" \\\n            -backend-config=\"region=us-east-1\"\n      \n      - name: Terraform Validate\n        run: terraform validate\n      \n      - name: Terraform Plan\n        id: plan\n        run: |\n          terraform plan -out=tfplan -no-color | tee plan_output.txt\n          \n          # Extract plan summary\n          echo \"PLAN_SUMMARY<<EOF\" >> $GITHUB_ENV\n          grep -E '(Plan:|No changes.|# )' plan_output.txt >> $GITHUB_ENV\n          echo \"EOF\" >> $GITHUB_ENV\n      \n      - name: Comment PR\n        if: github.event_name == 'pull_request'\n        uses: actions/github-script@v6\n        with:\n          script: |\n            const output = `#### Terraform Plan \ud83d\udcd6\n            \\`\\`\\`\n            ${process.env.PLAN_SUMMARY}\n            \\`\\`\\`\n            \n            *Pushed by: @${{ github.actor }}, Action: \\`${{ github.event_name }}\\`*`;\n            \n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: output\n            });\n      \n      - name: Terraform Apply\n        if: github.ref == 'refs/heads/main' && github.event_name == 'push'\n        run: terraform apply tfplan\n```\n\n### 6. Monitoring and Alerting Automation\n\nAutomate monitoring setup:\n\n**Monitoring Stack Deployment**\n```yaml\n# .github/workflows/monitoring.yml\nname: Deploy Monitoring\n\non:\n  push:\n    paths:\n      - 'monitoring/**'\n      - '.github/workflows/monitoring.yml'\n    branches:\n      - main\n\njobs:\n  deploy-monitoring:\n    name: Deploy Monitoring Stack\n    runs-on: ubuntu-latest\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Setup Helm\n        uses: azure/setup-helm@v3\n        with:\n          version: '3.12.0'\n      \n      - name: Configure Kubernetes\n        run: |\n          echo \"${{ secrets.KUBE_CONFIG }}\" | base64 -d > kubeconfig\n          export KUBECONFIG=kubeconfig\n      \n      - name: Add Helm repositories\n        run: |\n          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n          helm repo add grafana https://grafana.github.io/helm-charts\n          helm repo update\n      \n      - name: Deploy Prometheus\n        run: |\n          helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \\\n            --namespace monitoring \\\n            --create-namespace \\\n            --values monitoring/prometheus-values.yaml \\\n            --wait\n      \n      - name: Deploy Grafana Dashboards\n        run: |\n          kubectl apply -f monitoring/dashboards/\n      \n      - name: Deploy Alert Rules\n        run: |\n          kubectl apply -f monitoring/alerts/\n      \n      - name: Setup Alert Routing\n        run: |\n          helm upgrade --install alertmanager prometheus-community/alertmanager \\\n            --namespace monitoring \\\n            --values monitoring/alertmanager-values.yaml\n```\n\n### 7. Dependency Update Automation\n\nAutomate dependency updates:\n\n**Renovate Configuration**\n```json\n{\n  \"extends\": [\n    \"config:base\",\n    \":dependencyDashboard\",\n    \":semanticCommits\",\n    \":automergeDigest\",\n    \":automergeMinor\"\n  ],\n  \"schedule\": [\"after 10pm every weekday\", \"before 5am every weekday\", \"every weekend\"],\n  \"timezone\": \"America/New_York\",\n  \"vulnerabilityAlerts\": {\n    \"labels\": [\"security\"],\n    \"automerge\": true\n  },\n  \"packageRules\": [\n    {\n      \"matchDepTypes\": [\"devDependencies\"],\n      \"automerge\": true\n    },\n    {\n      \"matchPackagePatterns\": [\"^@types/\"],\n      \"automerge\": true\n    },\n    {\n      \"matchPackageNames\": [\"node\"],\n      \"enabled\": false\n    },\n    {\n      \"matchPackagePatterns\": [\"^eslint\"],\n      \"groupName\": \"eslint packages\",\n      \"automerge\": true\n    },\n    {\n      \"matchManagers\": [\"docker\"],\n      \"pinDigests\": true\n    }\n  ],\n  \"postUpdateOptions\": [\n    \"npmDedupe\",\n    \"yarnDedupeHighest\"\n  ],\n  \"prConcurrentLimit\": 3,\n  \"prCreation\": \"not-pending\",\n  \"rebaseWhen\": \"behind-base-branch\",\n  \"semanticCommitScope\": \"deps\"\n}\n```\n\n### 8. Documentation Automation\n\nAutomate documentation generation:\n\n**Documentation Workflow**\n```yaml\n# .github/workflows/docs.yml\nname: Documentation\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'src/**'\n      - 'docs/**'\n      - 'README.md'\n\njobs:\n  generate-docs:\n    name: Generate Documentation\n    runs-on: ubuntu-latest\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: 18\n      \n      - name: Install dependencies\n        run: npm ci\n      \n      - name: Generate API docs\n        run: |\n          npm run docs:api\n          npm run docs:typescript\n      \n      - name: Generate architecture diagrams\n        run: |\n          npm install -g @mermaid-js/mermaid-cli\n          mmdc -i docs/architecture.mmd -o docs/architecture.png\n      \n      - name: Build documentation site\n        run: |\n          npm run docs:build\n      \n      - name: Deploy to GitHub Pages\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./docs/dist\n          cname: docs.example.com\n```\n\n**Documentation Generation Script**\n```typescript\n// scripts/generate-docs.ts\nimport { Application, TSConfigReader, TypeDocReader } from 'typedoc';\nimport { generateMarkdown } from './markdown-generator';\nimport { createApiReference } from './api-reference';\n\nasync function generateDocumentation() {\n  // TypeDoc for TypeScript documentation\n  const app = new Application();\n  app.options.addReader(new TSConfigReader());\n  app.options.addReader(new TypeDocReader());\n  \n  app.bootstrap({\n    entryPoints: ['src/index.ts'],\n    out: 'docs/api',\n    theme: 'default',\n    includeVersion: true,\n    excludePrivate: true,\n    readme: 'README.md',\n    plugin: ['typedoc-plugin-markdown']\n  });\n  \n  const project = app.convert();\n  if (project) {\n    await app.generateDocs(project, 'docs/api');\n    \n    // Generate custom markdown docs\n    await generateMarkdown(project, {\n      output: 'docs/guides',\n      includeExamples: true,\n      generateTOC: true\n    });\n    \n    // Create API reference\n    await createApiReference(project, {\n      format: 'openapi',\n      output: 'docs/openapi.json',\n      includeSchemas: true\n    });\n  }\n  \n  // Generate architecture documentation\n  await generateArchitectureDocs();\n  \n  // Generate deployment guides\n  await generateDeploymentGuides();\n}\n\nasync function generateArchitectureDocs() {\n  const mermaidDiagrams = `\n    graph TB\n      A[Client] --> B[Load Balancer]\n      B --> C[Web Server]\n      C --> D[Application Server]\n      D --> E[Database]\n      D --> F[Cache]\n      D --> G[Message Queue]\n  `;\n  \n  // Save diagrams and generate documentation\n  await fs.writeFile('docs/architecture.mmd', mermaidDiagrams);\n}\n```\n\n### 9. Security Automation\n\nAutomate security scanning and compliance:\n\n**Security Scanning Workflow**\n```yaml\n# .github/workflows/security.yml\nname: Security Scan\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n  schedule:\n    - cron: '0 0 * * 0'  # Weekly on Sunday\n\njobs:\n  security-scan:\n    name: Security Scanning\n    runs-on: ubuntu-latest\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Run Trivy vulnerability scanner\n        uses: aquasecurity/trivy-action@master\n        with:\n          scan-type: 'fs'\n          scan-ref: '.'\n          format: 'sarif'\n          output: 'trivy-results.sarif'\n          severity: 'CRITICAL,HIGH'\n      \n      - name: Upload Trivy results\n        uses: github/codeql-action/upload-sarif@v2\n        with:\n          sarif_file: 'trivy-results.sarif'\n      \n      - name: Run Snyk security scan\n        uses: snyk/actions/node@master\n        env:\n          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n        with:\n          args: --severity-threshold=high\n      \n      - name: Run OWASP Dependency Check\n        uses: dependency-check/Dependency-Check_Action@main\n        with:\n          project: ${{ github.repository }}\n          path: '.'\n          format: 'ALL'\n          args: >\n            --enableRetired\n            --enableExperimental\n      \n      - name: SonarCloud Scan\n        uses: SonarSource/sonarcloud-github-action@master\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}\n      \n      - name: Run Semgrep\n        uses: returntocorp/semgrep-action@v1\n        with:\n          config: >-\n            p/security-audit\n            p/secrets\n            p/owasp-top-ten\n      \n      - name: GitLeaks secret scanning\n        uses: gitleaks/gitleaks-action@v2\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n```\n\n### 10. Workflow Orchestration\n\nCreate complex workflow orchestration:\n\n**Workflow Orchestrator**\n```typescript\n// workflow-orchestrator.ts\nimport { EventEmitter } from 'events';\nimport { Logger } from 'winston';\n\ninterface WorkflowStep {\n  name: string;\n  type: 'parallel' | 'sequential';\n  steps?: WorkflowStep[];\n  action?: () => Promise<any>;\n  retries?: number;\n  timeout?: number;\n  condition?: () => boolean;\n  onError?: 'fail' | 'continue' | 'retry';\n}\n\nexport class WorkflowOrchestrator extends EventEmitter {\n  constructor(\n    private logger: Logger,\n    private config: WorkflowConfig\n  ) {\n    super();\n  }\n  \n  async execute(workflow: WorkflowStep): Promise<WorkflowResult> {\n    const startTime = Date.now();\n    const result: WorkflowResult = {\n      success: true,\n      steps: [],\n      duration: 0\n    };\n    \n    try {\n      await this.executeStep(workflow, result);\n    } catch (error) {\n      result.success = false;\n      result.error = error;\n      this.emit('workflow:failed', result);\n    }\n    \n    result.duration = Date.now() - startTime;\n    this.emit('workflow:completed', result);\n    \n    return result;\n  }\n  \n  private async executeStep(\n    step: WorkflowStep,\n    result: WorkflowResult,\n    parentPath: string = ''\n  ): Promise<void> {\n    const stepPath = parentPath ? `${parentPath}.${step.name}` : step.name;\n    \n    this.emit('step:start', { step: stepPath });\n    \n    // Check condition\n    if (step.condition && !step.condition()) {\n      this.logger.info(`Skipping step ${stepPath} due to condition`);\n      this.emit('step:skipped', { step: stepPath });\n      return;\n    }\n    \n    const stepResult: StepResult = {\n      name: step.name,\n      path: stepPath,\n      startTime: Date.now(),\n      success: true\n    };\n    \n    try {\n      if (step.action) {\n        // Execute single action\n        await this.executeAction(step, stepResult);\n      } else if (step.steps) {\n        // Execute sub-steps\n        if (step.type === 'parallel') {\n          await this.executeParallel(step.steps, result, stepPath);\n        } else {\n          await this.executeSequential(step.steps, result, stepPath);\n        }\n      }\n      \n      stepResult.endTime = Date.now();\n      stepResult.duration = stepResult.endTime - stepResult.startTime;\n      result.steps.push(stepResult);\n      \n      this.emit('step:complete', { step: stepPath, result: stepResult });\n    } catch (error) {\n      stepResult.success = false;\n      stepResult.error = error;\n      result.steps.push(stepResult);\n      \n      this.emit('step:failed', { step: stepPath, error });\n      \n      if (step.onError === 'fail') {\n        throw error;\n      }\n    }\n  }\n  \n  private async executeAction(\n    step: WorkflowStep,\n    stepResult: StepResult\n  ): Promise<void> {\n    const timeout = step.timeout || this.config.defaultTimeout;\n    const retries = step.retries || 0;\n    \n    let lastError: Error;\n    \n    for (let attempt = 0; attempt <= retries; attempt++) {\n      try {\n        const result = await Promise.race([\n          step.action!(),\n          this.createTimeout(timeout)\n        ]);\n        \n        stepResult.output = result;\n        return;\n      } catch (error) {\n        lastError = error as Error;\n        \n        if (attempt < retries) {\n          this.logger.warn(`Step ${step.name} failed, retry ${attempt + 1}/${retries}`);\n          await this.delay(this.calculateBackoff(attempt));\n        }\n      }\n    }\n    \n    throw lastError!;\n  }\n  \n  private async executeParallel(\n    steps: WorkflowStep[],\n    result: WorkflowResult,\n    parentPath: string\n  ): Promise<void> {\n    await Promise.all(\n      steps.map(step => this.executeStep(step, result, parentPath))\n    );\n  }\n  \n  private async executeSequential(\n    steps: WorkflowStep[],\n    result: WorkflowResult,\n    parentPath: string\n  ): Promise<void> {\n    for (const step of steps) {\n      await this.executeStep(step, result, parentPath);\n    }\n  }\n  \n  private createTimeout(ms: number): Promise<never> {\n    return new Promise((_, reject) => {\n      setTimeout(() => reject(new Error(`Timeout after ${ms}ms`)), ms);\n    });\n  }\n  \n  private calculateBackoff(attempt: number): number {\n    return Math.min(1000 * Math.pow(2, attempt), 30000);\n  }\n  \n  private delay(ms: number): Promise<void> {\n    return new Promise(resolve => setTimeout(resolve, ms));\n  }\n}\n\n// Example workflow definition\nexport const deploymentWorkflow: WorkflowStep = {\n  name: 'deployment',\n  type: 'sequential',\n  steps: [\n    {\n      name: 'pre-deployment',\n      type: 'parallel',\n      steps: [\n        {\n          name: 'backup-database',\n          action: async () => {\n            // Backup database\n          },\n          timeout: 300000 // 5 minutes\n        },\n        {\n          name: 'health-check',\n          action: async () => {\n            // Check system health\n          },\n          retries: 3\n        }\n      ]\n    },\n    {\n      name: 'deployment',\n      type: 'sequential',\n      steps: [\n        {\n          name: 'blue-green-switch',\n          action: async () => {\n            // Switch traffic to new version\n          },\n          onError: 'retry',\n          retries: 2\n        },\n        {\n          name: 'smoke-tests',\n          action: async () => {\n            // Run smoke tests\n          },\n          onError: 'fail'\n        }\n      ]\n    },\n    {\n      name: 'post-deployment',\n      type: 'parallel',\n      steps: [\n        {\n          name: 'notify-teams',\n          action: async () => {\n            // Send notifications\n          },\n          onError: 'continue'\n        },\n        {\n          name: 'update-monitoring',\n          action: async () => {\n            // Update monitoring dashboards\n          }\n        }\n      ]\n    }\n  ]\n};\n```\n\n## Output Format\n\n1. **Workflow Analysis**: Current processes and automation opportunities\n2. **CI/CD Pipeline**: Complete GitHub Actions/GitLab CI configuration\n3. **Release Automation**: Semantic versioning and release workflows\n4. **Development Automation**: Pre-commit hooks and setup scripts\n5. **Infrastructure Automation**: Terraform and Kubernetes workflows\n6. **Security Automation**: Scanning and compliance workflows\n7. **Documentation Generation**: Automated docs and diagrams\n8. **Workflow Orchestration**: Complex workflow management\n9. **Monitoring Integration**: Automated alerts and dashboards\n10. **Implementation Guide**: Step-by-step setup instructions\n\nFocus on creating reliable, maintainable automation that reduces manual work while maintaining quality and security standards."
    }
  ]
}