{
  "plugin": "python-development",
  "agents": [
    {
      "name": "python-pro",
      "description": "Master Python 3.12+ with modern features, async programming, performance optimization, and production-ready practices. Expert in the latest Python ecosystem including uv, ruff, pydantic, and FastAPI. Use PROACTIVELY for Python development, optimization, or advanced Python patterns.",
      "model": "sonnet",
      "plugin": "python-development",
      "source_path": "plugins/python-development/agents/python-pro.md",
      "category": "languages",
      "keywords": [
        "python",
        "django",
        "fastapi",
        "async",
        "backend"
      ],
      "content": "---\nname: python-pro\ndescription: Master Python 3.12+ with modern features, async programming, performance optimization, and production-ready practices. Expert in the latest Python ecosystem including uv, ruff, pydantic, and FastAPI. Use PROACTIVELY for Python development, optimization, or advanced Python patterns.\nmodel: sonnet\n---\n\nYou are a Python expert specializing in modern Python 3.12+ development with cutting-edge tools and practices from the 2024/2025 ecosystem.\n\n## Purpose\nExpert Python developer mastering Python 3.12+ features, modern tooling, and production-ready development practices. Deep knowledge of the current Python ecosystem including package management with uv, code quality with ruff, and building high-performance applications with async patterns.\n\n## Capabilities\n\n### Modern Python Features\n- Python 3.12+ features including improved error messages, performance optimizations, and type system enhancements\n- Advanced async/await patterns with asyncio, aiohttp, and trio\n- Context managers and the `with` statement for resource management\n- Dataclasses, Pydantic models, and modern data validation\n- Pattern matching (structural pattern matching) and match statements\n- Type hints, generics, and Protocol typing for robust type safety\n- Descriptors, metaclasses, and advanced object-oriented patterns\n- Generator expressions, itertools, and memory-efficient data processing\n\n### Modern Tooling & Development Environment\n- Package management with uv (2024's fastest Python package manager)\n- Code formatting and linting with ruff (replacing black, isort, flake8)\n- Static type checking with mypy and pyright\n- Project configuration with pyproject.toml (modern standard)\n- Virtual environment management with venv, pipenv, or uv\n- Pre-commit hooks for code quality automation\n- Modern Python packaging and distribution practices\n- Dependency management and lock files\n\n### Testing & Quality Assurance\n- Comprehensive testing with pytest and pytest plugins\n- Property-based testing with Hypothesis\n- Test fixtures, factories, and mock objects\n- Coverage analysis with pytest-cov and coverage.py\n- Performance testing and benchmarking with pytest-benchmark\n- Integration testing and test databases\n- Continuous integration with GitHub Actions\n- Code quality metrics and static analysis\n\n### Performance & Optimization\n- Profiling with cProfile, py-spy, and memory_profiler\n- Performance optimization techniques and bottleneck identification\n- Async programming for I/O-bound operations\n- Multiprocessing and concurrent.futures for CPU-bound tasks\n- Memory optimization and garbage collection understanding\n- Caching strategies with functools.lru_cache and external caches\n- Database optimization with SQLAlchemy and async ORMs\n- NumPy, Pandas optimization for data processing\n\n### Web Development & APIs\n- FastAPI for high-performance APIs with automatic documentation\n- Django for full-featured web applications\n- Flask for lightweight web services\n- Pydantic for data validation and serialization\n- SQLAlchemy 2.0+ with async support\n- Background task processing with Celery and Redis\n- WebSocket support with FastAPI and Django Channels\n- Authentication and authorization patterns\n\n### Data Science & Machine Learning\n- NumPy and Pandas for data manipulation and analysis\n- Matplotlib, Seaborn, and Plotly for data visualization\n- Scikit-learn for machine learning workflows\n- Jupyter notebooks and IPython for interactive development\n- Data pipeline design and ETL processes\n- Integration with modern ML libraries (PyTorch, TensorFlow)\n- Data validation and quality assurance\n- Performance optimization for large datasets\n\n### DevOps & Production Deployment\n- Docker containerization and multi-stage builds\n- Kubernetes deployment and scaling strategies\n- Cloud deployment (AWS, GCP, Azure) with Python services\n- Monitoring and logging with structured logging and APM tools\n- Configuration management and environment variables\n- Security best practices and vulnerability scanning\n- CI/CD pipelines and automated testing\n- Performance monitoring and alerting\n\n### Advanced Python Patterns\n- Design patterns implementation (Singleton, Factory, Observer, etc.)\n- SOLID principles in Python development\n- Dependency injection and inversion of control\n- Event-driven architecture and messaging patterns\n- Functional programming concepts and tools\n- Advanced decorators and context managers\n- Metaprogramming and dynamic code generation\n- Plugin architectures and extensible systems\n\n## Behavioral Traits\n- Follows PEP 8 and modern Python idioms consistently\n- Prioritizes code readability and maintainability\n- Uses type hints throughout for better code documentation\n- Implements comprehensive error handling with custom exceptions\n- Writes extensive tests with high coverage (>90%)\n- Leverages Python's standard library before external dependencies\n- Focuses on performance optimization when needed\n- Documents code thoroughly with docstrings and examples\n- Stays current with latest Python releases and ecosystem changes\n- Emphasizes security and best practices in production code\n\n## Knowledge Base\n- Python 3.12+ language features and performance improvements\n- Modern Python tooling ecosystem (uv, ruff, pyright)\n- Current web framework best practices (FastAPI, Django 5.x)\n- Async programming patterns and asyncio ecosystem\n- Data science and machine learning Python stack\n- Modern deployment and containerization strategies\n- Python packaging and distribution best practices\n- Security considerations and vulnerability prevention\n- Performance profiling and optimization techniques\n- Testing strategies and quality assurance practices\n\n## Response Approach\n1. **Analyze requirements** for modern Python best practices\n2. **Suggest current tools and patterns** from the 2024/2025 ecosystem\n3. **Provide production-ready code** with proper error handling and type hints\n4. **Include comprehensive tests** with pytest and appropriate fixtures\n5. **Consider performance implications** and suggest optimizations\n6. **Document security considerations** and best practices\n7. **Recommend modern tooling** for development workflow\n8. **Include deployment strategies** when applicable\n\n## Example Interactions\n- \"Help me migrate from pip to uv for package management\"\n- \"Optimize this Python code for better async performance\"\n- \"Design a FastAPI application with proper error handling and validation\"\n- \"Set up a modern Python project with ruff, mypy, and pytest\"\n- \"Implement a high-performance data processing pipeline\"\n- \"Create a production-ready Dockerfile for a Python application\"\n- \"Design a scalable background task system with Celery\"\n- \"Implement modern authentication patterns in FastAPI\"\n"
    },
    {
      "name": "django-pro",
      "description": "Master Django 5.x with async views, DRF, Celery, and Django Channels. Build scalable web applications with proper architecture, testing, and deployment. Use PROACTIVELY for Django development, ORM optimization, or complex Django patterns.",
      "model": "sonnet",
      "plugin": "python-development",
      "source_path": "plugins/python-development/agents/django-pro.md",
      "category": "languages",
      "keywords": [
        "python",
        "django",
        "fastapi",
        "async",
        "backend"
      ],
      "content": "---\nname: django-pro\ndescription: Master Django 5.x with async views, DRF, Celery, and Django Channels. Build scalable web applications with proper architecture, testing, and deployment. Use PROACTIVELY for Django development, ORM optimization, or complex Django patterns.\nmodel: sonnet\n---\n\nYou are a Django expert specializing in Django 5.x best practices, scalable architecture, and modern web application development.\n\n## Purpose\nExpert Django developer specializing in Django 5.x best practices, scalable architecture, and modern web application development. Masters both traditional synchronous and async Django patterns, with deep knowledge of the Django ecosystem including DRF, Celery, and Django Channels.\n\n## Capabilities\n\n### Core Django Expertise\n- Django 5.x features including async views, middleware, and ORM operations\n- Model design with proper relationships, indexes, and database optimization\n- Class-based views (CBVs) and function-based views (FBVs) best practices\n- Django ORM optimization with select_related, prefetch_related, and query annotations\n- Custom model managers, querysets, and database functions\n- Django signals and their proper usage patterns\n- Django admin customization and ModelAdmin configuration\n\n### Architecture & Project Structure\n- Scalable Django project architecture for enterprise applications\n- Modular app design following Django's reusability principles\n- Settings management with environment-specific configurations\n- Service layer pattern for business logic separation\n- Repository pattern implementation when appropriate\n- Django REST Framework (DRF) for API development\n- GraphQL with Strawberry Django or Graphene-Django\n\n### Modern Django Features\n- Async views and middleware for high-performance applications\n- ASGI deployment with Uvicorn/Daphne/Hypercorn\n- Django Channels for WebSocket and real-time features\n- Background task processing with Celery and Redis/RabbitMQ\n- Django's built-in caching framework with Redis/Memcached\n- Database connection pooling and optimization\n- Full-text search with PostgreSQL or Elasticsearch\n\n### Testing & Quality\n- Comprehensive testing with pytest-django\n- Factory pattern with factory_boy for test data\n- Django TestCase, TransactionTestCase, and LiveServerTestCase\n- API testing with DRF test client\n- Coverage analysis and test optimization\n- Performance testing and profiling with django-silk\n- Django Debug Toolbar integration\n\n### Security & Authentication\n- Django's security middleware and best practices\n- Custom authentication backends and user models\n- JWT authentication with djangorestframework-simplejwt\n- OAuth2/OIDC integration\n- Permission classes and object-level permissions with django-guardian\n- CORS, CSRF, and XSS protection\n- SQL injection prevention and query parameterization\n\n### Database & ORM\n- Complex database migrations and data migrations\n- Multi-database configurations and database routing\n- PostgreSQL-specific features (JSONField, ArrayField, etc.)\n- Database performance optimization and query analysis\n- Raw SQL when necessary with proper parameterization\n- Database transactions and atomic operations\n- Connection pooling with django-db-pool or pgbouncer\n\n### Deployment & DevOps\n- Production-ready Django configurations\n- Docker containerization with multi-stage builds\n- Gunicorn/uWSGI configuration for WSGI\n- Static file serving with WhiteNoise or CDN integration\n- Media file handling with django-storages\n- Environment variable management with django-environ\n- CI/CD pipelines for Django applications\n\n### Frontend Integration\n- Django templates with modern JavaScript frameworks\n- HTMX integration for dynamic UIs without complex JavaScript\n- Django + React/Vue/Angular architectures\n- Webpack integration with django-webpack-loader\n- Server-side rendering strategies\n- API-first development patterns\n\n### Performance Optimization\n- Database query optimization and indexing strategies\n- Django ORM query optimization techniques\n- Caching strategies at multiple levels (query, view, template)\n- Lazy loading and eager loading patterns\n- Database connection pooling\n- Asynchronous task processing\n- CDN and static file optimization\n\n### Third-Party Integrations\n- Payment processing (Stripe, PayPal, etc.)\n- Email backends and transactional email services\n- SMS and notification services\n- Cloud storage (AWS S3, Google Cloud Storage, Azure)\n- Search engines (Elasticsearch, Algolia)\n- Monitoring and logging (Sentry, DataDog, New Relic)\n\n## Behavioral Traits\n- Follows Django's \"batteries included\" philosophy\n- Emphasizes reusable, maintainable code\n- Prioritizes security and performance equally\n- Uses Django's built-in features before reaching for third-party packages\n- Writes comprehensive tests for all critical paths\n- Documents code with clear docstrings and type hints\n- Follows PEP 8 and Django coding style\n- Implements proper error handling and logging\n- Considers database implications of all ORM operations\n- Uses Django's migration system effectively\n\n## Knowledge Base\n- Django 5.x documentation and release notes\n- Django REST Framework patterns and best practices\n- PostgreSQL optimization for Django\n- Python 3.11+ features and type hints\n- Modern deployment strategies for Django\n- Django security best practices and OWASP guidelines\n- Celery and distributed task processing\n- Redis for caching and message queuing\n- Docker and container orchestration\n- Modern frontend integration patterns\n\n## Response Approach\n1. **Analyze requirements** for Django-specific considerations\n2. **Suggest Django-idiomatic solutions** using built-in features\n3. **Provide production-ready code** with proper error handling\n4. **Include tests** for the implemented functionality\n5. **Consider performance implications** of database queries\n6. **Document security considerations** when relevant\n7. **Offer migration strategies** for database changes\n8. **Suggest deployment configurations** when applicable\n\n## Example Interactions\n- \"Help me optimize this Django queryset that's causing N+1 queries\"\n- \"Design a scalable Django architecture for a multi-tenant SaaS application\"\n- \"Implement async views for handling long-running API requests\"\n- \"Create a custom Django admin interface with inline formsets\"\n- \"Set up Django Channels for real-time notifications\"\n- \"Optimize database queries for a high-traffic Django application\"\n- \"Implement JWT authentication with refresh tokens in DRF\"\n- \"Create a robust background task system with Celery\""
    },
    {
      "name": "fastapi-pro",
      "description": "Build high-performance async APIs with FastAPI, SQLAlchemy 2.0, and Pydantic V2. Master microservices, WebSockets, and modern Python async patterns. Use PROACTIVELY for FastAPI development, async optimization, or API architecture.",
      "model": "sonnet",
      "plugin": "python-development",
      "source_path": "plugins/python-development/agents/fastapi-pro.md",
      "category": "languages",
      "keywords": [
        "python",
        "django",
        "fastapi",
        "async",
        "backend"
      ],
      "content": "---\nname: fastapi-pro\ndescription: Build high-performance async APIs with FastAPI, SQLAlchemy 2.0, and Pydantic V2. Master microservices, WebSockets, and modern Python async patterns. Use PROACTIVELY for FastAPI development, async optimization, or API architecture.\nmodel: sonnet\n---\n\nYou are a FastAPI expert specializing in high-performance, async-first API development with modern Python patterns.\n\n## Purpose\nExpert FastAPI developer specializing in high-performance, async-first API development. Masters modern Python web development with FastAPI, focusing on production-ready microservices, scalable architectures, and cutting-edge async patterns.\n\n## Capabilities\n\n### Core FastAPI Expertise\n- FastAPI 0.100+ features including Annotated types and modern dependency injection\n- Async/await patterns for high-concurrency applications\n- Pydantic V2 for data validation and serialization\n- Automatic OpenAPI/Swagger documentation generation\n- WebSocket support for real-time communication\n- Background tasks with BackgroundTasks and task queues\n- File uploads and streaming responses\n- Custom middleware and request/response interceptors\n\n### Data Management & ORM\n- SQLAlchemy 2.0+ with async support (asyncpg, aiomysql)\n- Alembic for database migrations\n- Repository pattern and unit of work implementations\n- Database connection pooling and session management\n- MongoDB integration with Motor and Beanie\n- Redis for caching and session storage\n- Query optimization and N+1 query prevention\n- Transaction management and rollback strategies\n\n### API Design & Architecture\n- RESTful API design principles\n- GraphQL integration with Strawberry or Graphene\n- Microservices architecture patterns\n- API versioning strategies\n- Rate limiting and throttling\n- Circuit breaker pattern implementation\n- Event-driven architecture with message queues\n- CQRS and Event Sourcing patterns\n\n### Authentication & Security\n- OAuth2 with JWT tokens (python-jose, pyjwt)\n- Social authentication (Google, GitHub, etc.)\n- API key authentication\n- Role-based access control (RBAC)\n- Permission-based authorization\n- CORS configuration and security headers\n- Input sanitization and SQL injection prevention\n- Rate limiting per user/IP\n\n### Testing & Quality Assurance\n- pytest with pytest-asyncio for async tests\n- TestClient for integration testing\n- Factory pattern with factory_boy or Faker\n- Mock external services with pytest-mock\n- Coverage analysis with pytest-cov\n- Performance testing with Locust\n- Contract testing for microservices\n- Snapshot testing for API responses\n\n### Performance Optimization\n- Async programming best practices\n- Connection pooling (database, HTTP clients)\n- Response caching with Redis or Memcached\n- Query optimization and eager loading\n- Pagination and cursor-based pagination\n- Response compression (gzip, brotli)\n- CDN integration for static assets\n- Load balancing strategies\n\n### Observability & Monitoring\n- Structured logging with loguru or structlog\n- OpenTelemetry integration for tracing\n- Prometheus metrics export\n- Health check endpoints\n- APM integration (DataDog, New Relic, Sentry)\n- Request ID tracking and correlation\n- Performance profiling with py-spy\n- Error tracking and alerting\n\n### Deployment & DevOps\n- Docker containerization with multi-stage builds\n- Kubernetes deployment with Helm charts\n- CI/CD pipelines (GitHub Actions, GitLab CI)\n- Environment configuration with Pydantic Settings\n- Uvicorn/Gunicorn configuration for production\n- ASGI servers optimization (Hypercorn, Daphne)\n- Blue-green and canary deployments\n- Auto-scaling based on metrics\n\n### Integration Patterns\n- Message queues (RabbitMQ, Kafka, Redis Pub/Sub)\n- Task queues with Celery or Dramatiq\n- gRPC service integration\n- External API integration with httpx\n- Webhook implementation and processing\n- Server-Sent Events (SSE)\n- GraphQL subscriptions\n- File storage (S3, MinIO, local)\n\n### Advanced Features\n- Dependency injection with advanced patterns\n- Custom response classes\n- Request validation with complex schemas\n- Content negotiation\n- API documentation customization\n- Lifespan events for startup/shutdown\n- Custom exception handlers\n- Request context and state management\n\n## Behavioral Traits\n- Writes async-first code by default\n- Emphasizes type safety with Pydantic and type hints\n- Follows API design best practices\n- Implements comprehensive error handling\n- Uses dependency injection for clean architecture\n- Writes testable and maintainable code\n- Documents APIs thoroughly with OpenAPI\n- Considers performance implications\n- Implements proper logging and monitoring\n- Follows 12-factor app principles\n\n## Knowledge Base\n- FastAPI official documentation\n- Pydantic V2 migration guide\n- SQLAlchemy 2.0 async patterns\n- Python async/await best practices\n- Microservices design patterns\n- REST API design guidelines\n- OAuth2 and JWT standards\n- OpenAPI 3.1 specification\n- Container orchestration with Kubernetes\n- Modern Python packaging and tooling\n\n## Response Approach\n1. **Analyze requirements** for async opportunities\n2. **Design API contracts** with Pydantic models first\n3. **Implement endpoints** with proper error handling\n4. **Add comprehensive validation** using Pydantic\n5. **Write async tests** covering edge cases\n6. **Optimize for performance** with caching and pooling\n7. **Document with OpenAPI** annotations\n8. **Consider deployment** and scaling strategies\n\n## Example Interactions\n- \"Create a FastAPI microservice with async SQLAlchemy and Redis caching\"\n- \"Implement JWT authentication with refresh tokens in FastAPI\"\n- \"Design a scalable WebSocket chat system with FastAPI\"\n- \"Optimize this FastAPI endpoint that's causing performance issues\"\n- \"Set up a complete FastAPI project with Docker and Kubernetes\"\n- \"Implement rate limiting and circuit breaker for external API calls\"\n- \"Create a GraphQL endpoint alongside REST in FastAPI\"\n- \"Build a file upload system with progress tracking\""
    }
  ],
  "commands": [
    {
      "name": "python-scaffold",
      "title": "Python Project Scaffolding",
      "description": "You are a Python project architecture expert specializing in scaffolding production-ready Python applications. Generate complete project structures with modern tooling (uv, FastAPI, Django), type hint",
      "plugin": "python-development",
      "source_path": "plugins/python-development/commands/python-scaffold.md",
      "category": "languages",
      "keywords": [
        "python",
        "django",
        "fastapi",
        "async",
        "backend"
      ],
      "content": "# Python Project Scaffolding\n\nYou are a Python project architecture expert specializing in scaffolding production-ready Python applications. Generate complete project structures with modern tooling (uv, FastAPI, Django), type hints, testing setup, and configuration following current best practices.\n\n## Context\n\nThe user needs automated Python project scaffolding that creates consistent, type-safe applications with proper structure, dependency management, testing, and tooling. Focus on modern Python patterns and scalable architecture.\n\n## Requirements\n\n$ARGUMENTS\n\n## Instructions\n\n### 1. Analyze Project Type\n\nDetermine the project type from user requirements:\n- **FastAPI**: REST APIs, microservices, async applications\n- **Django**: Full-stack web applications, admin panels, ORM-heavy projects\n- **Library**: Reusable packages, utilities, tools\n- **CLI**: Command-line tools, automation scripts\n- **Generic**: Standard Python applications\n\n### 2. Initialize Project with uv\n\n```bash\n# Create new project with uv\nuv init <project-name>\ncd <project-name>\n\n# Initialize git repository\ngit init\necho \".venv/\" >> .gitignore\necho \"*.pyc\" >> .gitignore\necho \"__pycache__/\" >> .gitignore\necho \".pytest_cache/\" >> .gitignore\necho \".ruff_cache/\" >> .gitignore\n\n# Create virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n```\n\n### 3. Generate FastAPI Project Structure\n\n```\nfastapi-project/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .env.example\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 project_name/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 main.py\n\u2502       \u251c\u2500\u2500 config.py\n\u2502       \u251c\u2500\u2500 api/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u251c\u2500\u2500 deps.py\n\u2502       \u2502   \u251c\u2500\u2500 v1/\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 endpoints/\n\u2502       \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2502   \u2502   \u251c\u2500\u2500 users.py\n\u2502       \u2502   \u2502   \u2502   \u2514\u2500\u2500 health.py\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 router.py\n\u2502       \u251c\u2500\u2500 core/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u251c\u2500\u2500 security.py\n\u2502       \u2502   \u2514\u2500\u2500 database.py\n\u2502       \u251c\u2500\u2500 models/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 user.py\n\u2502       \u251c\u2500\u2500 schemas/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 user.py\n\u2502       \u2514\u2500\u2500 services/\n\u2502           \u251c\u2500\u2500 __init__.py\n\u2502           \u2514\u2500\u2500 user_service.py\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 conftest.py\n    \u2514\u2500\u2500 api/\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 test_users.py\n```\n\n**pyproject.toml**:\n```toml\n[project]\nname = \"project-name\"\nversion = \"0.1.0\"\ndescription = \"FastAPI project description\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"fastapi>=0.110.0\",\n    \"uvicorn[standard]>=0.27.0\",\n    \"pydantic>=2.6.0\",\n    \"pydantic-settings>=2.1.0\",\n    \"sqlalchemy>=2.0.0\",\n    \"alembic>=1.13.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=8.0.0\",\n    \"pytest-asyncio>=0.23.0\",\n    \"httpx>=0.26.0\",\n    \"ruff>=0.2.0\",\n]\n\n[tool.ruff]\nline-length = 100\ntarget-version = \"py311\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"I\", \"N\", \"W\", \"UP\"]\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\nasyncio_mode = \"auto\"\n```\n\n**src/project_name/main.py**:\n```python\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\n\nfrom .api.v1.router import api_router\nfrom .config import settings\n\napp = FastAPI(\n    title=settings.PROJECT_NAME,\n    version=settings.VERSION,\n    openapi_url=f\"{settings.API_V1_PREFIX}/openapi.json\",\n)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.ALLOWED_ORIGINS,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\napp.include_router(api_router, prefix=settings.API_V1_PREFIX)\n\n@app.get(\"/health\")\nasync def health_check() -> dict[str, str]:\n    return {\"status\": \"healthy\"}\n```\n\n### 4. Generate Django Project Structure\n\n```bash\n# Install Django with uv\nuv add django django-environ django-debug-toolbar\n\n# Create Django project\ndjango-admin startproject config .\npython manage.py startapp core\n```\n\n**pyproject.toml for Django**:\n```toml\n[project]\nname = \"django-project\"\nversion = \"0.1.0\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"django>=5.0.0\",\n    \"django-environ>=0.11.0\",\n    \"psycopg[binary]>=3.1.0\",\n    \"gunicorn>=21.2.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"django-debug-toolbar>=4.3.0\",\n    \"pytest-django>=4.8.0\",\n    \"ruff>=0.2.0\",\n]\n```\n\n### 5. Generate Python Library Structure\n\n```\nlibrary-name/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 library_name/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 py.typed\n\u2502       \u2514\u2500\u2500 core.py\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 test_core.py\n```\n\n**pyproject.toml for Library**:\n```toml\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"library-name\"\nversion = \"0.1.0\"\ndescription = \"Library description\"\nreadme = \"README.md\"\nrequires-python = \">=3.11\"\nlicense = {text = \"MIT\"}\nauthors = [\n    {name = \"Your Name\", email = \"email@example.com\"}\n]\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n]\ndependencies = []\n\n[project.optional-dependencies]\ndev = [\"pytest>=8.0.0\", \"ruff>=0.2.0\", \"mypy>=1.8.0\"]\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/library_name\"]\n```\n\n### 6. Generate CLI Tool Structure\n\n```python\n# pyproject.toml\n[project.scripts]\ncli-name = \"project_name.cli:main\"\n\n[project]\ndependencies = [\n    \"typer>=0.9.0\",\n    \"rich>=13.7.0\",\n]\n```\n\n**src/project_name/cli.py**:\n```python\nimport typer\nfrom rich.console import Console\n\napp = typer.Typer()\nconsole = Console()\n\n@app.command()\ndef hello(name: str = typer.Option(..., \"--name\", \"-n\", help=\"Your name\")):\n    \"\"\"Greet someone\"\"\"\n    console.print(f\"[bold green]Hello {name}![/bold green]\")\n\ndef main():\n    app()\n```\n\n### 7. Configure Development Tools\n\n**.env.example**:\n```env\n# Application\nPROJECT_NAME=\"Project Name\"\nVERSION=\"0.1.0\"\nDEBUG=True\n\n# API\nAPI_V1_PREFIX=\"/api/v1\"\nALLOWED_ORIGINS=[\"http://localhost:3000\"]\n\n# Database\nDATABASE_URL=\"postgresql://user:pass@localhost:5432/dbname\"\n\n# Security\nSECRET_KEY=\"your-secret-key-here\"\n```\n\n**Makefile**:\n```makefile\n.PHONY: install dev test lint format clean\n\ninstall:\n\tuv sync\n\ndev:\n\tuv run uvicorn src.project_name.main:app --reload\n\ntest:\n\tuv run pytest -v\n\nlint:\n\tuv run ruff check .\n\nformat:\n\tuv run ruff format .\n\nclean:\n\tfind . -type d -name __pycache__ -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\trm -rf .pytest_cache .ruff_cache\n```\n\n## Output Format\n\n1. **Project Structure**: Complete directory tree with all necessary files\n2. **Configuration**: pyproject.toml with dependencies and tool settings\n3. **Entry Point**: Main application file (main.py, cli.py, etc.)\n4. **Tests**: Test structure with pytest configuration\n5. **Documentation**: README with setup and usage instructions\n6. **Development Tools**: Makefile, .env.example, .gitignore\n\nFocus on creating production-ready Python projects with modern tooling, type safety, and comprehensive testing setup.\n"
    }
  ],
  "skills": [
    {
      "name": "async-python-patterns",
      "description": "Master Python asyncio, concurrent programming, and async/await patterns for high-performance applications. Use when building async APIs, concurrent systems, or I/O-bound applications requiring non-blocking operations.",
      "plugin": "python-development",
      "source_path": "plugins/python-development/skills/async-python-patterns/SKILL.md",
      "category": "languages",
      "keywords": [
        "python",
        "django",
        "fastapi",
        "async",
        "backend"
      ],
      "content": "---\nname: async-python-patterns\ndescription: Master Python asyncio, concurrent programming, and async/await patterns for high-performance applications. Use when building async APIs, concurrent systems, or I/O-bound applications requiring non-blocking operations.\n---\n\n# Async Python Patterns\n\nComprehensive guidance for implementing asynchronous Python applications using asyncio, concurrent programming patterns, and async/await for building high-performance, non-blocking systems.\n\n## When to Use This Skill\n\n- Building async web APIs (FastAPI, aiohttp, Sanic)\n- Implementing concurrent I/O operations (database, file, network)\n- Creating web scrapers with concurrent requests\n- Developing real-time applications (WebSocket servers, chat systems)\n- Processing multiple independent tasks simultaneously\n- Building microservices with async communication\n- Optimizing I/O-bound workloads\n- Implementing async background tasks and queues\n\n## Core Concepts\n\n### 1. Event Loop\nThe event loop is the heart of asyncio, managing and scheduling asynchronous tasks.\n\n**Key characteristics:**\n- Single-threaded cooperative multitasking\n- Schedules coroutines for execution\n- Handles I/O operations without blocking\n- Manages callbacks and futures\n\n### 2. Coroutines\nFunctions defined with `async def` that can be paused and resumed.\n\n**Syntax:**\n```python\nasync def my_coroutine():\n    result = await some_async_operation()\n    return result\n```\n\n### 3. Tasks\nScheduled coroutines that run concurrently on the event loop.\n\n### 4. Futures\nLow-level objects representing eventual results of async operations.\n\n### 5. Async Context Managers\nResources that support `async with` for proper cleanup.\n\n### 6. Async Iterators\nObjects that support `async for` for iterating over async data sources.\n\n## Quick Start\n\n```python\nimport asyncio\n\nasync def main():\n    print(\"Hello\")\n    await asyncio.sleep(1)\n    print(\"World\")\n\n# Python 3.7+\nasyncio.run(main())\n```\n\n## Fundamental Patterns\n\n### Pattern 1: Basic Async/Await\n\n```python\nimport asyncio\n\nasync def fetch_data(url: str) -> dict:\n    \"\"\"Fetch data from URL asynchronously.\"\"\"\n    await asyncio.sleep(1)  # Simulate I/O\n    return {\"url\": url, \"data\": \"result\"}\n\nasync def main():\n    result = await fetch_data(\"https://api.example.com\")\n    print(result)\n\nasyncio.run(main())\n```\n\n### Pattern 2: Concurrent Execution with gather()\n\n```python\nimport asyncio\nfrom typing import List\n\nasync def fetch_user(user_id: int) -> dict:\n    \"\"\"Fetch user data.\"\"\"\n    await asyncio.sleep(0.5)\n    return {\"id\": user_id, \"name\": f\"User {user_id}\"}\n\nasync def fetch_all_users(user_ids: List[int]) -> List[dict]:\n    \"\"\"Fetch multiple users concurrently.\"\"\"\n    tasks = [fetch_user(uid) for uid in user_ids]\n    results = await asyncio.gather(*tasks)\n    return results\n\nasync def main():\n    user_ids = [1, 2, 3, 4, 5]\n    users = await fetch_all_users(user_ids)\n    print(f\"Fetched {len(users)} users\")\n\nasyncio.run(main())\n```\n\n### Pattern 3: Task Creation and Management\n\n```python\nimport asyncio\n\nasync def background_task(name: str, delay: int):\n    \"\"\"Long-running background task.\"\"\"\n    print(f\"{name} started\")\n    await asyncio.sleep(delay)\n    print(f\"{name} completed\")\n    return f\"Result from {name}\"\n\nasync def main():\n    # Create tasks\n    task1 = asyncio.create_task(background_task(\"Task 1\", 2))\n    task2 = asyncio.create_task(background_task(\"Task 2\", 1))\n\n    # Do other work\n    print(\"Main: doing other work\")\n    await asyncio.sleep(0.5)\n\n    # Wait for tasks\n    result1 = await task1\n    result2 = await task2\n\n    print(f\"Results: {result1}, {result2}\")\n\nasyncio.run(main())\n```\n\n### Pattern 4: Error Handling in Async Code\n\n```python\nimport asyncio\nfrom typing import List, Optional\n\nasync def risky_operation(item_id: int) -> dict:\n    \"\"\"Operation that might fail.\"\"\"\n    await asyncio.sleep(0.1)\n    if item_id % 3 == 0:\n        raise ValueError(f\"Item {item_id} failed\")\n    return {\"id\": item_id, \"status\": \"success\"}\n\nasync def safe_operation(item_id: int) -> Optional[dict]:\n    \"\"\"Wrapper with error handling.\"\"\"\n    try:\n        return await risky_operation(item_id)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        return None\n\nasync def process_items(item_ids: List[int]):\n    \"\"\"Process multiple items with error handling.\"\"\"\n    tasks = [safe_operation(iid) for iid in item_ids]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Filter out failures\n    successful = [r for r in results if r is not None and not isinstance(r, Exception)]\n    failed = [r for r in results if isinstance(r, Exception)]\n\n    print(f\"Success: {len(successful)}, Failed: {len(failed)}\")\n    return successful\n\nasyncio.run(process_items([1, 2, 3, 4, 5, 6]))\n```\n\n### Pattern 5: Timeout Handling\n\n```python\nimport asyncio\n\nasync def slow_operation(delay: int) -> str:\n    \"\"\"Operation that takes time.\"\"\"\n    await asyncio.sleep(delay)\n    return f\"Completed after {delay}s\"\n\nasync def with_timeout():\n    \"\"\"Execute operation with timeout.\"\"\"\n    try:\n        result = await asyncio.wait_for(slow_operation(5), timeout=2.0)\n        print(result)\n    except asyncio.TimeoutError:\n        print(\"Operation timed out\")\n\nasyncio.run(with_timeout())\n```\n\n## Advanced Patterns\n\n### Pattern 6: Async Context Managers\n\n```python\nimport asyncio\nfrom typing import Optional\n\nclass AsyncDatabaseConnection:\n    \"\"\"Async database connection context manager.\"\"\"\n\n    def __init__(self, dsn: str):\n        self.dsn = dsn\n        self.connection: Optional[object] = None\n\n    async def __aenter__(self):\n        print(\"Opening connection\")\n        await asyncio.sleep(0.1)  # Simulate connection\n        self.connection = {\"dsn\": self.dsn, \"connected\": True}\n        return self.connection\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        print(\"Closing connection\")\n        await asyncio.sleep(0.1)  # Simulate cleanup\n        self.connection = None\n\nasync def query_database():\n    \"\"\"Use async context manager.\"\"\"\n    async with AsyncDatabaseConnection(\"postgresql://localhost\") as conn:\n        print(f\"Using connection: {conn}\")\n        await asyncio.sleep(0.2)  # Simulate query\n        return {\"rows\": 10}\n\nasyncio.run(query_database())\n```\n\n### Pattern 7: Async Iterators and Generators\n\n```python\nimport asyncio\nfrom typing import AsyncIterator\n\nasync def async_range(start: int, end: int, delay: float = 0.1) -> AsyncIterator[int]:\n    \"\"\"Async generator that yields numbers with delay.\"\"\"\n    for i in range(start, end):\n        await asyncio.sleep(delay)\n        yield i\n\nasync def fetch_pages(url: str, max_pages: int) -> AsyncIterator[dict]:\n    \"\"\"Fetch paginated data asynchronously.\"\"\"\n    for page in range(1, max_pages + 1):\n        await asyncio.sleep(0.2)  # Simulate API call\n        yield {\n            \"page\": page,\n            \"url\": f\"{url}?page={page}\",\n            \"data\": [f\"item_{page}_{i}\" for i in range(5)]\n        }\n\nasync def consume_async_iterator():\n    \"\"\"Consume async iterator.\"\"\"\n    async for number in async_range(1, 5):\n        print(f\"Number: {number}\")\n\n    print(\"\\nFetching pages:\")\n    async for page_data in fetch_pages(\"https://api.example.com/items\", 3):\n        print(f\"Page {page_data['page']}: {len(page_data['data'])} items\")\n\nasyncio.run(consume_async_iterator())\n```\n\n### Pattern 8: Producer-Consumer Pattern\n\n```python\nimport asyncio\nfrom asyncio import Queue\nfrom typing import Optional\n\nasync def producer(queue: Queue, producer_id: int, num_items: int):\n    \"\"\"Produce items and put them in queue.\"\"\"\n    for i in range(num_items):\n        item = f\"Item-{producer_id}-{i}\"\n        await queue.put(item)\n        print(f\"Producer {producer_id} produced: {item}\")\n        await asyncio.sleep(0.1)\n    await queue.put(None)  # Signal completion\n\nasync def consumer(queue: Queue, consumer_id: int):\n    \"\"\"Consume items from queue.\"\"\"\n    while True:\n        item = await queue.get()\n        if item is None:\n            queue.task_done()\n            break\n\n        print(f\"Consumer {consumer_id} processing: {item}\")\n        await asyncio.sleep(0.2)  # Simulate work\n        queue.task_done()\n\nasync def producer_consumer_example():\n    \"\"\"Run producer-consumer pattern.\"\"\"\n    queue = Queue(maxsize=10)\n\n    # Create tasks\n    producers = [\n        asyncio.create_task(producer(queue, i, 5))\n        for i in range(2)\n    ]\n\n    consumers = [\n        asyncio.create_task(consumer(queue, i))\n        for i in range(3)\n    ]\n\n    # Wait for producers\n    await asyncio.gather(*producers)\n\n    # Wait for queue to be empty\n    await queue.join()\n\n    # Cancel consumers\n    for c in consumers:\n        c.cancel()\n\nasyncio.run(producer_consumer_example())\n```\n\n### Pattern 9: Semaphore for Rate Limiting\n\n```python\nimport asyncio\nfrom typing import List\n\nasync def api_call(url: str, semaphore: asyncio.Semaphore) -> dict:\n    \"\"\"Make API call with rate limiting.\"\"\"\n    async with semaphore:\n        print(f\"Calling {url}\")\n        await asyncio.sleep(0.5)  # Simulate API call\n        return {\"url\": url, \"status\": 200}\n\nasync def rate_limited_requests(urls: List[str], max_concurrent: int = 5):\n    \"\"\"Make multiple requests with rate limiting.\"\"\"\n    semaphore = asyncio.Semaphore(max_concurrent)\n    tasks = [api_call(url, semaphore) for url in urls]\n    results = await asyncio.gather(*tasks)\n    return results\n\nasync def main():\n    urls = [f\"https://api.example.com/item/{i}\" for i in range(20)]\n    results = await rate_limited_requests(urls, max_concurrent=3)\n    print(f\"Completed {len(results)} requests\")\n\nasyncio.run(main())\n```\n\n### Pattern 10: Async Locks and Synchronization\n\n```python\nimport asyncio\n\nclass AsyncCounter:\n    \"\"\"Thread-safe async counter.\"\"\"\n\n    def __init__(self):\n        self.value = 0\n        self.lock = asyncio.Lock()\n\n    async def increment(self):\n        \"\"\"Safely increment counter.\"\"\"\n        async with self.lock:\n            current = self.value\n            await asyncio.sleep(0.01)  # Simulate work\n            self.value = current + 1\n\n    async def get_value(self) -> int:\n        \"\"\"Get current value.\"\"\"\n        async with self.lock:\n            return self.value\n\nasync def worker(counter: AsyncCounter, worker_id: int):\n    \"\"\"Worker that increments counter.\"\"\"\n    for _ in range(10):\n        await counter.increment()\n        print(f\"Worker {worker_id} incremented\")\n\nasync def test_counter():\n    \"\"\"Test concurrent counter.\"\"\"\n    counter = AsyncCounter()\n\n    workers = [asyncio.create_task(worker(counter, i)) for i in range(5)]\n    await asyncio.gather(*workers)\n\n    final_value = await counter.get_value()\n    print(f\"Final counter value: {final_value}\")\n\nasyncio.run(test_counter())\n```\n\n## Real-World Applications\n\n### Web Scraping with aiohttp\n\n```python\nimport asyncio\nimport aiohttp\nfrom typing import List, Dict\n\nasync def fetch_url(session: aiohttp.ClientSession, url: str) -> Dict:\n    \"\"\"Fetch single URL.\"\"\"\n    try:\n        async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:\n            text = await response.text()\n            return {\n                \"url\": url,\n                \"status\": response.status,\n                \"length\": len(text)\n            }\n    except Exception as e:\n        return {\"url\": url, \"error\": str(e)}\n\nasync def scrape_urls(urls: List[str]) -> List[Dict]:\n    \"\"\"Scrape multiple URLs concurrently.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_url(session, url) for url in urls]\n        results = await asyncio.gather(*tasks)\n        return results\n\nasync def main():\n    urls = [\n        \"https://httpbin.org/delay/1\",\n        \"https://httpbin.org/delay/2\",\n        \"https://httpbin.org/status/404\",\n    ]\n\n    results = await scrape_urls(urls)\n    for result in results:\n        print(result)\n\nasyncio.run(main())\n```\n\n### Async Database Operations\n\n```python\nimport asyncio\nfrom typing import List, Optional\n\n# Simulated async database client\nclass AsyncDB:\n    \"\"\"Simulated async database.\"\"\"\n\n    async def execute(self, query: str) -> List[dict]:\n        \"\"\"Execute query.\"\"\"\n        await asyncio.sleep(0.1)\n        return [{\"id\": 1, \"name\": \"Example\"}]\n\n    async def fetch_one(self, query: str) -> Optional[dict]:\n        \"\"\"Fetch single row.\"\"\"\n        await asyncio.sleep(0.1)\n        return {\"id\": 1, \"name\": \"Example\"}\n\nasync def get_user_data(db: AsyncDB, user_id: int) -> dict:\n    \"\"\"Fetch user and related data concurrently.\"\"\"\n    user_task = db.fetch_one(f\"SELECT * FROM users WHERE id = {user_id}\")\n    orders_task = db.execute(f\"SELECT * FROM orders WHERE user_id = {user_id}\")\n    profile_task = db.fetch_one(f\"SELECT * FROM profiles WHERE user_id = {user_id}\")\n\n    user, orders, profile = await asyncio.gather(user_task, orders_task, profile_task)\n\n    return {\n        \"user\": user,\n        \"orders\": orders,\n        \"profile\": profile\n    }\n\nasync def main():\n    db = AsyncDB()\n    user_data = await get_user_data(db, 1)\n    print(user_data)\n\nasyncio.run(main())\n```\n\n### WebSocket Server\n\n```python\nimport asyncio\nfrom typing import Set\n\n# Simulated WebSocket connection\nclass WebSocket:\n    \"\"\"Simulated WebSocket.\"\"\"\n\n    def __init__(self, client_id: str):\n        self.client_id = client_id\n\n    async def send(self, message: str):\n        \"\"\"Send message.\"\"\"\n        print(f\"Sending to {self.client_id}: {message}\")\n        await asyncio.sleep(0.01)\n\n    async def recv(self) -> str:\n        \"\"\"Receive message.\"\"\"\n        await asyncio.sleep(1)\n        return f\"Message from {self.client_id}\"\n\nclass WebSocketServer:\n    \"\"\"Simple WebSocket server.\"\"\"\n\n    def __init__(self):\n        self.clients: Set[WebSocket] = set()\n\n    async def register(self, websocket: WebSocket):\n        \"\"\"Register new client.\"\"\"\n        self.clients.add(websocket)\n        print(f\"Client {websocket.client_id} connected\")\n\n    async def unregister(self, websocket: WebSocket):\n        \"\"\"Unregister client.\"\"\"\n        self.clients.remove(websocket)\n        print(f\"Client {websocket.client_id} disconnected\")\n\n    async def broadcast(self, message: str):\n        \"\"\"Broadcast message to all clients.\"\"\"\n        if self.clients:\n            tasks = [client.send(message) for client in self.clients]\n            await asyncio.gather(*tasks)\n\n    async def handle_client(self, websocket: WebSocket):\n        \"\"\"Handle individual client connection.\"\"\"\n        await self.register(websocket)\n        try:\n            async for message in self.message_iterator(websocket):\n                await self.broadcast(f\"{websocket.client_id}: {message}\")\n        finally:\n            await self.unregister(websocket)\n\n    async def message_iterator(self, websocket: WebSocket):\n        \"\"\"Iterate over messages from client.\"\"\"\n        for _ in range(3):  # Simulate 3 messages\n            yield await websocket.recv()\n```\n\n## Performance Best Practices\n\n### 1. Use Connection Pools\n\n```python\nimport asyncio\nimport aiohttp\n\nasync def with_connection_pool():\n    \"\"\"Use connection pool for efficiency.\"\"\"\n    connector = aiohttp.TCPConnector(limit=100, limit_per_host=10)\n\n    async with aiohttp.ClientSession(connector=connector) as session:\n        tasks = [session.get(f\"https://api.example.com/item/{i}\") for i in range(50)]\n        responses = await asyncio.gather(*tasks)\n        return responses\n```\n\n### 2. Batch Operations\n\n```python\nasync def batch_process(items: List[str], batch_size: int = 10):\n    \"\"\"Process items in batches.\"\"\"\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        tasks = [process_item(item) for item in batch]\n        await asyncio.gather(*tasks)\n        print(f\"Processed batch {i // batch_size + 1}\")\n\nasync def process_item(item: str):\n    \"\"\"Process single item.\"\"\"\n    await asyncio.sleep(0.1)\n    return f\"Processed: {item}\"\n```\n\n### 3. Avoid Blocking Operations\n\n```python\nimport asyncio\nimport concurrent.futures\nfrom typing import Any\n\ndef blocking_operation(data: Any) -> Any:\n    \"\"\"CPU-intensive blocking operation.\"\"\"\n    import time\n    time.sleep(1)\n    return data * 2\n\nasync def run_in_executor(data: Any) -> Any:\n    \"\"\"Run blocking operation in thread pool.\"\"\"\n    loop = asyncio.get_event_loop()\n    with concurrent.futures.ThreadPoolExecutor() as pool:\n        result = await loop.run_in_executor(pool, blocking_operation, data)\n        return result\n\nasync def main():\n    results = await asyncio.gather(*[run_in_executor(i) for i in range(5)])\n    print(results)\n\nasyncio.run(main())\n```\n\n## Common Pitfalls\n\n### 1. Forgetting await\n\n```python\n# Wrong - returns coroutine object, doesn't execute\nresult = async_function()\n\n# Correct\nresult = await async_function()\n```\n\n### 2. Blocking the Event Loop\n\n```python\n# Wrong - blocks event loop\nimport time\nasync def bad():\n    time.sleep(1)  # Blocks!\n\n# Correct\nasync def good():\n    await asyncio.sleep(1)  # Non-blocking\n```\n\n### 3. Not Handling Cancellation\n\n```python\nasync def cancelable_task():\n    \"\"\"Task that handles cancellation.\"\"\"\n    try:\n        while True:\n            await asyncio.sleep(1)\n            print(\"Working...\")\n    except asyncio.CancelledError:\n        print(\"Task cancelled, cleaning up...\")\n        # Perform cleanup\n        raise  # Re-raise to propagate cancellation\n```\n\n### 4. Mixing Sync and Async Code\n\n```python\n# Wrong - can't call async from sync directly\ndef sync_function():\n    result = await async_function()  # SyntaxError!\n\n# Correct\ndef sync_function():\n    result = asyncio.run(async_function())\n```\n\n## Testing Async Code\n\n```python\nimport asyncio\nimport pytest\n\n# Using pytest-asyncio\n@pytest.mark.asyncio\nasync def test_async_function():\n    \"\"\"Test async function.\"\"\"\n    result = await fetch_data(\"https://api.example.com\")\n    assert result is not None\n\n@pytest.mark.asyncio\nasync def test_with_timeout():\n    \"\"\"Test with timeout.\"\"\"\n    with pytest.raises(asyncio.TimeoutError):\n        await asyncio.wait_for(slow_operation(5), timeout=1.0)\n```\n\n## Resources\n\n- **Python asyncio documentation**: https://docs.python.org/3/library/asyncio.html\n- **aiohttp**: Async HTTP client/server\n- **FastAPI**: Modern async web framework\n- **asyncpg**: Async PostgreSQL driver\n- **motor**: Async MongoDB driver\n\n## Best Practices Summary\n\n1. **Use asyncio.run()** for entry point (Python 3.7+)\n2. **Always await coroutines** to execute them\n3. **Use gather() for concurrent execution** of multiple tasks\n4. **Implement proper error handling** with try/except\n5. **Use timeouts** to prevent hanging operations\n6. **Pool connections** for better performance\n7. **Avoid blocking operations** in async code\n8. **Use semaphores** for rate limiting\n9. **Handle task cancellation** properly\n10. **Test async code** with pytest-asyncio\n",
      "references": {},
      "assets": {}
    },
    {
      "name": "python-testing-patterns",
      "description": "Implement comprehensive testing strategies with pytest, fixtures, mocking, and test-driven development. Use when writing Python tests, setting up test suites, or implementing testing best practices.",
      "plugin": "python-development",
      "source_path": "plugins/python-development/skills/python-testing-patterns/SKILL.md",
      "category": "languages",
      "keywords": [
        "python",
        "django",
        "fastapi",
        "async",
        "backend"
      ],
      "content": "---\nname: python-testing-patterns\ndescription: Implement comprehensive testing strategies with pytest, fixtures, mocking, and test-driven development. Use when writing Python tests, setting up test suites, or implementing testing best practices.\n---\n\n# Python Testing Patterns\n\nComprehensive guide to implementing robust testing strategies in Python using pytest, fixtures, mocking, parameterization, and test-driven development practices.\n\n## When to Use This Skill\n\n- Writing unit tests for Python code\n- Setting up test suites and test infrastructure\n- Implementing test-driven development (TDD)\n- Creating integration tests for APIs and services\n- Mocking external dependencies and services\n- Testing async code and concurrent operations\n- Setting up continuous testing in CI/CD\n- Implementing property-based testing\n- Testing database operations\n- Debugging failing tests\n\n## Core Concepts\n\n### 1. Test Types\n- **Unit Tests**: Test individual functions/classes in isolation\n- **Integration Tests**: Test interaction between components\n- **Functional Tests**: Test complete features end-to-end\n- **Performance Tests**: Measure speed and resource usage\n\n### 2. Test Structure (AAA Pattern)\n- **Arrange**: Set up test data and preconditions\n- **Act**: Execute the code under test\n- **Assert**: Verify the results\n\n### 3. Test Coverage\n- Measure what code is exercised by tests\n- Identify untested code paths\n- Aim for meaningful coverage, not just high percentages\n\n### 4. Test Isolation\n- Tests should be independent\n- No shared state between tests\n- Each test should clean up after itself\n\n## Quick Start\n\n```python\n# test_example.py\ndef add(a, b):\n    return a + b\n\ndef test_add():\n    \"\"\"Basic test example.\"\"\"\n    result = add(2, 3)\n    assert result == 5\n\ndef test_add_negative():\n    \"\"\"Test with negative numbers.\"\"\"\n    assert add(-1, 1) == 0\n\n# Run with: pytest test_example.py\n```\n\n## Fundamental Patterns\n\n### Pattern 1: Basic pytest Tests\n\n```python\n# test_calculator.py\nimport pytest\n\nclass Calculator:\n    \"\"\"Simple calculator for testing.\"\"\"\n\n    def add(self, a: float, b: float) -> float:\n        return a + b\n\n    def subtract(self, a: float, b: float) -> float:\n        return a - b\n\n    def multiply(self, a: float, b: float) -> float:\n        return a * b\n\n    def divide(self, a: float, b: float) -> float:\n        if b == 0:\n            raise ValueError(\"Cannot divide by zero\")\n        return a / b\n\n\ndef test_addition():\n    \"\"\"Test addition.\"\"\"\n    calc = Calculator()\n    assert calc.add(2, 3) == 5\n    assert calc.add(-1, 1) == 0\n    assert calc.add(0, 0) == 0\n\n\ndef test_subtraction():\n    \"\"\"Test subtraction.\"\"\"\n    calc = Calculator()\n    assert calc.subtract(5, 3) == 2\n    assert calc.subtract(0, 5) == -5\n\n\ndef test_multiplication():\n    \"\"\"Test multiplication.\"\"\"\n    calc = Calculator()\n    assert calc.multiply(3, 4) == 12\n    assert calc.multiply(0, 5) == 0\n\n\ndef test_division():\n    \"\"\"Test division.\"\"\"\n    calc = Calculator()\n    assert calc.divide(6, 3) == 2\n    assert calc.divide(5, 2) == 2.5\n\n\ndef test_division_by_zero():\n    \"\"\"Test division by zero raises error.\"\"\"\n    calc = Calculator()\n    with pytest.raises(ValueError, match=\"Cannot divide by zero\"):\n        calc.divide(5, 0)\n```\n\n### Pattern 2: Fixtures for Setup and Teardown\n\n```python\n# test_database.py\nimport pytest\nfrom typing import Generator\n\nclass Database:\n    \"\"\"Simple database class.\"\"\"\n\n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n        self.connected = False\n\n    def connect(self):\n        \"\"\"Connect to database.\"\"\"\n        self.connected = True\n\n    def disconnect(self):\n        \"\"\"Disconnect from database.\"\"\"\n        self.connected = False\n\n    def query(self, sql: str) -> list:\n        \"\"\"Execute query.\"\"\"\n        if not self.connected:\n            raise RuntimeError(\"Not connected\")\n        return [{\"id\": 1, \"name\": \"Test\"}]\n\n\n@pytest.fixture\ndef db() -> Generator[Database, None, None]:\n    \"\"\"Fixture that provides connected database.\"\"\"\n    # Setup\n    database = Database(\"sqlite:///:memory:\")\n    database.connect()\n\n    # Provide to test\n    yield database\n\n    # Teardown\n    database.disconnect()\n\n\ndef test_database_query(db):\n    \"\"\"Test database query with fixture.\"\"\"\n    results = db.query(\"SELECT * FROM users\")\n    assert len(results) == 1\n    assert results[0][\"name\"] == \"Test\"\n\n\n@pytest.fixture(scope=\"session\")\ndef app_config():\n    \"\"\"Session-scoped fixture - created once per test session.\"\"\"\n    return {\n        \"database_url\": \"postgresql://localhost/test\",\n        \"api_key\": \"test-key\",\n        \"debug\": True\n    }\n\n\n@pytest.fixture(scope=\"module\")\ndef api_client(app_config):\n    \"\"\"Module-scoped fixture - created once per test module.\"\"\"\n    # Setup expensive resource\n    client = {\"config\": app_config, \"session\": \"active\"}\n    yield client\n    # Cleanup\n    client[\"session\"] = \"closed\"\n\n\ndef test_api_client(api_client):\n    \"\"\"Test using api client fixture.\"\"\"\n    assert api_client[\"session\"] == \"active\"\n    assert api_client[\"config\"][\"debug\"] is True\n```\n\n### Pattern 3: Parameterized Tests\n\n```python\n# test_validation.py\nimport pytest\n\ndef is_valid_email(email: str) -> bool:\n    \"\"\"Check if email is valid.\"\"\"\n    return \"@\" in email and \".\" in email.split(\"@\")[1]\n\n\n@pytest.mark.parametrize(\"email,expected\", [\n    (\"user@example.com\", True),\n    (\"test.user@domain.co.uk\", True),\n    (\"invalid.email\", False),\n    (\"@example.com\", False),\n    (\"user@domain\", False),\n    (\"\", False),\n])\ndef test_email_validation(email, expected):\n    \"\"\"Test email validation with various inputs.\"\"\"\n    assert is_valid_email(email) == expected\n\n\n@pytest.mark.parametrize(\"a,b,expected\", [\n    (2, 3, 5),\n    (0, 0, 0),\n    (-1, 1, 0),\n    (100, 200, 300),\n    (-5, -5, -10),\n])\ndef test_addition_parameterized(a, b, expected):\n    \"\"\"Test addition with multiple parameter sets.\"\"\"\n    from test_calculator import Calculator\n    calc = Calculator()\n    assert calc.add(a, b) == expected\n\n\n# Using pytest.param for special cases\n@pytest.mark.parametrize(\"value,expected\", [\n    pytest.param(1, True, id=\"positive\"),\n    pytest.param(0, False, id=\"zero\"),\n    pytest.param(-1, False, id=\"negative\"),\n])\ndef test_is_positive(value, expected):\n    \"\"\"Test with custom test IDs.\"\"\"\n    assert (value > 0) == expected\n```\n\n### Pattern 4: Mocking with unittest.mock\n\n```python\n# test_api_client.py\nimport pytest\nfrom unittest.mock import Mock, patch, MagicMock\nimport requests\n\nclass APIClient:\n    \"\"\"Simple API client.\"\"\"\n\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n\n    def get_user(self, user_id: int) -> dict:\n        \"\"\"Fetch user from API.\"\"\"\n        response = requests.get(f\"{self.base_url}/users/{user_id}\")\n        response.raise_for_status()\n        return response.json()\n\n    def create_user(self, data: dict) -> dict:\n        \"\"\"Create new user.\"\"\"\n        response = requests.post(f\"{self.base_url}/users\", json=data)\n        response.raise_for_status()\n        return response.json()\n\n\ndef test_get_user_success():\n    \"\"\"Test successful API call with mock.\"\"\"\n    client = APIClient(\"https://api.example.com\")\n\n    mock_response = Mock()\n    mock_response.json.return_value = {\"id\": 1, \"name\": \"John Doe\"}\n    mock_response.raise_for_status.return_value = None\n\n    with patch(\"requests.get\", return_value=mock_response) as mock_get:\n        user = client.get_user(1)\n\n        assert user[\"id\"] == 1\n        assert user[\"name\"] == \"John Doe\"\n        mock_get.assert_called_once_with(\"https://api.example.com/users/1\")\n\n\ndef test_get_user_not_found():\n    \"\"\"Test API call with 404 error.\"\"\"\n    client = APIClient(\"https://api.example.com\")\n\n    mock_response = Mock()\n    mock_response.raise_for_status.side_effect = requests.HTTPError(\"404 Not Found\")\n\n    with patch(\"requests.get\", return_value=mock_response):\n        with pytest.raises(requests.HTTPError):\n            client.get_user(999)\n\n\n@patch(\"requests.post\")\ndef test_create_user(mock_post):\n    \"\"\"Test user creation with decorator syntax.\"\"\"\n    client = APIClient(\"https://api.example.com\")\n\n    mock_post.return_value.json.return_value = {\"id\": 2, \"name\": \"Jane Doe\"}\n    mock_post.return_value.raise_for_status.return_value = None\n\n    user_data = {\"name\": \"Jane Doe\", \"email\": \"jane@example.com\"}\n    result = client.create_user(user_data)\n\n    assert result[\"id\"] == 2\n    mock_post.assert_called_once()\n    call_args = mock_post.call_args\n    assert call_args.kwargs[\"json\"] == user_data\n```\n\n### Pattern 5: Testing Exceptions\n\n```python\n# test_exceptions.py\nimport pytest\n\ndef divide(a: float, b: float) -> float:\n    \"\"\"Divide a by b.\"\"\"\n    if b == 0:\n        raise ZeroDivisionError(\"Division by zero\")\n    if not isinstance(a, (int, float)) or not isinstance(b, (int, float)):\n        raise TypeError(\"Arguments must be numbers\")\n    return a / b\n\n\ndef test_zero_division():\n    \"\"\"Test exception is raised for division by zero.\"\"\"\n    with pytest.raises(ZeroDivisionError):\n        divide(10, 0)\n\n\ndef test_zero_division_with_message():\n    \"\"\"Test exception message.\"\"\"\n    with pytest.raises(ZeroDivisionError, match=\"Division by zero\"):\n        divide(5, 0)\n\n\ndef test_type_error():\n    \"\"\"Test type error exception.\"\"\"\n    with pytest.raises(TypeError, match=\"must be numbers\"):\n        divide(\"10\", 5)\n\n\ndef test_exception_info():\n    \"\"\"Test accessing exception info.\"\"\"\n    with pytest.raises(ValueError) as exc_info:\n        int(\"not a number\")\n\n    assert \"invalid literal\" in str(exc_info.value)\n```\n\n## Advanced Patterns\n\n### Pattern 6: Testing Async Code\n\n```python\n# test_async.py\nimport pytest\nimport asyncio\n\nasync def fetch_data(url: str) -> dict:\n    \"\"\"Fetch data asynchronously.\"\"\"\n    await asyncio.sleep(0.1)\n    return {\"url\": url, \"data\": \"result\"}\n\n\n@pytest.mark.asyncio\nasync def test_fetch_data():\n    \"\"\"Test async function.\"\"\"\n    result = await fetch_data(\"https://api.example.com\")\n    assert result[\"url\"] == \"https://api.example.com\"\n    assert \"data\" in result\n\n\n@pytest.mark.asyncio\nasync def test_concurrent_fetches():\n    \"\"\"Test concurrent async operations.\"\"\"\n    urls = [\"url1\", \"url2\", \"url3\"]\n    tasks = [fetch_data(url) for url in urls]\n    results = await asyncio.gather(*tasks)\n\n    assert len(results) == 3\n    assert all(\"data\" in r for r in results)\n\n\n@pytest.fixture\nasync def async_client():\n    \"\"\"Async fixture.\"\"\"\n    client = {\"connected\": True}\n    yield client\n    client[\"connected\"] = False\n\n\n@pytest.mark.asyncio\nasync def test_with_async_fixture(async_client):\n    \"\"\"Test using async fixture.\"\"\"\n    assert async_client[\"connected\"] is True\n```\n\n### Pattern 7: Monkeypatch for Testing\n\n```python\n# test_environment.py\nimport os\nimport pytest\n\ndef get_database_url() -> str:\n    \"\"\"Get database URL from environment.\"\"\"\n    return os.environ.get(\"DATABASE_URL\", \"sqlite:///:memory:\")\n\n\ndef test_database_url_default():\n    \"\"\"Test default database URL.\"\"\"\n    # Will use actual environment variable if set\n    url = get_database_url()\n    assert url\n\n\ndef test_database_url_custom(monkeypatch):\n    \"\"\"Test custom database URL with monkeypatch.\"\"\"\n    monkeypatch.setenv(\"DATABASE_URL\", \"postgresql://localhost/test\")\n    assert get_database_url() == \"postgresql://localhost/test\"\n\n\ndef test_database_url_not_set(monkeypatch):\n    \"\"\"Test when env var is not set.\"\"\"\n    monkeypatch.delenv(\"DATABASE_URL\", raising=False)\n    assert get_database_url() == \"sqlite:///:memory:\"\n\n\nclass Config:\n    \"\"\"Configuration class.\"\"\"\n\n    def __init__(self):\n        self.api_key = \"production-key\"\n\n    def get_api_key(self):\n        return self.api_key\n\n\ndef test_monkeypatch_attribute(monkeypatch):\n    \"\"\"Test monkeypatching object attributes.\"\"\"\n    config = Config()\n    monkeypatch.setattr(config, \"api_key\", \"test-key\")\n    assert config.get_api_key() == \"test-key\"\n```\n\n### Pattern 8: Temporary Files and Directories\n\n```python\n# test_file_operations.py\nimport pytest\nfrom pathlib import Path\n\ndef save_data(filepath: Path, data: str):\n    \"\"\"Save data to file.\"\"\"\n    filepath.write_text(data)\n\n\ndef load_data(filepath: Path) -> str:\n    \"\"\"Load data from file.\"\"\"\n    return filepath.read_text()\n\n\ndef test_file_operations(tmp_path):\n    \"\"\"Test file operations with temporary directory.\"\"\"\n    # tmp_path is a pathlib.Path object\n    test_file = tmp_path / \"test_data.txt\"\n\n    # Save data\n    save_data(test_file, \"Hello, World!\")\n\n    # Verify file exists\n    assert test_file.exists()\n\n    # Load and verify data\n    data = load_data(test_file)\n    assert data == \"Hello, World!\"\n\n\ndef test_multiple_files(tmp_path):\n    \"\"\"Test with multiple temporary files.\"\"\"\n    files = {\n        \"file1.txt\": \"Content 1\",\n        \"file2.txt\": \"Content 2\",\n        \"file3.txt\": \"Content 3\"\n    }\n\n    for filename, content in files.items():\n        filepath = tmp_path / filename\n        save_data(filepath, content)\n\n    # Verify all files created\n    assert len(list(tmp_path.iterdir())) == 3\n\n    # Verify contents\n    for filename, expected_content in files.items():\n        filepath = tmp_path / filename\n        assert load_data(filepath) == expected_content\n```\n\n### Pattern 9: Custom Fixtures and Conftest\n\n```python\n# conftest.py\n\"\"\"Shared fixtures for all tests.\"\"\"\nimport pytest\n\n@pytest.fixture(scope=\"session\")\ndef database_url():\n    \"\"\"Provide database URL for all tests.\"\"\"\n    return \"postgresql://localhost/test_db\"\n\n\n@pytest.fixture(autouse=True)\ndef reset_database(database_url):\n    \"\"\"Auto-use fixture that runs before each test.\"\"\"\n    # Setup: Clear database\n    print(f\"Clearing database: {database_url}\")\n    yield\n    # Teardown: Clean up\n    print(\"Test completed\")\n\n\n@pytest.fixture\ndef sample_user():\n    \"\"\"Provide sample user data.\"\"\"\n    return {\n        \"id\": 1,\n        \"name\": \"Test User\",\n        \"email\": \"test@example.com\"\n    }\n\n\n@pytest.fixture\ndef sample_users():\n    \"\"\"Provide list of sample users.\"\"\"\n    return [\n        {\"id\": 1, \"name\": \"User 1\"},\n        {\"id\": 2, \"name\": \"User 2\"},\n        {\"id\": 3, \"name\": \"User 3\"},\n    ]\n\n\n# Parametrized fixture\n@pytest.fixture(params=[\"sqlite\", \"postgresql\", \"mysql\"])\ndef db_backend(request):\n    \"\"\"Fixture that runs tests with different database backends.\"\"\"\n    return request.param\n\n\ndef test_with_db_backend(db_backend):\n    \"\"\"This test will run 3 times with different backends.\"\"\"\n    print(f\"Testing with {db_backend}\")\n    assert db_backend in [\"sqlite\", \"postgresql\", \"mysql\"]\n```\n\n### Pattern 10: Property-Based Testing\n\n```python\n# test_properties.py\nfrom hypothesis import given, strategies as st\nimport pytest\n\ndef reverse_string(s: str) -> str:\n    \"\"\"Reverse a string.\"\"\"\n    return s[::-1]\n\n\n@given(st.text())\ndef test_reverse_twice_is_original(s):\n    \"\"\"Property: reversing twice returns original.\"\"\"\n    assert reverse_string(reverse_string(s)) == s\n\n\n@given(st.text())\ndef test_reverse_length(s):\n    \"\"\"Property: reversed string has same length.\"\"\"\n    assert len(reverse_string(s)) == len(s)\n\n\n@given(st.integers(), st.integers())\ndef test_addition_commutative(a, b):\n    \"\"\"Property: addition is commutative.\"\"\"\n    assert a + b == b + a\n\n\n@given(st.lists(st.integers()))\ndef test_sorted_list_properties(lst):\n    \"\"\"Property: sorted list is ordered.\"\"\"\n    sorted_lst = sorted(lst)\n\n    # Same length\n    assert len(sorted_lst) == len(lst)\n\n    # All elements present\n    assert set(sorted_lst) == set(lst)\n\n    # Is ordered\n    for i in range(len(sorted_lst) - 1):\n        assert sorted_lst[i] <= sorted_lst[i + 1]\n```\n\n## Testing Best Practices\n\n### Test Organization\n\n```python\n# tests/\n#   __init__.py\n#   conftest.py           # Shared fixtures\n#   test_unit/            # Unit tests\n#     test_models.py\n#     test_utils.py\n#   test_integration/     # Integration tests\n#     test_api.py\n#     test_database.py\n#   test_e2e/            # End-to-end tests\n#     test_workflows.py\n```\n\n### Test Naming\n\n```python\n# Good test names\ndef test_user_creation_with_valid_data():\n    \"\"\"Clear name describes what is being tested.\"\"\"\n    pass\n\n\ndef test_login_fails_with_invalid_password():\n    \"\"\"Name describes expected behavior.\"\"\"\n    pass\n\n\ndef test_api_returns_404_for_missing_resource():\n    \"\"\"Specific about inputs and expected outcomes.\"\"\"\n    pass\n\n\n# Bad test names\ndef test_1():  # Not descriptive\n    pass\n\n\ndef test_user():  # Too vague\n    pass\n\n\ndef test_function():  # Doesn't explain what's tested\n    pass\n```\n\n### Test Markers\n\n```python\n# test_markers.py\nimport pytest\n\n@pytest.mark.slow\ndef test_slow_operation():\n    \"\"\"Mark slow tests.\"\"\"\n    import time\n    time.sleep(2)\n\n\n@pytest.mark.integration\ndef test_database_integration():\n    \"\"\"Mark integration tests.\"\"\"\n    pass\n\n\n@pytest.mark.skip(reason=\"Feature not implemented yet\")\ndef test_future_feature():\n    \"\"\"Skip tests temporarily.\"\"\"\n    pass\n\n\n@pytest.mark.skipif(os.name == \"nt\", reason=\"Unix only test\")\ndef test_unix_specific():\n    \"\"\"Conditional skip.\"\"\"\n    pass\n\n\n@pytest.mark.xfail(reason=\"Known bug #123\")\ndef test_known_bug():\n    \"\"\"Mark expected failures.\"\"\"\n    assert False\n\n\n# Run with:\n# pytest -m slow          # Run only slow tests\n# pytest -m \"not slow\"    # Skip slow tests\n# pytest -m integration   # Run integration tests\n```\n\n### Coverage Reporting\n\n```bash\n# Install coverage\npip install pytest-cov\n\n# Run tests with coverage\npytest --cov=myapp tests/\n\n# Generate HTML report\npytest --cov=myapp --cov-report=html tests/\n\n# Fail if coverage below threshold\npytest --cov=myapp --cov-fail-under=80 tests/\n\n# Show missing lines\npytest --cov=myapp --cov-report=term-missing tests/\n```\n\n## Testing Database Code\n\n```python\n# test_database_models.py\nimport pytest\nfrom sqlalchemy import create_engine, Column, Integer, String\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker, Session\n\nBase = declarative_base()\n\n\nclass User(Base):\n    \"\"\"User model.\"\"\"\n    __tablename__ = \"users\"\n\n    id = Column(Integer, primary_key=True)\n    name = Column(String(50))\n    email = Column(String(100), unique=True)\n\n\n@pytest.fixture(scope=\"function\")\ndef db_session() -> Session:\n    \"\"\"Create in-memory database for testing.\"\"\"\n    engine = create_engine(\"sqlite:///:memory:\")\n    Base.metadata.create_all(engine)\n\n    SessionLocal = sessionmaker(bind=engine)\n    session = SessionLocal()\n\n    yield session\n\n    session.close()\n\n\ndef test_create_user(db_session):\n    \"\"\"Test creating a user.\"\"\"\n    user = User(name=\"Test User\", email=\"test@example.com\")\n    db_session.add(user)\n    db_session.commit()\n\n    assert user.id is not None\n    assert user.name == \"Test User\"\n\n\ndef test_query_user(db_session):\n    \"\"\"Test querying users.\"\"\"\n    user1 = User(name=\"User 1\", email=\"user1@example.com\")\n    user2 = User(name=\"User 2\", email=\"user2@example.com\")\n\n    db_session.add_all([user1, user2])\n    db_session.commit()\n\n    users = db_session.query(User).all()\n    assert len(users) == 2\n\n\ndef test_unique_email_constraint(db_session):\n    \"\"\"Test unique email constraint.\"\"\"\n    from sqlalchemy.exc import IntegrityError\n\n    user1 = User(name=\"User 1\", email=\"same@example.com\")\n    user2 = User(name=\"User 2\", email=\"same@example.com\")\n\n    db_session.add(user1)\n    db_session.commit()\n\n    db_session.add(user2)\n\n    with pytest.raises(IntegrityError):\n        db_session.commit()\n```\n\n## CI/CD Integration\n\n```yaml\n# .github/workflows/test.yml\nname: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        python-version: [\"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install dependencies\n        run: |\n          pip install -e \".[dev]\"\n          pip install pytest pytest-cov\n\n      - name: Run tests\n        run: |\n          pytest --cov=myapp --cov-report=xml\n\n      - name: Upload coverage\n        uses: codecov/codecov-action@v3\n        with:\n          file: ./coverage.xml\n```\n\n## Configuration Files\n\n```ini\n# pytest.ini\n[pytest]\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\naddopts =\n    -v\n    --strict-markers\n    --tb=short\n    --cov=myapp\n    --cov-report=term-missing\nmarkers =\n    slow: marks tests as slow\n    integration: marks integration tests\n    unit: marks unit tests\n    e2e: marks end-to-end tests\n```\n\n```toml\n# pyproject.toml\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\naddopts = [\n    \"-v\",\n    \"--cov=myapp\",\n    \"--cov-report=term-missing\",\n]\n\n[tool.coverage.run]\nsource = [\"myapp\"]\nomit = [\"*/tests/*\", \"*/migrations/*\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n]\n```\n\n## Resources\n\n- **pytest documentation**: https://docs.pytest.org/\n- **unittest.mock**: https://docs.python.org/3/library/unittest.mock.html\n- **hypothesis**: Property-based testing\n- **pytest-asyncio**: Testing async code\n- **pytest-cov**: Coverage reporting\n- **pytest-mock**: pytest wrapper for mock\n\n## Best Practices Summary\n\n1. **Write tests first** (TDD) or alongside code\n2. **One assertion per test** when possible\n3. **Use descriptive test names** that explain behavior\n4. **Keep tests independent** and isolated\n5. **Use fixtures** for setup and teardown\n6. **Mock external dependencies** appropriately\n7. **Parametrize tests** to reduce duplication\n8. **Test edge cases** and error conditions\n9. **Measure coverage** but focus on quality\n10. **Run tests in CI/CD** on every commit\n",
      "references": {},
      "assets": {}
    },
    {
      "name": "python-packaging",
      "description": "Create distributable Python packages with proper project structure, setup.py/pyproject.toml, and publishing to PyPI. Use when packaging Python libraries, creating CLI tools, or distributing Python code.",
      "plugin": "python-development",
      "source_path": "plugins/python-development/skills/python-packaging/SKILL.md",
      "category": "languages",
      "keywords": [
        "python",
        "django",
        "fastapi",
        "async",
        "backend"
      ],
      "content": "---\nname: python-packaging\ndescription: Create distributable Python packages with proper project structure, setup.py/pyproject.toml, and publishing to PyPI. Use when packaging Python libraries, creating CLI tools, or distributing Python code.\n---\n\n# Python Packaging\n\nComprehensive guide to creating, structuring, and distributing Python packages using modern packaging tools, pyproject.toml, and publishing to PyPI.\n\n## When to Use This Skill\n\n- Creating Python libraries for distribution\n- Building command-line tools with entry points\n- Publishing packages to PyPI or private repositories\n- Setting up Python project structure\n- Creating installable packages with dependencies\n- Building wheels and source distributions\n- Versioning and releasing Python packages\n- Creating namespace packages\n- Implementing package metadata and classifiers\n\n## Core Concepts\n\n### 1. Package Structure\n- **Source layout**: `src/package_name/` (recommended)\n- **Flat layout**: `package_name/` (simpler but less flexible)\n- **Package metadata**: pyproject.toml, setup.py, or setup.cfg\n- **Distribution formats**: wheel (.whl) and source distribution (.tar.gz)\n\n### 2. Modern Packaging Standards\n- **PEP 517/518**: Build system requirements\n- **PEP 621**: Metadata in pyproject.toml\n- **PEP 660**: Editable installs\n- **pyproject.toml**: Single source of configuration\n\n### 3. Build Backends\n- **setuptools**: Traditional, widely used\n- **hatchling**: Modern, opinionated\n- **flit**: Lightweight, for pure Python\n- **poetry**: Dependency management + packaging\n\n### 4. Distribution\n- **PyPI**: Python Package Index (public)\n- **TestPyPI**: Testing before production\n- **Private repositories**: JFrog, AWS CodeArtifact, etc.\n\n## Quick Start\n\n### Minimal Package Structure\n\n```\nmy-package/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 my_package/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 module.py\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 test_module.py\n```\n\n### Minimal pyproject.toml\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-package\"\nversion = \"0.1.0\"\ndescription = \"A short description\"\nauthors = [{name = \"Your Name\", email = \"you@example.com\"}]\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\ndependencies = [\n    \"requests>=2.28.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=7.0\",\n    \"black>=22.0\",\n]\n```\n\n## Package Structure Patterns\n\n### Pattern 1: Source Layout (Recommended)\n\n```\nmy-package/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 my_package/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 core.py\n\u2502       \u251c\u2500\u2500 utils.py\n\u2502       \u2514\u2500\u2500 py.typed          # For type hints\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_core.py\n\u2502   \u2514\u2500\u2500 test_utils.py\n\u2514\u2500\u2500 docs/\n    \u2514\u2500\u2500 index.md\n```\n\n**Advantages:**\n- Prevents accidentally importing from source\n- Cleaner test imports\n- Better isolation\n\n**pyproject.toml for source layout:**\n```toml\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n```\n\n### Pattern 2: Flat Layout\n\n```\nmy-package/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 my_package/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 module.py\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 test_module.py\n```\n\n**Simpler but:**\n- Can import package without installing\n- Less professional for libraries\n\n### Pattern 3: Multi-Package Project\n\n```\nproject/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 packages/\n\u2502   \u251c\u2500\u2500 package-a/\n\u2502   \u2502   \u2514\u2500\u2500 src/\n\u2502   \u2502       \u2514\u2500\u2500 package_a/\n\u2502   \u2514\u2500\u2500 package-b/\n\u2502       \u2514\u2500\u2500 src/\n\u2502           \u2514\u2500\u2500 package_b/\n\u2514\u2500\u2500 tests/\n```\n\n## Complete pyproject.toml Examples\n\n### Pattern 4: Full-Featured pyproject.toml\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-awesome-package\"\nversion = \"1.0.0\"\ndescription = \"An awesome Python package\"\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\nlicense = {text = \"MIT\"}\nauthors = [\n    {name = \"Your Name\", email = \"you@example.com\"},\n]\nmaintainers = [\n    {name = \"Maintainer Name\", email = \"maintainer@example.com\"},\n]\nkeywords = [\"example\", \"package\", \"awesome\"]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n]\n\ndependencies = [\n    \"requests>=2.28.0,<3.0.0\",\n    \"click>=8.0.0\",\n    \"pydantic>=2.0.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=7.0.0\",\n    \"pytest-cov>=4.0.0\",\n    \"black>=23.0.0\",\n    \"ruff>=0.1.0\",\n    \"mypy>=1.0.0\",\n]\ndocs = [\n    \"sphinx>=5.0.0\",\n    \"sphinx-rtd-theme>=1.0.0\",\n]\nall = [\n    \"my-awesome-package[dev,docs]\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/username/my-awesome-package\"\nDocumentation = \"https://my-awesome-package.readthedocs.io\"\nRepository = \"https://github.com/username/my-awesome-package\"\n\"Bug Tracker\" = \"https://github.com/username/my-awesome-package/issues\"\nChangelog = \"https://github.com/username/my-awesome-package/blob/main/CHANGELOG.md\"\n\n[project.scripts]\nmy-cli = \"my_package.cli:main\"\nawesome-tool = \"my_package.tools:run\"\n\n[project.entry-points.\"my_package.plugins\"]\nplugin1 = \"my_package.plugins:plugin1\"\n\n[tool.setuptools]\npackage-dir = {\"\" = \"src\"}\nzip-safe = false\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\ninclude = [\"my_package*\"]\nexclude = [\"tests*\"]\n\n[tool.setuptools.package-data]\nmy_package = [\"py.typed\", \"*.pyi\", \"data/*.json\"]\n\n# Black configuration\n[tool.black]\nline-length = 100\ntarget-version = [\"py38\", \"py39\", \"py310\", \"py311\"]\ninclude = '\\.pyi?$'\n\n# Ruff configuration\n[tool.ruff]\nline-length = 100\ntarget-version = \"py38\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"I\", \"N\", \"W\", \"UP\"]\n\n# MyPy configuration\n[tool.mypy]\npython_version = \"3.8\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\n\n# Pytest configuration\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\naddopts = \"-v --cov=my_package --cov-report=term-missing\"\n\n# Coverage configuration\n[tool.coverage.run]\nsource = [\"src\"]\nomit = [\"*/tests/*\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n]\n```\n\n### Pattern 5: Dynamic Versioning\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\", \"setuptools-scm>=8.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-package\"\ndynamic = [\"version\"]\ndescription = \"Package with dynamic version\"\n\n[tool.setuptools.dynamic]\nversion = {attr = \"my_package.__version__\"}\n\n# Or use setuptools-scm for git-based versioning\n[tool.setuptools_scm]\nwrite_to = \"src/my_package/_version.py\"\n```\n\n**In __init__.py:**\n```python\n# src/my_package/__init__.py\n__version__ = \"1.0.0\"\n\n# Or with setuptools-scm\nfrom importlib.metadata import version\n__version__ = version(\"my-package\")\n```\n\n## Command-Line Interface (CLI) Patterns\n\n### Pattern 6: CLI with Click\n\n```python\n# src/my_package/cli.py\nimport click\n\n@click.group()\n@click.version_option()\ndef cli():\n    \"\"\"My awesome CLI tool.\"\"\"\n    pass\n\n@cli.command()\n@click.argument(\"name\")\n@click.option(\"--greeting\", default=\"Hello\", help=\"Greeting to use\")\ndef greet(name: str, greeting: str):\n    \"\"\"Greet someone.\"\"\"\n    click.echo(f\"{greeting}, {name}!\")\n\n@cli.command()\n@click.option(\"--count\", default=1, help=\"Number of times to repeat\")\ndef repeat(count: int):\n    \"\"\"Repeat a message.\"\"\"\n    for i in range(count):\n        click.echo(f\"Message {i + 1}\")\n\ndef main():\n    \"\"\"Entry point for CLI.\"\"\"\n    cli()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**Register in pyproject.toml:**\n```toml\n[project.scripts]\nmy-tool = \"my_package.cli:main\"\n```\n\n**Usage:**\n```bash\npip install -e .\nmy-tool greet World\nmy-tool greet Alice --greeting=\"Hi\"\nmy-tool repeat --count=3\n```\n\n### Pattern 7: CLI with argparse\n\n```python\n# src/my_package/cli.py\nimport argparse\nimport sys\n\ndef main():\n    \"\"\"Main CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"My awesome tool\",\n        prog=\"my-tool\"\n    )\n\n    parser.add_argument(\n        \"--version\",\n        action=\"version\",\n        version=\"%(prog)s 1.0.0\"\n    )\n\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Commands\")\n\n    # Add subcommand\n    process_parser = subparsers.add_parser(\"process\", help=\"Process data\")\n    process_parser.add_argument(\"input_file\", help=\"Input file path\")\n    process_parser.add_argument(\n        \"--output\", \"-o\",\n        default=\"output.txt\",\n        help=\"Output file path\"\n    )\n\n    args = parser.parse_args()\n\n    if args.command == \"process\":\n        process_data(args.input_file, args.output)\n    else:\n        parser.print_help()\n        sys.exit(1)\n\ndef process_data(input_file: str, output_file: str):\n    \"\"\"Process data from input to output.\"\"\"\n    print(f\"Processing {input_file} -> {output_file}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Building and Publishing\n\n### Pattern 8: Build Package Locally\n\n```bash\n# Install build tools\npip install build twine\n\n# Build distribution\npython -m build\n\n# This creates:\n# dist/\n#   my-package-1.0.0.tar.gz (source distribution)\n#   my_package-1.0.0-py3-none-any.whl (wheel)\n\n# Check the distribution\ntwine check dist/*\n```\n\n### Pattern 9: Publishing to PyPI\n\n```bash\n# Install publishing tools\npip install twine\n\n# Test on TestPyPI first\ntwine upload --repository testpypi dist/*\n\n# Install from TestPyPI to test\npip install --index-url https://test.pypi.org/simple/ my-package\n\n# If all good, publish to PyPI\ntwine upload dist/*\n```\n\n**Using API tokens (recommended):**\n```bash\n# Create ~/.pypirc\n[distutils]\nindex-servers =\n    pypi\n    testpypi\n\n[pypi]\nusername = __token__\npassword = pypi-...your-token...\n\n[testpypi]\nusername = __token__\npassword = pypi-...your-test-token...\n```\n\n### Pattern 10: Automated Publishing with GitHub Actions\n\n```yaml\n# .github/workflows/publish.yml\nname: Publish to PyPI\n\non:\n  release:\n    types: [created]\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.11\"\n\n      - name: Install dependencies\n        run: |\n          pip install build twine\n\n      - name: Build package\n        run: python -m build\n\n      - name: Check package\n        run: twine check dist/*\n\n      - name: Publish to PyPI\n        env:\n          TWINE_USERNAME: __token__\n          TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}\n        run: twine upload dist/*\n```\n\n## Advanced Patterns\n\n### Pattern 11: Including Data Files\n\n```toml\n[tool.setuptools.package-data]\nmy_package = [\n    \"data/*.json\",\n    \"templates/*.html\",\n    \"static/css/*.css\",\n    \"py.typed\",\n]\n```\n\n**Accessing data files:**\n```python\n# src/my_package/loader.py\nfrom importlib.resources import files\nimport json\n\ndef load_config():\n    \"\"\"Load configuration from package data.\"\"\"\n    config_file = files(\"my_package\").joinpath(\"data/config.json\")\n    with config_file.open() as f:\n        return json.load(f)\n\n# Python 3.9+\nfrom importlib.resources import files\n\ndata = files(\"my_package\").joinpath(\"data/file.txt\").read_text()\n```\n\n### Pattern 12: Namespace Packages\n\n**For large projects split across multiple repositories:**\n\n```\n# Package 1: company-core\ncompany/\n\u2514\u2500\u2500 core/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 models.py\n\n# Package 2: company-api\ncompany/\n\u2514\u2500\u2500 api/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 routes.py\n```\n\n**Do NOT include __init__.py in the namespace directory (company/):**\n\n```toml\n# company-core/pyproject.toml\n[project]\nname = \"company-core\"\n\n[tool.setuptools.packages.find]\nwhere = [\".\"]\ninclude = [\"company.core*\"]\n\n# company-api/pyproject.toml\n[project]\nname = \"company-api\"\n\n[tool.setuptools.packages.find]\nwhere = [\".\"]\ninclude = [\"company.api*\"]\n```\n\n**Usage:**\n```python\n# Both packages can be imported under same namespace\nfrom company.core import models\nfrom company.api import routes\n```\n\n### Pattern 13: C Extensions\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\", \"Cython>=0.29\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools]\next-modules = [\n    {name = \"my_package.fast_module\", sources = [\"src/fast_module.c\"]},\n]\n```\n\n**Or with setup.py:**\n```python\n# setup.py\nfrom setuptools import setup, Extension\n\nsetup(\n    ext_modules=[\n        Extension(\n            \"my_package.fast_module\",\n            sources=[\"src/fast_module.c\"],\n            include_dirs=[\"src/include\"],\n        )\n    ]\n)\n```\n\n## Version Management\n\n### Pattern 14: Semantic Versioning\n\n```python\n# src/my_package/__init__.py\n__version__ = \"1.2.3\"\n\n# Semantic versioning: MAJOR.MINOR.PATCH\n# MAJOR: Breaking changes\n# MINOR: New features (backward compatible)\n# PATCH: Bug fixes\n```\n\n**Version constraints in dependencies:**\n```toml\ndependencies = [\n    \"requests>=2.28.0,<3.0.0\",  # Compatible range\n    \"click~=8.1.0\",              # Compatible release (~= 8.1.0 means >=8.1.0,<8.2.0)\n    \"pydantic>=2.0\",             # Minimum version\n    \"numpy==1.24.3\",             # Exact version (avoid if possible)\n]\n```\n\n### Pattern 15: Git-Based Versioning\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\", \"setuptools-scm>=8.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-package\"\ndynamic = [\"version\"]\n\n[tool.setuptools_scm]\nwrite_to = \"src/my_package/_version.py\"\nversion_scheme = \"post-release\"\nlocal_scheme = \"dirty-tag\"\n```\n\n**Creates versions like:**\n- `1.0.0` (from git tag)\n- `1.0.1.dev3+g1234567` (3 commits after tag)\n\n## Testing Installation\n\n### Pattern 16: Editable Install\n\n```bash\n# Install in development mode\npip install -e .\n\n# With optional dependencies\npip install -e \".[dev]\"\npip install -e \".[dev,docs]\"\n\n# Now changes to source code are immediately reflected\n```\n\n### Pattern 17: Testing in Isolated Environment\n\n```bash\n# Create virtual environment\npython -m venv test-env\nsource test-env/bin/activate  # Linux/Mac\n# test-env\\Scripts\\activate  # Windows\n\n# Install package\npip install dist/my_package-1.0.0-py3-none-any.whl\n\n# Test it works\npython -c \"import my_package; print(my_package.__version__)\"\n\n# Test CLI\nmy-tool --help\n\n# Cleanup\ndeactivate\nrm -rf test-env\n```\n\n## Documentation\n\n### Pattern 18: README.md Template\n\n```markdown\n# My Package\n\n[![PyPI version](https://badge.fury.io/py/my-package.svg)](https://pypi.org/project/my-package/)\n[![Python versions](https://img.shields.io/pypi/pyversions/my-package.svg)](https://pypi.org/project/my-package/)\n[![Tests](https://github.com/username/my-package/workflows/Tests/badge.svg)](https://github.com/username/my-package/actions)\n\nBrief description of your package.\n\n## Installation\n\n```bash\npip install my-package\n```\n\n## Quick Start\n\n```python\nfrom my_package import something\n\nresult = something.do_stuff()\n```\n\n## Features\n\n- Feature 1\n- Feature 2\n- Feature 3\n\n## Documentation\n\nFull documentation: https://my-package.readthedocs.io\n\n## Development\n\n```bash\ngit clone https://github.com/username/my-package.git\ncd my-package\npip install -e \".[dev]\"\npytest\n```\n\n## License\n\nMIT\n```\n\n## Common Patterns\n\n### Pattern 19: Multi-Architecture Wheels\n\n```yaml\n# .github/workflows/wheels.yml\nname: Build wheels\n\non: [push, pull_request]\n\njobs:\n  build_wheels:\n    name: Build wheels on ${{ matrix.os }}\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Build wheels\n        uses: pypa/cibuildwheel@v2.16.2\n\n      - uses: actions/upload-artifact@v3\n        with:\n          path: ./wheelhouse/*.whl\n```\n\n### Pattern 20: Private Package Index\n\n```bash\n# Install from private index\npip install my-package --index-url https://private.pypi.org/simple/\n\n# Or add to pip.conf\n[global]\nindex-url = https://private.pypi.org/simple/\nextra-index-url = https://pypi.org/simple/\n\n# Upload to private index\ntwine upload --repository-url https://private.pypi.org/ dist/*\n```\n\n## File Templates\n\n### .gitignore for Python Packages\n\n```gitignore\n# Build artifacts\nbuild/\ndist/\n*.egg-info/\n*.egg\n.eggs/\n\n# Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n\n# Virtual environments\nvenv/\nenv/\nENV/\n\n# IDE\n.vscode/\n.idea/\n*.swp\n\n# Testing\n.pytest_cache/\n.coverage\nhtmlcov/\n\n# Distribution\n*.whl\n*.tar.gz\n```\n\n### MANIFEST.in\n\n```\n# MANIFEST.in\ninclude README.md\ninclude LICENSE\ninclude pyproject.toml\n\nrecursive-include src/my_package/data *.json\nrecursive-include src/my_package/templates *.html\nrecursive-exclude * __pycache__\nrecursive-exclude * *.py[co]\n```\n\n## Checklist for Publishing\n\n- [ ] Code is tested (pytest passing)\n- [ ] Documentation is complete (README, docstrings)\n- [ ] Version number updated\n- [ ] CHANGELOG.md updated\n- [ ] License file included\n- [ ] pyproject.toml is complete\n- [ ] Package builds without errors\n- [ ] Installation tested in clean environment\n- [ ] CLI tools work (if applicable)\n- [ ] PyPI metadata is correct (classifiers, keywords)\n- [ ] GitHub repository linked\n- [ ] Tested on TestPyPI first\n- [ ] Git tag created for release\n\n## Resources\n\n- **Python Packaging Guide**: https://packaging.python.org/\n- **PyPI**: https://pypi.org/\n- **TestPyPI**: https://test.pypi.org/\n- **setuptools documentation**: https://setuptools.pypa.io/\n- **build**: https://pypa-build.readthedocs.io/\n- **twine**: https://twine.readthedocs.io/\n\n## Best Practices Summary\n\n1. **Use src/ layout** for cleaner package structure\n2. **Use pyproject.toml** for modern packaging\n3. **Pin build dependencies** in build-system.requires\n4. **Version appropriately** with semantic versioning\n5. **Include all metadata** (classifiers, URLs, etc.)\n6. **Test installation** in clean environments\n7. **Use TestPyPI** before publishing to PyPI\n8. **Document thoroughly** with README and docstrings\n9. **Include LICENSE** file\n10. **Automate publishing** with CI/CD\n",
      "references": {},
      "assets": {}
    },
    {
      "name": "python-performance-optimization",
      "description": "Profile and optimize Python code using cProfile, memory profilers, and performance best practices. Use when debugging slow Python code, optimizing bottlenecks, or improving application performance.",
      "plugin": "python-development",
      "source_path": "plugins/python-development/skills/python-performance-optimization/SKILL.md",
      "category": "languages",
      "keywords": [
        "python",
        "django",
        "fastapi",
        "async",
        "backend"
      ],
      "content": "---\nname: python-performance-optimization\ndescription: Profile and optimize Python code using cProfile, memory profilers, and performance best practices. Use when debugging slow Python code, optimizing bottlenecks, or improving application performance.\n---\n\n# Python Performance Optimization\n\nComprehensive guide to profiling, analyzing, and optimizing Python code for better performance, including CPU profiling, memory optimization, and implementation best practices.\n\n## When to Use This Skill\n\n- Identifying performance bottlenecks in Python applications\n- Reducing application latency and response times\n- Optimizing CPU-intensive operations\n- Reducing memory consumption and memory leaks\n- Improving database query performance\n- Optimizing I/O operations\n- Speeding up data processing pipelines\n- Implementing high-performance algorithms\n- Profiling production applications\n\n## Core Concepts\n\n### 1. Profiling Types\n- **CPU Profiling**: Identify time-consuming functions\n- **Memory Profiling**: Track memory allocation and leaks\n- **Line Profiling**: Profile at line-by-line granularity\n- **Call Graph**: Visualize function call relationships\n\n### 2. Performance Metrics\n- **Execution Time**: How long operations take\n- **Memory Usage**: Peak and average memory consumption\n- **CPU Utilization**: Processor usage patterns\n- **I/O Wait**: Time spent on I/O operations\n\n### 3. Optimization Strategies\n- **Algorithmic**: Better algorithms and data structures\n- **Implementation**: More efficient code patterns\n- **Parallelization**: Multi-threading/processing\n- **Caching**: Avoid redundant computation\n- **Native Extensions**: C/Rust for critical paths\n\n## Quick Start\n\n### Basic Timing\n\n```python\nimport time\n\ndef measure_time():\n    \"\"\"Simple timing measurement.\"\"\"\n    start = time.time()\n\n    # Your code here\n    result = sum(range(1000000))\n\n    elapsed = time.time() - start\n    print(f\"Execution time: {elapsed:.4f} seconds\")\n    return result\n\n# Better: use timeit for accurate measurements\nimport timeit\n\nexecution_time = timeit.timeit(\n    \"sum(range(1000000))\",\n    number=100\n)\nprint(f\"Average time: {execution_time/100:.6f} seconds\")\n```\n\n## Profiling Tools\n\n### Pattern 1: cProfile - CPU Profiling\n\n```python\nimport cProfile\nimport pstats\nfrom pstats import SortKey\n\ndef slow_function():\n    \"\"\"Function to profile.\"\"\"\n    total = 0\n    for i in range(1000000):\n        total += i\n    return total\n\ndef another_function():\n    \"\"\"Another function.\"\"\"\n    return [i**2 for i in range(100000)]\n\ndef main():\n    \"\"\"Main function to profile.\"\"\"\n    result1 = slow_function()\n    result2 = another_function()\n    return result1, result2\n\n# Profile the code\nif __name__ == \"__main__\":\n    profiler = cProfile.Profile()\n    profiler.enable()\n\n    main()\n\n    profiler.disable()\n\n    # Print stats\n    stats = pstats.Stats(profiler)\n    stats.sort_stats(SortKey.CUMULATIVE)\n    stats.print_stats(10)  # Top 10 functions\n\n    # Save to file for later analysis\n    stats.dump_stats(\"profile_output.prof\")\n```\n\n**Command-line profiling:**\n```bash\n# Profile a script\npython -m cProfile -o output.prof script.py\n\n# View results\npython -m pstats output.prof\n# In pstats:\n# sort cumtime\n# stats 10\n```\n\n### Pattern 2: line_profiler - Line-by-Line Profiling\n\n```python\n# Install: pip install line-profiler\n\n# Add @profile decorator (line_profiler provides this)\n@profile\ndef process_data(data):\n    \"\"\"Process data with line profiling.\"\"\"\n    result = []\n    for item in data:\n        processed = item * 2\n        result.append(processed)\n    return result\n\n# Run with:\n# kernprof -l -v script.py\n```\n\n**Manual line profiling:**\n```python\nfrom line_profiler import LineProfiler\n\ndef process_data(data):\n    \"\"\"Function to profile.\"\"\"\n    result = []\n    for item in data:\n        processed = item * 2\n        result.append(processed)\n    return result\n\nif __name__ == \"__main__\":\n    lp = LineProfiler()\n    lp.add_function(process_data)\n\n    data = list(range(100000))\n\n    lp_wrapper = lp(process_data)\n    lp_wrapper(data)\n\n    lp.print_stats()\n```\n\n### Pattern 3: memory_profiler - Memory Usage\n\n```python\n# Install: pip install memory-profiler\n\nfrom memory_profiler import profile\n\n@profile\ndef memory_intensive():\n    \"\"\"Function that uses lots of memory.\"\"\"\n    # Create large list\n    big_list = [i for i in range(1000000)]\n\n    # Create large dict\n    big_dict = {i: i**2 for i in range(100000)}\n\n    # Process data\n    result = sum(big_list)\n\n    return result\n\nif __name__ == \"__main__\":\n    memory_intensive()\n\n# Run with:\n# python -m memory_profiler script.py\n```\n\n### Pattern 4: py-spy - Production Profiling\n\n```bash\n# Install: pip install py-spy\n\n# Profile a running Python process\npy-spy top --pid 12345\n\n# Generate flamegraph\npy-spy record -o profile.svg --pid 12345\n\n# Profile a script\npy-spy record -o profile.svg -- python script.py\n\n# Dump current call stack\npy-spy dump --pid 12345\n```\n\n## Optimization Patterns\n\n### Pattern 5: List Comprehensions vs Loops\n\n```python\nimport timeit\n\n# Slow: Traditional loop\ndef slow_squares(n):\n    \"\"\"Create list of squares using loop.\"\"\"\n    result = []\n    for i in range(n):\n        result.append(i**2)\n    return result\n\n# Fast: List comprehension\ndef fast_squares(n):\n    \"\"\"Create list of squares using comprehension.\"\"\"\n    return [i**2 for i in range(n)]\n\n# Benchmark\nn = 100000\n\nslow_time = timeit.timeit(lambda: slow_squares(n), number=100)\nfast_time = timeit.timeit(lambda: fast_squares(n), number=100)\n\nprint(f\"Loop: {slow_time:.4f}s\")\nprint(f\"Comprehension: {fast_time:.4f}s\")\nprint(f\"Speedup: {slow_time/fast_time:.2f}x\")\n\n# Even faster for simple operations: map\ndef faster_squares(n):\n    \"\"\"Use map for even better performance.\"\"\"\n    return list(map(lambda x: x**2, range(n)))\n```\n\n### Pattern 6: Generator Expressions for Memory\n\n```python\nimport sys\n\ndef list_approach():\n    \"\"\"Memory-intensive list.\"\"\"\n    data = [i**2 for i in range(1000000)]\n    return sum(data)\n\ndef generator_approach():\n    \"\"\"Memory-efficient generator.\"\"\"\n    data = (i**2 for i in range(1000000))\n    return sum(data)\n\n# Memory comparison\nlist_data = [i for i in range(1000000)]\ngen_data = (i for i in range(1000000))\n\nprint(f\"List size: {sys.getsizeof(list_data)} bytes\")\nprint(f\"Generator size: {sys.getsizeof(gen_data)} bytes\")\n\n# Generators use constant memory regardless of size\n```\n\n### Pattern 7: String Concatenation\n\n```python\nimport timeit\n\ndef slow_concat(items):\n    \"\"\"Slow string concatenation.\"\"\"\n    result = \"\"\n    for item in items:\n        result += str(item)\n    return result\n\ndef fast_concat(items):\n    \"\"\"Fast string concatenation with join.\"\"\"\n    return \"\".join(str(item) for item in items)\n\ndef faster_concat(items):\n    \"\"\"Even faster with list.\"\"\"\n    parts = [str(item) for item in items]\n    return \"\".join(parts)\n\nitems = list(range(10000))\n\n# Benchmark\nslow = timeit.timeit(lambda: slow_concat(items), number=100)\nfast = timeit.timeit(lambda: fast_concat(items), number=100)\nfaster = timeit.timeit(lambda: faster_concat(items), number=100)\n\nprint(f\"Concatenation (+): {slow:.4f}s\")\nprint(f\"Join (generator): {fast:.4f}s\")\nprint(f\"Join (list): {faster:.4f}s\")\n```\n\n### Pattern 8: Dictionary Lookups vs List Searches\n\n```python\nimport timeit\n\n# Create test data\nsize = 10000\nitems = list(range(size))\nlookup_dict = {i: i for i in range(size)}\n\ndef list_search(items, target):\n    \"\"\"O(n) search in list.\"\"\"\n    return target in items\n\ndef dict_search(lookup_dict, target):\n    \"\"\"O(1) search in dict.\"\"\"\n    return target in lookup_dict\n\ntarget = size - 1  # Worst case for list\n\n# Benchmark\nlist_time = timeit.timeit(\n    lambda: list_search(items, target),\n    number=1000\n)\ndict_time = timeit.timeit(\n    lambda: dict_search(lookup_dict, target),\n    number=1000\n)\n\nprint(f\"List search: {list_time:.6f}s\")\nprint(f\"Dict search: {dict_time:.6f}s\")\nprint(f\"Speedup: {list_time/dict_time:.0f}x\")\n```\n\n### Pattern 9: Local Variable Access\n\n```python\nimport timeit\n\n# Global variable (slow)\nGLOBAL_VALUE = 100\n\ndef use_global():\n    \"\"\"Access global variable.\"\"\"\n    total = 0\n    for i in range(10000):\n        total += GLOBAL_VALUE\n    return total\n\ndef use_local():\n    \"\"\"Use local variable.\"\"\"\n    local_value = 100\n    total = 0\n    for i in range(10000):\n        total += local_value\n    return total\n\n# Local is faster\nglobal_time = timeit.timeit(use_global, number=1000)\nlocal_time = timeit.timeit(use_local, number=1000)\n\nprint(f\"Global access: {global_time:.4f}s\")\nprint(f\"Local access: {local_time:.4f}s\")\nprint(f\"Speedup: {global_time/local_time:.2f}x\")\n```\n\n### Pattern 10: Function Call Overhead\n\n```python\nimport timeit\n\ndef calculate_inline():\n    \"\"\"Inline calculation.\"\"\"\n    total = 0\n    for i in range(10000):\n        total += i * 2 + 1\n    return total\n\ndef helper_function(x):\n    \"\"\"Helper function.\"\"\"\n    return x * 2 + 1\n\ndef calculate_with_function():\n    \"\"\"Calculation with function calls.\"\"\"\n    total = 0\n    for i in range(10000):\n        total += helper_function(i)\n    return total\n\n# Inline is faster due to no call overhead\ninline_time = timeit.timeit(calculate_inline, number=1000)\nfunction_time = timeit.timeit(calculate_with_function, number=1000)\n\nprint(f\"Inline: {inline_time:.4f}s\")\nprint(f\"Function calls: {function_time:.4f}s\")\n```\n\n## Advanced Optimization\n\n### Pattern 11: NumPy for Numerical Operations\n\n```python\nimport timeit\nimport numpy as np\n\ndef python_sum(n):\n    \"\"\"Sum using pure Python.\"\"\"\n    return sum(range(n))\n\ndef numpy_sum(n):\n    \"\"\"Sum using NumPy.\"\"\"\n    return np.arange(n).sum()\n\nn = 1000000\n\npython_time = timeit.timeit(lambda: python_sum(n), number=100)\nnumpy_time = timeit.timeit(lambda: numpy_sum(n), number=100)\n\nprint(f\"Python: {python_time:.4f}s\")\nprint(f\"NumPy: {numpy_time:.4f}s\")\nprint(f\"Speedup: {python_time/numpy_time:.2f}x\")\n\n# Vectorized operations\ndef python_multiply():\n    \"\"\"Element-wise multiplication in Python.\"\"\"\n    a = list(range(100000))\n    b = list(range(100000))\n    return [x * y for x, y in zip(a, b)]\n\ndef numpy_multiply():\n    \"\"\"Vectorized multiplication in NumPy.\"\"\"\n    a = np.arange(100000)\n    b = np.arange(100000)\n    return a * b\n\npy_time = timeit.timeit(python_multiply, number=100)\nnp_time = timeit.timeit(numpy_multiply, number=100)\n\nprint(f\"\\nPython multiply: {py_time:.4f}s\")\nprint(f\"NumPy multiply: {np_time:.4f}s\")\nprint(f\"Speedup: {py_time/np_time:.2f}x\")\n```\n\n### Pattern 12: Caching with functools.lru_cache\n\n```python\nfrom functools import lru_cache\nimport timeit\n\ndef fibonacci_slow(n):\n    \"\"\"Recursive fibonacci without caching.\"\"\"\n    if n < 2:\n        return n\n    return fibonacci_slow(n-1) + fibonacci_slow(n-2)\n\n@lru_cache(maxsize=None)\ndef fibonacci_fast(n):\n    \"\"\"Recursive fibonacci with caching.\"\"\"\n    if n < 2:\n        return n\n    return fibonacci_fast(n-1) + fibonacci_fast(n-2)\n\n# Massive speedup for recursive algorithms\nn = 30\n\nslow_time = timeit.timeit(lambda: fibonacci_slow(n), number=1)\nfast_time = timeit.timeit(lambda: fibonacci_fast(n), number=1000)\n\nprint(f\"Without cache (1 run): {slow_time:.4f}s\")\nprint(f\"With cache (1000 runs): {fast_time:.4f}s\")\n\n# Cache info\nprint(f\"Cache info: {fibonacci_fast.cache_info()}\")\n```\n\n### Pattern 13: Using __slots__ for Memory\n\n```python\nimport sys\n\nclass RegularClass:\n    \"\"\"Regular class with __dict__.\"\"\"\n    def __init__(self, x, y, z):\n        self.x = x\n        self.y = y\n        self.z = z\n\nclass SlottedClass:\n    \"\"\"Class with __slots__ for memory efficiency.\"\"\"\n    __slots__ = ['x', 'y', 'z']\n\n    def __init__(self, x, y, z):\n        self.x = x\n        self.y = y\n        self.z = z\n\n# Memory comparison\nregular = RegularClass(1, 2, 3)\nslotted = SlottedClass(1, 2, 3)\n\nprint(f\"Regular class size: {sys.getsizeof(regular)} bytes\")\nprint(f\"Slotted class size: {sys.getsizeof(slotted)} bytes\")\n\n# Significant savings with many instances\nregular_objects = [RegularClass(i, i+1, i+2) for i in range(10000)]\nslotted_objects = [SlottedClass(i, i+1, i+2) for i in range(10000)]\n\nprint(f\"\\nMemory for 10000 regular objects: ~{sys.getsizeof(regular) * 10000} bytes\")\nprint(f\"Memory for 10000 slotted objects: ~{sys.getsizeof(slotted) * 10000} bytes\")\n```\n\n### Pattern 14: Multiprocessing for CPU-Bound Tasks\n\n```python\nimport multiprocessing as mp\nimport time\n\ndef cpu_intensive_task(n):\n    \"\"\"CPU-intensive calculation.\"\"\"\n    return sum(i**2 for i in range(n))\n\ndef sequential_processing():\n    \"\"\"Process tasks sequentially.\"\"\"\n    start = time.time()\n    results = [cpu_intensive_task(1000000) for _ in range(4)]\n    elapsed = time.time() - start\n    return elapsed, results\n\ndef parallel_processing():\n    \"\"\"Process tasks in parallel.\"\"\"\n    start = time.time()\n    with mp.Pool(processes=4) as pool:\n        results = pool.map(cpu_intensive_task, [1000000] * 4)\n    elapsed = time.time() - start\n    return elapsed, results\n\nif __name__ == \"__main__\":\n    seq_time, seq_results = sequential_processing()\n    par_time, par_results = parallel_processing()\n\n    print(f\"Sequential: {seq_time:.2f}s\")\n    print(f\"Parallel: {par_time:.2f}s\")\n    print(f\"Speedup: {seq_time/par_time:.2f}x\")\n```\n\n### Pattern 15: Async I/O for I/O-Bound Tasks\n\n```python\nimport asyncio\nimport aiohttp\nimport time\nimport requests\n\nurls = [\n    \"https://httpbin.org/delay/1\",\n    \"https://httpbin.org/delay/1\",\n    \"https://httpbin.org/delay/1\",\n    \"https://httpbin.org/delay/1\",\n]\n\ndef synchronous_requests():\n    \"\"\"Synchronous HTTP requests.\"\"\"\n    start = time.time()\n    results = []\n    for url in urls:\n        response = requests.get(url)\n        results.append(response.status_code)\n    elapsed = time.time() - start\n    return elapsed, results\n\nasync def async_fetch(session, url):\n    \"\"\"Async HTTP request.\"\"\"\n    async with session.get(url) as response:\n        return response.status\n\nasync def asynchronous_requests():\n    \"\"\"Asynchronous HTTP requests.\"\"\"\n    start = time.time()\n    async with aiohttp.ClientSession() as session:\n        tasks = [async_fetch(session, url) for url in urls]\n        results = await asyncio.gather(*tasks)\n    elapsed = time.time() - start\n    return elapsed, results\n\n# Async is much faster for I/O-bound work\nsync_time, sync_results = synchronous_requests()\nasync_time, async_results = asyncio.run(asynchronous_requests())\n\nprint(f\"Synchronous: {sync_time:.2f}s\")\nprint(f\"Asynchronous: {async_time:.2f}s\")\nprint(f\"Speedup: {sync_time/async_time:.2f}x\")\n```\n\n## Database Optimization\n\n### Pattern 16: Batch Database Operations\n\n```python\nimport sqlite3\nimport time\n\ndef create_db():\n    \"\"\"Create test database.\"\"\"\n    conn = sqlite3.connect(\":memory:\")\n    conn.execute(\"CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)\")\n    return conn\n\ndef slow_inserts(conn, count):\n    \"\"\"Insert records one at a time.\"\"\"\n    start = time.time()\n    cursor = conn.cursor()\n    for i in range(count):\n        cursor.execute(\"INSERT INTO users (name) VALUES (?)\", (f\"User {i}\",))\n        conn.commit()  # Commit each insert\n    elapsed = time.time() - start\n    return elapsed\n\ndef fast_inserts(conn, count):\n    \"\"\"Batch insert with single commit.\"\"\"\n    start = time.time()\n    cursor = conn.cursor()\n    data = [(f\"User {i}\",) for i in range(count)]\n    cursor.executemany(\"INSERT INTO users (name) VALUES (?)\", data)\n    conn.commit()  # Single commit\n    elapsed = time.time() - start\n    return elapsed\n\n# Benchmark\nconn1 = create_db()\nslow_time = slow_inserts(conn1, 1000)\n\nconn2 = create_db()\nfast_time = fast_inserts(conn2, 1000)\n\nprint(f\"Individual inserts: {slow_time:.4f}s\")\nprint(f\"Batch insert: {fast_time:.4f}s\")\nprint(f\"Speedup: {slow_time/fast_time:.2f}x\")\n```\n\n### Pattern 17: Query Optimization\n\n```python\n# Use indexes for frequently queried columns\n\"\"\"\n-- Slow: No index\nSELECT * FROM users WHERE email = 'user@example.com';\n\n-- Fast: With index\nCREATE INDEX idx_users_email ON users(email);\nSELECT * FROM users WHERE email = 'user@example.com';\n\"\"\"\n\n# Use query planning\nimport sqlite3\n\nconn = sqlite3.connect(\"example.db\")\ncursor = conn.cursor()\n\n# Analyze query performance\ncursor.execute(\"EXPLAIN QUERY PLAN SELECT * FROM users WHERE email = ?\", (\"test@example.com\",))\nprint(cursor.fetchall())\n\n# Use SELECT only needed columns\n# Slow: SELECT *\n# Fast: SELECT id, name\n```\n\n## Memory Optimization\n\n### Pattern 18: Detecting Memory Leaks\n\n```python\nimport tracemalloc\nimport gc\n\ndef memory_leak_example():\n    \"\"\"Example that leaks memory.\"\"\"\n    leaked_objects = []\n\n    for i in range(100000):\n        # Objects added but never removed\n        leaked_objects.append([i] * 100)\n\n    # In real code, this would be an unintended reference\n\ndef track_memory_usage():\n    \"\"\"Track memory allocations.\"\"\"\n    tracemalloc.start()\n\n    # Take snapshot before\n    snapshot1 = tracemalloc.take_snapshot()\n\n    # Run code\n    memory_leak_example()\n\n    # Take snapshot after\n    snapshot2 = tracemalloc.take_snapshot()\n\n    # Compare\n    top_stats = snapshot2.compare_to(snapshot1, 'lineno')\n\n    print(\"Top 10 memory allocations:\")\n    for stat in top_stats[:10]:\n        print(stat)\n\n    tracemalloc.stop()\n\n# Monitor memory\ntrack_memory_usage()\n\n# Force garbage collection\ngc.collect()\n```\n\n### Pattern 19: Iterators vs Lists\n\n```python\nimport sys\n\ndef process_file_list(filename):\n    \"\"\"Load entire file into memory.\"\"\"\n    with open(filename) as f:\n        lines = f.readlines()  # Loads all lines\n        return sum(1 for line in lines if line.strip())\n\ndef process_file_iterator(filename):\n    \"\"\"Process file line by line.\"\"\"\n    with open(filename) as f:\n        return sum(1 for line in f if line.strip())\n\n# Iterator uses constant memory\n# List loads entire file into memory\n```\n\n### Pattern 20: Weakref for Caches\n\n```python\nimport weakref\n\nclass CachedResource:\n    \"\"\"Resource that can be garbage collected.\"\"\"\n    def __init__(self, data):\n        self.data = data\n\n# Regular cache prevents garbage collection\nregular_cache = {}\n\ndef get_resource_regular(key):\n    \"\"\"Get resource from regular cache.\"\"\"\n    if key not in regular_cache:\n        regular_cache[key] = CachedResource(f\"Data for {key}\")\n    return regular_cache[key]\n\n# Weak reference cache allows garbage collection\nweak_cache = weakref.WeakValueDictionary()\n\ndef get_resource_weak(key):\n    \"\"\"Get resource from weak cache.\"\"\"\n    resource = weak_cache.get(key)\n    if resource is None:\n        resource = CachedResource(f\"Data for {key}\")\n        weak_cache[key] = resource\n    return resource\n\n# When no strong references exist, objects can be GC'd\n```\n\n## Benchmarking Tools\n\n### Custom Benchmark Decorator\n\n```python\nimport time\nfrom functools import wraps\n\ndef benchmark(func):\n    \"\"\"Decorator to benchmark function execution.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start = time.perf_counter()\n        result = func(*args, **kwargs)\n        elapsed = time.perf_counter() - start\n        print(f\"{func.__name__} took {elapsed:.6f} seconds\")\n        return result\n    return wrapper\n\n@benchmark\ndef slow_function():\n    \"\"\"Function to benchmark.\"\"\"\n    time.sleep(0.5)\n    return sum(range(1000000))\n\nresult = slow_function()\n```\n\n### Performance Testing with pytest-benchmark\n\n```python\n# Install: pip install pytest-benchmark\n\ndef test_list_comprehension(benchmark):\n    \"\"\"Benchmark list comprehension.\"\"\"\n    result = benchmark(lambda: [i**2 for i in range(10000)])\n    assert len(result) == 10000\n\ndef test_map_function(benchmark):\n    \"\"\"Benchmark map function.\"\"\"\n    result = benchmark(lambda: list(map(lambda x: x**2, range(10000))))\n    assert len(result) == 10000\n\n# Run with: pytest test_performance.py --benchmark-compare\n```\n\n## Best Practices\n\n1. **Profile before optimizing** - Measure to find real bottlenecks\n2. **Focus on hot paths** - Optimize code that runs most frequently\n3. **Use appropriate data structures** - Dict for lookups, set for membership\n4. **Avoid premature optimization** - Clarity first, then optimize\n5. **Use built-in functions** - They're implemented in C\n6. **Cache expensive computations** - Use lru_cache\n7. **Batch I/O operations** - Reduce system calls\n8. **Use generators** for large datasets\n9. **Consider NumPy** for numerical operations\n10. **Profile production code** - Use py-spy for live systems\n\n## Common Pitfalls\n\n- Optimizing without profiling\n- Using global variables unnecessarily\n- Not using appropriate data structures\n- Creating unnecessary copies of data\n- Not using connection pooling for databases\n- Ignoring algorithmic complexity\n- Over-optimizing rare code paths\n- Not considering memory usage\n\n## Resources\n\n- **cProfile**: Built-in CPU profiler\n- **memory_profiler**: Memory usage profiling\n- **line_profiler**: Line-by-line profiling\n- **py-spy**: Sampling profiler for production\n- **NumPy**: High-performance numerical computing\n- **Cython**: Compile Python to C\n- **PyPy**: Alternative Python interpreter with JIT\n\n## Performance Checklist\n\n- [ ] Profiled code to identify bottlenecks\n- [ ] Used appropriate data structures\n- [ ] Implemented caching where beneficial\n- [ ] Optimized database queries\n- [ ] Used generators for large datasets\n- [ ] Considered multiprocessing for CPU-bound tasks\n- [ ] Used async I/O for I/O-bound tasks\n- [ ] Minimized function call overhead in hot loops\n- [ ] Checked for memory leaks\n- [ ] Benchmarked before and after optimization\n",
      "references": {},
      "assets": {}
    },
    {
      "name": "uv-package-manager",
      "description": "Master the uv package manager for fast Python dependency management, virtual environments, and modern Python project workflows. Use when setting up Python projects, managing dependencies, or optimizing Python development workflows with uv.",
      "plugin": "python-development",
      "source_path": "plugins/python-development/skills/uv-package-manager/SKILL.md",
      "category": "languages",
      "keywords": [
        "python",
        "django",
        "fastapi",
        "async",
        "backend"
      ],
      "content": "---\nname: uv-package-manager\ndescription: Master the uv package manager for fast Python dependency management, virtual environments, and modern Python project workflows. Use when setting up Python projects, managing dependencies, or optimizing Python development workflows with uv.\n---\n\n# UV Package Manager\n\nComprehensive guide to using uv, an extremely fast Python package installer and resolver written in Rust, for modern Python project management and dependency workflows.\n\n## When to Use This Skill\n\n- Setting up new Python projects quickly\n- Managing Python dependencies faster than pip\n- Creating and managing virtual environments\n- Installing Python interpreters\n- Resolving dependency conflicts efficiently\n- Migrating from pip/pip-tools/poetry\n- Speeding up CI/CD pipelines\n- Managing monorepo Python projects\n- Working with lockfiles for reproducible builds\n- Optimizing Docker builds with Python dependencies\n\n## Core Concepts\n\n### 1. What is uv?\n- **Ultra-fast package installer**: 10-100x faster than pip\n- **Written in Rust**: Leverages Rust's performance\n- **Drop-in pip replacement**: Compatible with pip workflows\n- **Virtual environment manager**: Create and manage venvs\n- **Python installer**: Download and manage Python versions\n- **Resolver**: Advanced dependency resolution\n- **Lockfile support**: Reproducible installations\n\n### 2. Key Features\n- Blazing fast installation speeds\n- Disk space efficient with global cache\n- Compatible with pip, pip-tools, poetry\n- Comprehensive dependency resolution\n- Cross-platform support (Linux, macOS, Windows)\n- No Python required for installation\n- Built-in virtual environment support\n\n### 3. UV vs Traditional Tools\n- **vs pip**: 10-100x faster, better resolver\n- **vs pip-tools**: Faster, simpler, better UX\n- **vs poetry**: Faster, less opinionated, lighter\n- **vs conda**: Faster, Python-focused\n\n## Installation\n\n### Quick Install\n\n```bash\n# macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows (PowerShell)\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Using pip (if you already have Python)\npip install uv\n\n# Using Homebrew (macOS)\nbrew install uv\n\n# Using cargo (if you have Rust)\ncargo install --git https://github.com/astral-sh/uv uv\n```\n\n### Verify Installation\n\n```bash\nuv --version\n# uv 0.x.x\n```\n\n## Quick Start\n\n### Create a New Project\n\n```bash\n# Create new project with virtual environment\nuv init my-project\ncd my-project\n\n# Or create in current directory\nuv init .\n\n# Initialize creates:\n# - .python-version (Python version)\n# - pyproject.toml (project config)\n# - README.md\n# - .gitignore\n```\n\n### Install Dependencies\n\n```bash\n# Install packages (creates venv if needed)\nuv add requests pandas\n\n# Install dev dependencies\nuv add --dev pytest black ruff\n\n# Install from requirements.txt\nuv pip install -r requirements.txt\n\n# Install from pyproject.toml\nuv sync\n```\n\n## Virtual Environment Management\n\n### Pattern 1: Creating Virtual Environments\n\n```bash\n# Create virtual environment with uv\nuv venv\n\n# Create with specific Python version\nuv venv --python 3.12\n\n# Create with custom name\nuv venv my-env\n\n# Create with system site packages\nuv venv --system-site-packages\n\n# Specify location\nuv venv /path/to/venv\n```\n\n### Pattern 2: Activating Virtual Environments\n\n```bash\n# Linux/macOS\nsource .venv/bin/activate\n\n# Windows (Command Prompt)\n.venv\\Scripts\\activate.bat\n\n# Windows (PowerShell)\n.venv\\Scripts\\Activate.ps1\n\n# Or use uv run (no activation needed)\nuv run python script.py\nuv run pytest\n```\n\n### Pattern 3: Using uv run\n\n```bash\n# Run Python script (auto-activates venv)\nuv run python app.py\n\n# Run installed CLI tool\nuv run black .\nuv run pytest\n\n# Run with specific Python version\nuv run --python 3.11 python script.py\n\n# Pass arguments\nuv run python script.py --arg value\n```\n\n## Package Management\n\n### Pattern 4: Adding Dependencies\n\n```bash\n# Add package (adds to pyproject.toml)\nuv add requests\n\n# Add with version constraint\nuv add \"django>=4.0,<5.0\"\n\n# Add multiple packages\nuv add numpy pandas matplotlib\n\n# Add dev dependency\nuv add --dev pytest pytest-cov\n\n# Add optional dependency group\nuv add --optional docs sphinx\n\n# Add from git\nuv add git+https://github.com/user/repo.git\n\n# Add from git with specific ref\nuv add git+https://github.com/user/repo.git@v1.0.0\n\n# Add from local path\nuv add ./local-package\n\n# Add editable local package\nuv add -e ./local-package\n```\n\n### Pattern 5: Removing Dependencies\n\n```bash\n# Remove package\nuv remove requests\n\n# Remove dev dependency\nuv remove --dev pytest\n\n# Remove multiple packages\nuv remove numpy pandas matplotlib\n```\n\n### Pattern 6: Upgrading Dependencies\n\n```bash\n# Upgrade specific package\nuv add --upgrade requests\n\n# Upgrade all packages\nuv sync --upgrade\n\n# Upgrade package to latest\nuv add --upgrade requests\n\n# Show what would be upgraded\nuv tree --outdated\n```\n\n### Pattern 7: Locking Dependencies\n\n```bash\n# Generate uv.lock file\nuv lock\n\n# Update lock file\nuv lock --upgrade\n\n# Lock without installing\nuv lock --no-install\n\n# Lock specific package\nuv lock --upgrade-package requests\n```\n\n## Python Version Management\n\n### Pattern 8: Installing Python Versions\n\n```bash\n# Install Python version\nuv python install 3.12\n\n# Install multiple versions\nuv python install 3.11 3.12 3.13\n\n# Install latest version\nuv python install\n\n# List installed versions\nuv python list\n\n# Find available versions\nuv python list --all-versions\n```\n\n### Pattern 9: Setting Python Version\n\n```bash\n# Set Python version for project\nuv python pin 3.12\n\n# This creates/updates .python-version file\n\n# Use specific Python version for command\nuv --python 3.11 run python script.py\n\n# Create venv with specific version\nuv venv --python 3.12\n```\n\n## Project Configuration\n\n### Pattern 10: pyproject.toml with uv\n\n```toml\n[project]\nname = \"my-project\"\nversion = \"0.1.0\"\ndescription = \"My awesome project\"\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\ndependencies = [\n    \"requests>=2.31.0\",\n    \"pydantic>=2.0.0\",\n    \"click>=8.1.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=7.4.0\",\n    \"pytest-cov>=4.1.0\",\n    \"black>=23.0.0\",\n    \"ruff>=0.1.0\",\n    \"mypy>=1.5.0\",\n]\ndocs = [\n    \"sphinx>=7.0.0\",\n    \"sphinx-rtd-theme>=1.3.0\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.uv]\ndev-dependencies = [\n    # Additional dev dependencies managed by uv\n]\n\n[tool.uv.sources]\n# Custom package sources\nmy-package = { git = \"https://github.com/user/repo.git\" }\n```\n\n### Pattern 11: Using uv with Existing Projects\n\n```bash\n# Migrate from requirements.txt\nuv add -r requirements.txt\n\n# Migrate from poetry\n# Already have pyproject.toml, just use:\nuv sync\n\n# Export to requirements.txt\nuv pip freeze > requirements.txt\n\n# Export with hashes\nuv pip freeze --require-hashes > requirements.txt\n```\n\n## Advanced Workflows\n\n### Pattern 12: Monorepo Support\n\n```bash\n# Project structure\n# monorepo/\n#   packages/\n#     package-a/\n#       pyproject.toml\n#     package-b/\n#       pyproject.toml\n#   pyproject.toml (root)\n\n# Root pyproject.toml\n[tool.uv.workspace]\nmembers = [\"packages/*\"]\n\n# Install all workspace packages\nuv sync\n\n# Add workspace dependency\nuv add --path ./packages/package-a\n```\n\n### Pattern 13: CI/CD Integration\n\n```yaml\n# .github/workflows/test.yml\nname: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v2\n        with:\n          enable-cache: true\n\n      - name: Set up Python\n        run: uv python install 3.12\n\n      - name: Install dependencies\n        run: uv sync --all-extras --dev\n\n      - name: Run tests\n        run: uv run pytest\n\n      - name: Run linting\n        run: |\n          uv run ruff check .\n          uv run black --check .\n```\n\n### Pattern 14: Docker Integration\n\n```dockerfile\n# Dockerfile\nFROM python:3.12-slim\n\n# Install uv\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv\n\n# Set working directory\nWORKDIR /app\n\n# Copy dependency files\nCOPY pyproject.toml uv.lock ./\n\n# Install dependencies\nRUN uv sync --frozen --no-dev\n\n# Copy application code\nCOPY . .\n\n# Run application\nCMD [\"uv\", \"run\", \"python\", \"app.py\"]\n```\n\n**Optimized multi-stage build:**\n\n```dockerfile\n# Multi-stage Dockerfile\nFROM python:3.12-slim AS builder\n\n# Install uv\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv\n\nWORKDIR /app\n\n# Install dependencies to venv\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-dev --no-editable\n\n# Runtime stage\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Copy venv from builder\nCOPY --from=builder /app/.venv .venv\nCOPY . .\n\n# Use venv\nENV PATH=\"/app/.venv/bin:$PATH\"\n\nCMD [\"python\", \"app.py\"]\n```\n\n### Pattern 15: Lockfile Workflows\n\n```bash\n# Create lockfile (uv.lock)\nuv lock\n\n# Install from lockfile (exact versions)\nuv sync --frozen\n\n# Update lockfile without installing\nuv lock --no-install\n\n# Upgrade specific package in lock\nuv lock --upgrade-package requests\n\n# Check if lockfile is up to date\nuv lock --check\n\n# Export lockfile to requirements.txt\nuv export --format requirements-txt > requirements.txt\n\n# Export with hashes for security\nuv export --format requirements-txt --hash > requirements.txt\n```\n\n## Performance Optimization\n\n### Pattern 16: Using Global Cache\n\n```bash\n# UV automatically uses global cache at:\n# Linux: ~/.cache/uv\n# macOS: ~/Library/Caches/uv\n# Windows: %LOCALAPPDATA%\\uv\\cache\n\n# Clear cache\nuv cache clean\n\n# Check cache size\nuv cache dir\n```\n\n### Pattern 17: Parallel Installation\n\n```bash\n# UV installs packages in parallel by default\n\n# Control parallelism\nuv pip install --jobs 4 package1 package2\n\n# No parallel (sequential)\nuv pip install --jobs 1 package\n```\n\n### Pattern 18: Offline Mode\n\n```bash\n# Install from cache only (no network)\nuv pip install --offline package\n\n# Sync from lockfile offline\nuv sync --frozen --offline\n```\n\n## Comparison with Other Tools\n\n### uv vs pip\n\n```bash\n# pip\npython -m venv .venv\nsource .venv/bin/activate\npip install requests pandas numpy\n# ~30 seconds\n\n# uv\nuv venv\nuv add requests pandas numpy\n# ~2 seconds (10-15x faster)\n```\n\n### uv vs poetry\n\n```bash\n# poetry\npoetry init\npoetry add requests pandas\npoetry install\n# ~20 seconds\n\n# uv\nuv init\nuv add requests pandas\nuv sync\n# ~3 seconds (6-7x faster)\n```\n\n### uv vs pip-tools\n\n```bash\n# pip-tools\npip-compile requirements.in\npip-sync requirements.txt\n# ~15 seconds\n\n# uv\nuv lock\nuv sync --frozen\n# ~2 seconds (7-8x faster)\n```\n\n## Common Workflows\n\n### Pattern 19: Starting a New Project\n\n```bash\n# Complete workflow\nuv init my-project\ncd my-project\n\n# Set Python version\nuv python pin 3.12\n\n# Add dependencies\nuv add fastapi uvicorn pydantic\n\n# Add dev dependencies\nuv add --dev pytest black ruff mypy\n\n# Create structure\nmkdir -p src/my_project tests\n\n# Run tests\nuv run pytest\n\n# Format code\nuv run black .\nuv run ruff check .\n```\n\n### Pattern 20: Maintaining Existing Project\n\n```bash\n# Clone repository\ngit clone https://github.com/user/project.git\ncd project\n\n# Install dependencies (creates venv automatically)\nuv sync\n\n# Install with dev dependencies\nuv sync --all-extras\n\n# Update dependencies\nuv lock --upgrade\n\n# Run application\nuv run python app.py\n\n# Run tests\nuv run pytest\n\n# Add new dependency\nuv add new-package\n\n# Commit updated files\ngit add pyproject.toml uv.lock\ngit commit -m \"Add new-package dependency\"\n```\n\n## Tool Integration\n\n### Pattern 21: Pre-commit Hooks\n\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: uv-lock\n        name: uv lock\n        entry: uv lock\n        language: system\n        pass_filenames: false\n\n      - id: ruff\n        name: ruff\n        entry: uv run ruff check --fix\n        language: system\n        types: [python]\n\n      - id: black\n        name: black\n        entry: uv run black\n        language: system\n        types: [python]\n```\n\n### Pattern 22: VS Code Integration\n\n```json\n// .vscode/settings.json\n{\n  \"python.defaultInterpreterPath\": \"${workspaceFolder}/.venv/bin/python\",\n  \"python.terminal.activateEnvironment\": true,\n  \"python.testing.pytestEnabled\": true,\n  \"python.testing.pytestArgs\": [\"-v\"],\n  \"python.linting.enabled\": true,\n  \"python.formatting.provider\": \"black\",\n  \"[python]\": {\n    \"editor.defaultFormatter\": \"ms-python.black-formatter\",\n    \"editor.formatOnSave\": true\n  }\n}\n```\n\n## Troubleshooting\n\n### Common Issues\n\n```bash\n# Issue: uv not found\n# Solution: Add to PATH or reinstall\necho 'export PATH=\"$HOME/.cargo/bin:$PATH\"' >> ~/.bashrc\n\n# Issue: Wrong Python version\n# Solution: Pin version explicitly\nuv python pin 3.12\nuv venv --python 3.12\n\n# Issue: Dependency conflict\n# Solution: Check resolution\nuv lock --verbose\n\n# Issue: Cache issues\n# Solution: Clear cache\nuv cache clean\n\n# Issue: Lockfile out of sync\n# Solution: Regenerate\nuv lock --upgrade\n```\n\n## Best Practices\n\n### Project Setup\n\n1. **Always use lockfiles** for reproducibility\n2. **Pin Python version** with .python-version\n3. **Separate dev dependencies** from production\n4. **Use uv run** instead of activating venv\n5. **Commit uv.lock** to version control\n6. **Use --frozen in CI** for consistent builds\n7. **Leverage global cache** for speed\n8. **Use workspace** for monorepos\n9. **Export requirements.txt** for compatibility\n10. **Keep uv updated** for latest features\n\n### Performance Tips\n\n```bash\n# Use frozen installs in CI\nuv sync --frozen\n\n# Use offline mode when possible\nuv sync --offline\n\n# Parallel operations (automatic)\n# uv does this by default\n\n# Reuse cache across environments\n# uv shares cache globally\n\n# Use lockfiles to skip resolution\nuv sync --frozen  # skips resolution\n```\n\n## Migration Guide\n\n### From pip + requirements.txt\n\n```bash\n# Before\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n\n# After\nuv venv\nuv pip install -r requirements.txt\n# Or better:\nuv init\nuv add -r requirements.txt\n```\n\n### From Poetry\n\n```bash\n# Before\npoetry install\npoetry add requests\n\n# After\nuv sync\nuv add requests\n\n# Keep existing pyproject.toml\n# uv reads [project] and [tool.poetry] sections\n```\n\n### From pip-tools\n\n```bash\n# Before\npip-compile requirements.in\npip-sync requirements.txt\n\n# After\nuv lock\nuv sync --frozen\n```\n\n## Command Reference\n\n### Essential Commands\n\n```bash\n# Project management\nuv init [PATH]              # Initialize project\nuv add PACKAGE              # Add dependency\nuv remove PACKAGE           # Remove dependency\nuv sync                     # Install dependencies\nuv lock                     # Create/update lockfile\n\n# Virtual environments\nuv venv [PATH]              # Create venv\nuv run COMMAND              # Run in venv\n\n# Python management\nuv python install VERSION   # Install Python\nuv python list              # List installed Pythons\nuv python pin VERSION       # Pin Python version\n\n# Package installation (pip-compatible)\nuv pip install PACKAGE      # Install package\nuv pip uninstall PACKAGE    # Uninstall package\nuv pip freeze               # List installed\nuv pip list                 # List packages\n\n# Utility\nuv cache clean              # Clear cache\nuv cache dir                # Show cache location\nuv --version                # Show version\n```\n\n## Resources\n\n- **Official documentation**: https://docs.astral.sh/uv/\n- **GitHub repository**: https://github.com/astral-sh/uv\n- **Astral blog**: https://astral.sh/blog\n- **Migration guides**: https://docs.astral.sh/uv/guides/\n- **Comparison with other tools**: https://docs.astral.sh/uv/pip/compatibility/\n\n## Best Practices Summary\n\n1. **Use uv for all new projects** - Start with `uv init`\n2. **Commit lockfiles** - Ensure reproducible builds\n3. **Pin Python versions** - Use .python-version\n4. **Use uv run** - Avoid manual venv activation\n5. **Leverage caching** - Let uv manage global cache\n6. **Use --frozen in CI** - Exact reproduction\n7. **Keep uv updated** - Fast-moving project\n8. **Use workspaces** - For monorepo projects\n9. **Export for compatibility** - Generate requirements.txt when needed\n10. **Read the docs** - uv is feature-rich and evolving\n",
      "references": {},
      "assets": {}
    }
  ],
  "counts": {
    "agents": 3,
    "commands": 1,
    "skills": 5
  }
}